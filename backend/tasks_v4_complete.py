# ================================================================ 
# AnalyLit V4.1 - T√¢ches RQ (100% PostgreSQL/SQLAlchemy) - CORRIG√â 
# ================================================================ 
import os
import io
import time
import json
import uuid
import math
import logging
import random
from datetime import datetime
import re
from functools import wraps
from pathlib import Path

# --- CORRECTIF DE COMPATIBILIT√â PYZOTERO / FEEDPARSER ---
# pyzotero tente de patcher une m√©thode interne de feedparser qui n'existe plus.
# Nous appliquons manuellement un patch compatible avant d'importer pyzotero.
import numpy as np
import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from pyzotero import zotero
from scipy import stats
from sklearn.metrics import cohen_kappa_score

import chromadb
from sentence_transformers import SentenceTransformer
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker, scoped_session
from redis import Redis
from rq import get_current_job

# --- Importer la config de l'application ---
from backend.config.config_v4 import get_config

# --- Importer les mod√®les de la base de donn√©es ---
from utils.models import (
    Project, SearchResult, Extraction, Grid, ChatMessage, AnalysisProfile, RiskOfBias,
    SCHEMA  # ‚úÖ CORRECTION: Importer la variable SCHEMA pour la configuration de l'engine.
)
#
import traceback
from sqlalchemy.orm import Session # Explicitly import Session for type hinting if needed, though Session is already defined below


# --- Importer les helpers/utilitaires applicatifs ---
from utils.fetchers import db_manager, fetch_unpaywall_pdf_url, fetch_article_details
from utils.ai_processors import call_ollama_api
from utils.file_handlers import sanitize_filename, extract_text_from_pdf
from utils.analysis import generate_discussion_draft
from utils.notifications import send_project_notification
from utils.helpers import http_get_with_retries
from utils.importers import ZoteroAbstractExtractor
from utils.importers import process_zotero_item_list

# Prompts templates
from utils.prompt_templates import (
    get_screening_prompt_template,
    get_full_extraction_prompt_template,
    get_synthesis_prompt_template,
    get_rag_chat_prompt_template,
    get_effective_prompt_template,
) # Import de la fonction centralis√©e
from utils.logging_config import setup_logging

# --- Configuration globale ---
config = get_config()

logger = logging.getLogger(__name__)
# ‚úÖ CORRECTION: The project data directory should be at the root of the app workspace,
# not inside the backend config. This resolves the PermissionError in tests.
PROJECTS_DIR = Path('/home/appuser/app/projects')
PROJECTS_DIR.mkdir(parents=True, exist_ok=True)

# --- Base de Donn√©es (SQLAlchemy Uniquement pour les t√¢ches) ---
# ‚úÖ CORRECTION: R√©tablir une factory de session pour les workers RQ,
from rq import Queue
from redis import Redis
# mais le d√©corateur ci-dessous s'assurera que les tests utilisent leur propre session.
# ‚úÖ CORRECTION FINALE: La connexion DB des workers doit √™tre sensible au mode de test.
# Si la variable d'environnement TESTING est 'true', on utilise la base de donn√©es de test.
# C'est la correction cl√© pour les tests qui bloquent.
is_testing = os.getenv('TESTING') == 'true'
db_url = os.getenv('TEST_DATABASE_URL') if is_testing else config.DATABASE_URL

if is_testing and not db_url:
    # Fallback pour s'assurer que les tests ne touchent jamais la DB de prod par accident
    db_url = 'postgresql://user:pass@localhost:5432/test_db_fallback'

engine = create_engine(
    db_url,
    pool_pre_ping=True,
    connect_args={"options": f"-csearch_path={SCHEMA},public"}
)
SessionFactory = sessionmaker(bind=engine, autoflush=False, autocommit=False)

config = get_config()
redis_conn = Redis.from_url(config.REDIS_URL)
analysis_queue = Queue('analysis_queue', connection=redis_conn)
# --- Embeddings / Vector store (RAG) ---
EMBEDDING_MODEL_NAME = getattr(config, "EMBEDDING_MODEL", "all-MiniLM-L6-v2")
EMBED_BATCH = getattr(config, "EMBED_BATCH", 32)
MIN_CHUNK_LEN = getattr(config, "MIN_CHUNK_LEN", 250)
USE_QUERY_EMBED = getattr(config, "USE_QUERY_EMBED", True)
CHUNK_SIZE = getattr(config, "CHUNK_SIZE", 1200)
CHUNK_OVERLAP = getattr(config, "CHUNK_OVERLAP", 200)

# Charge un mod√®le d'embedding localement
try:
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
except Exception as e:
    logger.error(f"Erreur chargement du mod√®le d'embedding '{EMBEDDING_MODEL_NAME}': {e}")
    embedding_model = None

# ================================================================ 
# === D√âCORATEUR DE GESTION DE SESSION DB
# ================================================================ 

def with_db_session(func):
    # Assurer que le logging est configur√© au d√©but de la t√¢che
    setup_logging()

    @wraps(func)
    def wrapper(*args, **kwargs):
        # ‚úÖ CORRECTION FINALE: Logique de session robuste pour les tests et la production.
        # Si le premier argument est d√©j√† une session SQLAlchemy (fournie par une fixture de test),
        # on l'utilise directement.
        if args and hasattr(args[0], 'query'): # D√©tecte si une session est d√©j√† pass√©e
            # La session est d√©j√† fournie (cas des tests)
            return func(*args, **kwargs)
        
        # Cas normal (ex√©cution par RQ) : cr√©er une nouvelle session pour la dur√©e de la t√¢che.
        session = SessionFactory()
        try:
            # Injecter la session comme premier argument pour standardiser la signature des t√¢ches.
            result = func(session, *args, **kwargs)
            session.commit()
            return result
        except Exception as e:
            session.rollback()
            logger.error(f"Erreur dans la t√¢che {func.__name__}: {e}", exc_info=True)
            raise # Propager l'exception pour que RQ marque la t√¢che comme √©chou√©e
        finally:
            session.close() # Toujours fermer la session pour lib√©rer la connexion.
    return wrapper

# ================================================================ 
# === FONCTIONS UTILITAIRES DB-SAFE (SQLAlchemy)
# ================================================================ 
def update_project_status(session, project_id: str, status: str, result: dict = None, discussion: str = None,
                          graph: dict = None, prisma_path: str = None, analysis_result: dict = None,
                          analysis_plot_path: str = None):
    """Met √† jour le statut et/ou champs r√©sultat d'un projet."""
    now_iso = datetime.now().isoformat()
    
    set_clauses = ["status = :status", "updated_at = :ts"]
    params = {"pid": project_id, "ts": now_iso, "status": status}

    if result is not None:
        set_clauses.append("synthesis_result = :res")
        params["res"] = json.dumps(result)
    if discussion is not None:
        set_clauses.append("discussion_draft = :d")
        params["d"] = discussion
    if graph is not None:
        set_clauses.append("knowledge_graph = :g")
        params["g"] = json.dumps(graph)
    if prisma_path is not None:
        set_clauses.append("prisma_flow_path = :p")
        params["p"] = prisma_path
    if analysis_result is not None:
        set_clauses.append("analysis_result = :ar")
        params["ar"] = json.dumps(analysis_result)
    if analysis_plot_path is not None:
        set_clauses.append("analysis_plot_path = :pp")
        params["pp"] = analysis_plot_path

    stmt = f"UPDATE projects SET {', '.join(set_clauses)} WHERE id = :pid"
    session.execute(text(stmt), params)

def log_processing_status(session, project_id: str, article_id: str, status: str, details: str):
    """Enregistre un √©v√©nement de traitement dans processing_log."""
    
    # Nous devons g√©n√©rer manuellement l'UUID car nous utilisons du SQL brut.
    log_id = str(uuid.uuid4()) 
    
    session.execute(text("""
        INSERT INTO processing_log (id, project_id, pmid, task_name, status, details, \"timestamp\")
        VALUES (:id, :project_id, :pmid, :task_name, :status, :details, :ts)
    """), {
        "id": log_id,
        "project_id": project_id, 
        "pmid": article_id, 
        "task_name": "process_article",
        "status": status, 
        "details": details, 
        "ts": datetime.now()
    })
    
def increment_processed_count(session, project_id: str):
    """Incr√©mente processed_count du projet."""
    session.execute(text("UPDATE projects SET processed_count = processed_count + 1 WHERE id = :id"), {"id": project_id})

def update_project_timing(session, project_id: str, duration: float):
    """Ajoute une dur√©e au total_processing_time."""
    session.execute(text("UPDATE projects SET total_processing_time = total_processing_time + :d WHERE id = :id"), {"d": float(duration), "id": project_id})

# ================================================================ 
# === T√¢ches RQ (100% SQLAlchemy)
# ================================================================ 

# === Utilitaires ===
def normalize_profile(profile: dict) -> dict:
    """
    Normalizes the profile dictionary to support both old and new keys.
    """
    if not profile:
        return {'preprocess': 'phi3:mini', 'extract': 'llama3.1:8b', 'synthesis': 'llama3.1:8b'}
    # Support both old and new keys, defaulting to phi3:mini or llama3.1:8b if not found
    return {
        'preprocess': profile.get('preprocess') or profile.get('preprocess_model') or 'phi3:mini',
        'extract': profile.get('extract') or profile.get('extract_model') or 'llama3.1:8b',
        'synthesis': profile.get('synthesis') or profile.get('synthesis_model') or 'llama3.1:8b'
    }



# --- Mock function for E2E tests ---
def _mock_multi_database_search_task(session, project_id: str, query: str, databases: list, max_results_per_db: int = 50, *args, **kwargs):
    """
    Mock version of multi_database_search_task for E2E tests.
    Inserts dummy search results directly into the database.
    """
    logger.debug(f"DEBUG: _mock_multi_database_search_task received args: {args}, kwargs: {kwargs}")
    logger.info(f"√∞≈∏‚Äù  MOCK Recherche multi-bases pour {project_id} - {databases}")
    
    dummy_results = [
        {
            "id": str(uuid.uuid4()),
            "project_id": project_id,
            "article_id": "PMID:12345678",
            "title": "Mock Article 1: Impact of Diabetes on Health",
            "abstract": "This is a mock abstract about diabetes and its health implications.",
            "authors": "Doe J, Smith A",
            "publication_date": "2023-01-15",
            "journal": "Mock Journal of Medicine",
            "doi": "10.1000/mock.12345678",
            "url": "http://mock.example.com/article1",
            "database_source": "pubmed",
            "created_at": datetime.now().isoformat()
        },
        {
            "id": str(uuid.uuid4()),
            "project_id": project_id,
            "article_id": "PMID:87654321",
            "title": "Mock Article 2: New Therapies for Type 2 Diabetes",
            "abstract": "A mock study on innovative treatments for type 2 diabetes.",
            "authors": "Brown B, Green C",
            "publication_date": "2022-11-01",
            "journal": "Mock Diabetes Research",
            "doi": "10.1000/mock.87654321",
            "url": "http://mock.example.com/article2",
            "database_source": "pubmed",
            "created_at": datetime.now().isoformat()
        }
    ]
    
    session.execute(text("""
        INSERT INTO search_results (id, project_id, article_id, title, abstract, authors, publication_date, journal, doi, url, database_source, created_at)
        VALUES (:id, :project_id, :article_id, :title, :abstract, :authors, :publication_date, :journal, :doi, :url, :database_source, :created_at)
        ON CONFLICT (project_id, article_id) DO NOTHING
    """), dummy_results)
    
    total_found = len(dummy_results)
    session.execute(text("UPDATE projects SET status = 'search_completed', pmids_count = :n, updated_at = :ts WHERE id = :id"), {"n": total_found, "ts": datetime.now().isoformat(), "id": project_id})
    send_project_notification(project_id, 'search_completed', f'MOCK Recherche termin√©e: {total_found} articles trouv√©s', {'total_results': total_found, 'databases': databases})
    logger.info(f"√¢≈ì‚Ä¶ MOCK Recherche multi-bases: total {total_found}")
    time.sleep(1) # Give DB time to commit


@with_db_session
def multi_database_search_task(session, project_id: str, query: str, databases: list, max_results_per_db: int = 50, expert_queries: dict = None):
    """
    Recherche dans plusieurs bases et ins√®re les r√©sultats dans search_results.
    G√®re √† la fois les requ√™tes simples et les requ√™tes expertes sp√©cifiques √† chaque base.
    """

    if os.environ.get("ANALYLIT_TEST_MODE") == "true":
        _mock_multi_database_search_task(session, project_id, query, databases, max_results_per_db)
        return

    # Logique pour d√©terminer la requ√™te principale √† afficher (pour la compatibilit√©)
    main_query_for_log = query
    if expert_queries:
        # Prend la premi√®re requ√™te experte comme requ√™te "principale" pour les logs
        main_query_for_log = next(iter(expert_queries.values()), "Recherche experte sans requ√™te principale")
        logger.info(f"√∞≈∏‚Äù  Recherche EXPERTE multi-bases pour {project_id} - {databases}")
    else:
        logger.info(f"√∞≈∏‚Äù  Recherche SIMPLE multi-bases pour {project_id} - {databases}")

    total_found = 0

    # S'assurer que le projet existe avant de continuer
    if not session.get(Project, project_id):
        logger.error(f"Erreur critique: Le projet {project_id} n'existe pas au d√©but de la t√¢che de recherche.")
        return

    all_records_to_insert = []
    failed_databases = []
    for db_name in databases:
        current_query = None  # Initialiser √† None
        results = []

        # D√©terminer la requ√™te √† utiliser pour cette base de donn√©es sp√©cifique
        if expert_queries:
            # Mode expert. On v√©rifie si une requ√™te sp√©cifique existe
            if db_name in expert_queries and expert_queries[db_name] and expert_queries[db_name].strip():
                # Oui, une requ√™te experte valide existe
                current_query = expert_queries[db_name]
            else:
                # Le mode expert est activ√©, mais pour cette DB, la requ√™te est vide ou absente.
                # On ne fait rien (current_query reste None), ce qui la fera skipper ci-dessous.
                pass
        else:
            # Pas de mode expert, on utilise la requ√™te simple
            current_query = query

        if not current_query or not current_query.strip():
            logger.info(f"Requ√™te vide pour {db_name}, base de donn√©es ignor√©e.")
            continue

        logger.info(f"√∞≈∏‚Äú≈° Recherche dans {db_name}...")
        try:
            if db_name == 'pubmed':
                # ‚úÖ CORRECTION: Impl√©mentation de la pagination pour PubMed
                from Bio import Entrez
                Entrez.email = config.UNPAYWALL_EMAIL
                
                max_results = min(max_results_per_db, config.MAX_PUBMED_RESULTS)
                page_size = config.PAGE_SIZE_PUBMED
                retstart = 0
                all_ids = []

                logger.info(f"R√©cup√©ration de jusqu'√† {max_results} articles de PubMed par pages de {page_size}...")

                while retstart < max_results:
                    handle = Entrez.esearch(
                        db="pubmed",
                        term=current_query,
                        retstart=retstart,
                        retmax=min(page_size, max_results - retstart),
                        usehistory="y"
                    )
                    record = Entrez.read(handle)
                    handle.close()

                    ids = record.get("IdList", [])
                    logger.info(f"Appel PubMed (retstart={retstart}, retmax={min(page_size, max_results - retstart)}): {len(ids)} IDs r√©cup√©r√©s.")

                    if not ids:
                        break
                    all_ids.extend(ids)
                    retstart += len(ids)
                    if len(ids) < min(page_size, max_results - retstart):
                        break
                
                results = db_manager.fetch_details_for_ids(all_ids)
            elif db_name == 'arxiv':
                results = db_manager.search_arxiv(current_query, max_results_per_db)
            elif db_name == 'crossref':
                results = db_manager.search_crossref(current_query, max_results_per_db)
            elif db_name == 'ieee':
                results = db_manager.search_ieee(current_query, max_results_per_db)
            else:
                logger.warning(f"Base inconnue ignor√©e: {db_name}")
                continue

            for r in results:
                all_records_to_insert.append({
                    "id": str(uuid.uuid4()), "pid": project_id, "aid": r.get('id'),
                    "title": r.get('title', ''), "abstract": r.get('abstract', ''),
                    "authors": r.get('authors', ''), "pub_date": r.get('publication_date', ''),
                    "journal": r.get('journal', ''), "doi": r.get('doi', ''),
                    "url": r.get('url', ''), "src": r.get('database_source', 'unknown'),
                    "ts": datetime.now().isoformat()
                })
            total_found += len(results)
            send_project_notification(project_id, 'search_progress', f'Recherche termin√©e dans {db_name}: {len(results)} r√©sultats', {'database': db_name, 'count': len(results)})
            time.sleep(0.6)
        except Exception as e:
            # Am√©lioration de la r√©silience : on logue l'erreur et on continue
            logger.error(f"√âchec de la recherche pour la base '{db_name}': {e}", exc_info=True)
            failed_databases.append(db_name)

    if all_records_to_insert:
        session.execute(text("""
            INSERT INTO search_results (id, project_id, article_id, title, abstract, authors, publication_date, journal, doi, url, database_source, created_at)
            VALUES (:id, :pid, :aid, :title, :abstract, :authors, :pub_date, :journal, :doi, :url, :src, :ts) 
            ON CONFLICT (project_id, article_id) DO NOTHING

        """), all_records_to_insert)
        session.commit() # Commit the transaction

        # Enqueue screening tasks
        logger.info(f"üöÄ Enqueuing {len(all_records_to_insert)} screening tasks...")

        # R√©cup√©rer le projet et le profil associ√© pour obtenir les mod√®les
        project = session.get(Project, project_id)

        # Logique simplifi√©e et plus robuste
        profile_name = 'standard' # Toujours commencer avec le d√©faut
        if project and project.profile_used:
            # Chercher le profil par son nom/id dans la DB
            profile_from_db = session.query(AnalysisProfile).filter_by(name=project.profile_used).first()
            if profile_from_db:
                profile_name = profile_from_db.name.lower()
            else:
                logger.warning(f"Profil '{project.profile_used}' non trouv√© dans la base de donn√©es. Using default 'standard'.")

        # Utiliser le nom du profil pour obtenir les mod√®les depuis la config
        profile_name = profile_name.strip()  # Trim leading/trailing whitespace from profile_name
        profile_dict = config.DEFAULT_MODELS.get(profile_name, config.DEFAULT_MODELS.get('standard',{'preprocess': 'phi3:mini', 'extract': 'llama3.1:8b', 'synthesis': 'llama3.1:8b'}))

        # Trim leading/trailing whitespace from profile_name
        profile_name = profile_name.strip()
        for record in all_records_to_insert:
            analysis_queue.enqueue(
                'backend.tasks_v4_complete.process_single_article_task',
                project_id=project_id,
                article_id=record['aid'],
                profile=profile_dict,
                analysis_mode='screening'  # On fait du screening !!
            )
        logger.info("‚úÖ Screening tasks enqueued.")


    session.execute(text("UPDATE projects SET status = 'search_completed', pmids_count = :n, updated_at = :ts WHERE id = :id"), {"n": total_found, "ts": datetime.now().isoformat(), "id": project_id})
    
    session.commit() # Commit the status update
    # Am√©lioration de la notification finale
    final_message = f'Recherche termin√©e: {total_found} articles trouv√©s.'

    if failed_databases:
        final_message += f" √âchec pour: {', '.join(failed_databases)}."

    send_project_notification(project_id, 'search_completed', final_message, {'total_results': total_found, 'databases': databases, 'failed': failed_databases})
    logger.info(f"√¢≈ì‚Ä¶ Recherche multi-bases: total {total_found}")

@with_db_session
def process_single_article_task(session, project_id: str, article_id: str, profile: dict, analysis_mode: str, custom_grid_id: str = None):
    """Traite un article: screening ou extraction compl√®te."""
    # 1) Normaliser le profil (compatibilit√© ancienne/nouvelle nomenclature)
    profile = normalize_profile(profile)
    
    logger.info(f"[process_single_article_task] project={project_id} article={article_id} mode={analysis_mode} profile={profile} grid={custom_grid_id}")
    
    start_time = time.time()
    
    try:
        # 2) R√©cup√©rer les donn√©es de l'article depuis search_results
        row = session.execute(
            text("SELECT * FROM search_results WHERE project_id = :pid AND article_id = :aid"), 
            {"pid": project_id, "aid": article_id}
        ).mappings().fetchone()
        
        if not row:
            log_processing_status(session, project_id, article_id, "erreur", "Article introuvable en base.")
            logger.error(f"Article {article_id} non trouv√© dans search_results pour project {project_id}")
            return {"status": "error", "message": "Article not found"}

        article = dict(row)
        
        # 3) D√©terminer le contenu textuel √† analyser (PDF si dispo, sinon abstract)
        text_for_analysis = ""
        analysis_source = "abstract"
        
        # V√©rifier si un PDF est disponible
        pdf_path = PROJECTS_DIR / project_id / f"{sanitize_filename(article_id)}.pdf"
        if pdf_path.exists():
            try:
                pdf_text = extract_text_from_pdf(str(pdf_path))
                if pdf_text and len(pdf_text) > 100:
                    text_for_analysis = pdf_text
                    analysis_source = "pdf"
                    logger.info(f"[process_single_article_task] Utilisation du PDF pour {article_id}")
            except Exception as e:
                logger.warning(f"[process_single_article_task] Erreur lecture PDF {article_id}: {e}")
        
        # Fallback sur titre + abstract
        if not text_for_analysis:
            text_for_analysis = f"{article.get('title', '')}\n\n{article.get('abstract', '')}"
            analysis_source = "abstract"
        
        # V√©rification contenu minimal
        if len(text_for_analysis.strip()) < 50:
            log_processing_status(session, project_id, article_id, "√©cart√©", "Contenu textuel insuffisant.")
            increment_processed_count(session, project_id)
            logger.warning(f"[process_single_article_task] Contenu insuffisant pour {article_id}")
            return {"status": "skipped", "reason": "insufficient_content"}
        
        # 4) Pr√©traitement du contenu avec le mod√®le preprocess
        if text_for_analysis:
            prompt_norm = f"Nettoie et normalise ce texte scientifique pour extraction d'informations. Retourne du texte propre.\n---\n{text_for_analysis[:3000]}"
            try:
                normalized = call_ollama_api(prompt_norm, profile["preprocess"], output_format=None)
                if isinstance(normalized, dict):
                    normalized = normalized.get("text") or json.dumps(normalized)
                text_for_analysis = normalized if normalized else text_for_analysis
            except Exception as e:
                logger.warning(f"[process_single_article_task] Pr√©traitement √©chou√© pour {article_id}: {e}")
        
        # 5) Traitement selon le mode d'analyse
        if analysis_mode == "screening":
            # Mode screening : √©valuation binaire de pertinence
            screening_prompt = (
                "Tu es un assistant pour le screening d'articles sur l'Alliance Th√©rapeutique Num√©rique (ATN). "
                "Analyse ce contenu et d√©termine s'il est pertinent pour une revue sur l'ATN.\n"
                "Crit√®res de pertinence :\n"
                "- Relation patient-IA ou soignant-IA\n"
                "- Technologies num√©riques en sant√©\n"
                "- Empathie artificielle, confiance algorithmique\n"
                "- Acceptabilit√© des plateformes de sant√©\n\n"
                "R√©ponds en JSON avec les cl√©s: is_relevant (bool), score (int 0-10), reason (string).\n---\n"
                f"{text_for_analysis[:4000]}"
            )
            
            extract_res = call_ollama_api(screening_prompt, profile["extract"], output_format="json")
            
            # Parsing s√©curis√© de la r√©ponse
            if isinstance(extract_res, str):
                try:
                    extract_res = json.loads(extract_res)
                except Exception as e:
                    logger.warning(f"[process_single_article_task] Parsing JSON √©chou√© pour {article_id}: {e}")
                    extract_res = {"is_relevant": False, "score": 0, "reason": "parse_error"}
            
            is_relevant = bool(extract_res.get("is_relevant", False))
            score = int(extract_res.get("score", 0))
            reason = extract_res.get("reason", "")
            
            # Sauvegarde du r√©sultat de screening
            session.execute(text("""
                INSERT INTO extractions (id, project_id, pmid, title, relevance_score, relevance_justification, analysis_source, created_at)
                VALUES (:id, :pid, :pmid, :title, :score, :just, :src, :ts)
                ON CONFLICT (project_id, pmid) DO UPDATE SET
                    relevance_score = EXCLUDED.relevance_score,
                    relevance_justification = EXCLUDED.relevance_justification,
                    analysis_source = EXCLUDED.analysis_source,
                    created_at = EXCLUDED.created_at
            """), {
                "id": str(uuid.uuid4()), 
                "pid": project_id, 
                "pmid": article_id, 
                "title": article.get("title", ""), 
                "score": score, 
                "just": reason, 
                "src": analysis_source, 
                "ts": datetime.now().isoformat()
            })
            
            logger.info(f"[process_single_article_task] Screening termin√© - {article_id}: relevant={is_relevant}, score={score}")
            result = {"status": "ok", "mode": "screening", "article_id": article_id, "score": score, "relevant": is_relevant}
            
        elif analysis_mode in ("full_extraction", "extraction", "extract"):
            # Mode extraction compl√®te : extraction structur√©e pour ATN
            
            # D√©terminer les champs √† extraire
            fields_list = []
            if custom_grid_id:
                grid_row = session.execute(
                    text("SELECT fields FROM extraction_grids WHERE id = :gid AND project_id = :pid"), 
                    {"gid": custom_grid_id, "pid": project_id}
                ).mappings().fetchone()
                
                if grid_row and grid_row.get("fields"):
                    fields_list_of_dicts = json.loads(grid_row["fields"])
                    if isinstance(fields_list_of_dicts, list):
                        fields_list = [d.get("name") for d in fields_list_of_dicts if d.get("name")]
            
            # Grille par d√©faut pour ATN si pas de grille custom
            if not fields_list:
                fields_list = [
                    "Population_√©tudi√©e", "Type_IA", "Contexte_th√©rapeutique", 
                    "Score_empathie_IA", "Score_empathie_humain", "WAI-SR_modifi√©",
                    "Taux_adh√©sion", "Confiance_algorithmique", "Acceptabilit√©_patients",
                    "Perspective_multipartie", "Consid√©ration_√©thique", "RGPD_conformit√©",
                    "R√©sultats_principaux", "Limites_√©tude"
                ]
            
            # Prompt d'extraction structur√©e
            fields_description = "\n".join([f"- {field}" for field in fields_list])
            extraction_prompt = f"""
Extrait les informations suivantes de cet article scientifique sur l'Alliance Th√©rapeutique Num√©rique (ATN).
Pour chaque champ, extrait l'information pertinente ou mets "Non sp√©cifi√©" si l'information n'est pas disponible.

CHAMPS √Ä EXTRAIRE:
{fields_description}

INSTRUCTIONS:
- Sois pr√©cis et concis
- Pour les scores num√©riques, utilise des valeurs num√©riques quand possible
- Pour les pourcentages, utilise le format "X%" ou la valeur num√©rique
- Retourne un objet JSON avec chaque champ comme cl√©

TEXTE DE L'ARTICLE:
---
{text_for_analysis[:6000]}
---
"""
            
            extracted = call_ollama_api(extraction_prompt, profile["extract"], output_format="json")
            
            # Parsing s√©curis√©
            if isinstance(extracted, str):
                try:
                    extracted = json.loads(extracted)
                except Exception as e:
                    logger.warning(f"[process_single_article_task] Parsing extraction √©chou√© pour {article_id}: {e}")
                    extracted = {"key_findings": extracted, "extraction_error": str(e)}
            
            if not isinstance(extracted, dict):
                extracted = {"raw_response": str(extracted)}
            
            # Sauvegarde de l'extraction compl√®te
            session.execute(text("""
                DELETE FROM extractions 
                WHERE project_id = :pid AND pmid = :pmid
            """), {"pid": project_id, "pmid": article_id})
            session.execute(text("""
                INSERT INTO extractions (id, project_id, pmid, title, extracted_data, relevance_score, relevance_justification, analysis_source, created_at)
                VALUES (:id, :pid, :pmid, :title, :ex_data, :score, :just, :src, :ts)
            """), {
                "id": str(uuid.uuid4()), 
                "pid": project_id,
                "pmid": article_id,
                "title": article.get("title", ""),
                "ex_data": json.dumps(extracted),
                "score": 10,  # Score √©lev√© par d√©faut pour extraction compl√®te
                "just": "Extraction d√©taill√©e ATN effectu√©e",
                "src": analysis_source,
                "ts": datetime.now().isoformat()
            })
            
            logger.info(f"[process_single_article_task] Extraction compl√®te termin√©e - {article_id}")
            result = {"status": "ok", "mode": "full_extraction", "article_id": article_id, "extracted_fields": len(extracted)}
            
        else:
            raise ValueError(f"Mode d'analyse inconnu: {analysis_mode}")
        
        # 6) Mise √† jour des compteurs de projet
        increment_processed_count(session, project_id)
        update_project_timing(session, project_id, time.time() - start_time)
        
        # Notification de progression
        send_project_notification(
            project_id, 
            'article_processed', 
            f'Article "{article.get("title", "")[:30]}..." trait√© ({analysis_mode})', 
            {'article_id': article_id, 'mode': analysis_mode}
        )
        
        return result
        
    except Exception as e:
        logger.exception("process_single_article_task failed: %s", e)
        
        # Marque l'article en erreur pour tra√ßabilit√©
        try:
            session.execute(text("""
                INSERT INTO extractions (id, project_id, pmid, title, relevance_score, relevance_justification, analysis_source, created_at)
                VALUES (:id, :pid, :pmid, :title, :score, :just, :src, :ts)
            """), {
                 "id": str(uuid.uuid4()), 

                "pid": project_id,
                "pmid": article_id,
                "title": f"ERREUR: {article_id}",
                "score": 0,
                "just": f"Erreur traitement: {str(e)}",
                "src": "error",
                "ts": datetime.now().isoformat()
            })
            
            increment_processed_count(session, project_id)
            
        except Exception as inner:
            logger.warning(f"[process_single_article_task] Impossible de persister le statut d'erreur: {inner}")
        
        # Remonter l'exception pour que RQ marque le job en failed
        raise
    logger.info(f"‚úÖ Extraction termin√©e pour {article_id} - donn√©es sauv√©es dans extractions")
    return result
    
@with_db_session
def run_synthesis_task(session, project_id: str, profile: dict):
    """G√©n√®re une synth√®se √† partir des articles pertinents (score >= 7)."""
    update_project_status(session, project_id, 'synthesizing')
    project = session.execute(text("SELECT description FROM projects WHERE id = :pid"), {"pid": project_id}).mappings().fetchone()
    project_description = project['description'] if project else "Non sp√©cifi√©"

    rows = session.execute(text("SELECT s.title, s.abstract FROM extractions e JOIN search_results s ON e.project_id = s.project_id AND e.pmid = s.article_id WHERE e.project_id = :pid AND e.relevance_score >= 7 ORDER BY e.relevance_score DESC LIMIT 30"), {"pid": project_id}).mappings().all()
    if not rows:
        update_project_status(session, project_id, 'failed')
        send_project_notification(project_id, 'synthesis_failed', 'Aucun article pertinent (score >= 7).')
        return

    abstracts = [f"Titre: {r['title']}\nR√©sum√©: {r['abstract']}" for r in rows if r.get('abstract')]
    if not abstracts:
        update_project_status(session, project_id, 'failed')
        send_project_notification(project_id, 'synthesis_failed', 'Articles pertinents sans r√©sum√©.')
        return

    data_for_prompt = "\n---\n".join(abstracts)
    tpl = get_effective_prompt_template('synthesis_prompt', get_synthesis_prompt_template())
    prompt = tpl.format(project_description=project_description, data_for_prompt=data_for_prompt)
    output = call_ollama_api(prompt, profile.get('synthesis_model', 'llama3.1:8b'), output_format="json")
    try:
        if output and isinstance(output, dict):
            update_project_status(session, project_id, status='completed', result=output)
            session.commit() # Commit the status update
            send_project_notification(project_id, 'synthesis_completed', 'Synth√®se g√©n√©r√©e.')
        else:
            update_project_status(session, project_id, status='failed')
            send_project_notification(project_id, 'synthesis_failed', 'R√©ponse IA invalide.')
    except Exception as e:
        update_project_status(session, project_id, status='failed')
        logger.error(f"Erreur dans la t√¢che de synth√®se : {e}", exc_info=True)
        raise

@with_db_session
def run_discussion_generation_task(session, project_id: str):
    """G√©n√®re le brouillon de la discussion."""
    try:

        update_project_status(session, project_id, 'generating_analysis')
        rows = session.execute(text("SELECT e.extracted_data, e.pmid, s.title, e.relevance_score FROM extractions e JOIN search_results s ON e.project_id = s.project_id AND e.pmid = s.article_id WHERE e.project_id = :pid AND e.relevance_score >= 7 AND e.extracted_data IS NOT NULL"), {"pid": project_id}).mappings().all()
        if not rows: # Ne pas lever d'erreur, mais mettre √† jour le statut et notifier.
            update_project_status(session, project_id, status='failed')
            send_project_notification(project_id, 'analysis_failed', "Aucune donn√©e d'extraction pertinente trouv√©e pour g√©n√©rer la discussion.")
            return

        # The user request mentioned a correction for a function call on line 300.
        # That line does not exist in this file. The logic for enqueuing tasks is in `server_v4_complete.py`.
        # The most similar logic in this file is the call to `generate_discussion_draft` below.
        # I will assume the intent was to ensure this part is correct, which it appears to be.
        # No changes are made based on the user's specific instruction for line 300 as it's not applicable here.
        df = pd.DataFrame(rows)
        profile = session.execute(text("SELECT profile_used FROM projects WHERE id = :pid"), {"pid": project_id}).scalar_one_or_none() or 'standard'
        # The following line correctly calls the local function `generate_discussion_draft`
        model_name = config.DEFAULT_MODELS.get(profile, {}).get('synthesis', 'llama3.1:8b')
        draft = generate_discussion_draft(df, lambda p, m: call_ollama_api(p, m, temperature=0.7), model_name)

        update_project_status(session, project_id, status='completed', discussion=draft)
        session.commit() # Commit the status update
        send_project_notification(project_id, 'analysis_completed', 'Le brouillon de discussion a √©t√© g√©n√©r√©.', {'discussion_draft': draft})
    except Exception as e:
        update_project_status(session, project_id, status='failed')
        logger.error(f"Erreur dans la t√¢che de discussion : {e}", exc_info=True)
        raise

@with_db_session
def run_knowledge_graph_task(session, project_id: str):
    """G√©n√®re un graphe de connaissances JSON √† partir des titres d'articles extraits."""
    update_project_status(session, project_id, status='generating_graph')
    
    # Utiliser le mod√®le de synth√®se pour cette t√¢che, car il est plus adapt√© √† la g√©n√©ration de JSON structur√©.
    profile_info = session.query(Project).filter_by(id=project_id).first()
    analysis_profile = session.query(AnalysisProfile).filter_by(id=profile_info.profile_used).first() if profile_info and profile_info.profile_used else None
    model_to_use = (analysis_profile.extract_model or analysis_profile.synthesis_model) if analysis_profile else 'llama3.1:8b'
    
    rows = session.execute(text("SELECT title, pmid FROM extractions WHERE project_id = :pid"), {"pid": project_id}).mappings().all()
    if not rows:
        update_project_status(session, project_id, status='failed')
        send_project_notification(project_id, 'analysis_failed', 'Aucun article trouv√© pour g√©n√©rer le graphe de connaissances.', {'analysis_type': 'knowledge_graph'})
        return
    
    titles = [f"{r['title']} (ID: {r['pmid']})" for r in rows[:100]]
    prompt = f"""√Ä partir de la liste de titres suivante, g√©n√®re un graphe de connaissances.
Identifie les 10 concepts les plus importants et leurs relations.
Ta r√©ponse doit √™tre UNIQUEMENT un objet JSON avec "nodes" [{{id, label}}] et "edges" [{{from, to, label}}].

Titres:
{json.dumps(titles, indent=2)}"""
    graph = call_ollama_api(prompt, model=model_to_use, output_format="json")
    
    if graph and isinstance(graph, dict) and 'nodes' in graph and 'edges' in graph:
        update_project_status(session, project_id, status='completed', graph=graph)
        session.commit() # Commit the status update
        send_project_notification(project_id, 'analysis_completed', 'Le graphe de connaissances est pr√™t.', {'analysis_type': 'knowledge_graph'})
    else:
        update_project_status(session, project_id, status='failed')
        send_project_notification(project_id, 'analysis_failed', 'La g√©n√©ration du graphe de connaissances a √©chou√©.', {'analysis_type': 'knowledge_graph'})

@with_db_session
def run_prisma_flow_task(session, project_id: str):
    """G√©n√®re un diagramme PRISMA simplifi√© et stocke l'image sur disque."""
    update_project_status(session, project_id, status='generating_prisma')
    
    total_found = session.execute(text("SELECT COUNT(*) FROM search_results WHERE project_id = :pid"), {"pid": project_id}).scalar_one()
    n_included = session.execute(text("SELECT COUNT(*) FROM extractions WHERE project_id = :pid"), {"pid": project_id}).scalar_one()
    
    if total_found == 0:
        update_project_status(session, project_id, status='completed')
        return
    
    n_after_duplicates, n_excluded_screening = total_found, total_found - n_included
    
    fig, ax = plt.subplots(figsize=(12, 16), dpi=300)
    plt.style.use('seaborn-v0_8-whitegrid')
    box_props = dict(boxstyle="round,pad=0.8", facecolor='lightblue', edgecolor='navy', linewidth=2, alpha=0.8)
    font_props = {'fontsize': 12, 'fontweight': 'bold', 'fontfamily': 'serif'}
    
    ax.text(0.5, 0.9, f'Articles identifi√©s\nn = {total_found}', ha='center', va='center', bbox=box_props, **font_props)
    ax.text(0.5, 0.7, f'Apr√®s exclusion doublons\nn = {n_after_duplicates}', ha='center', va='center', bbox=box_props, **font_props)
    ax.text(0.5, 0.5, f'Articles √©valu√©s\nn = {n_after_duplicates}', ha='center', va='center', bbox=box_props, **font_props)
    ax.text(0.5, 0.3, f'√âtudes incluses\nn = {n_included}', ha='center', va='center', bbox=box_props, **font_props)
    ax.text(1.0, 0.5, f'Exclus au criblage\nn = {n_excluded_screening}', ha='left', va='center', bbox=box_props, **font_props)
    ax.axis('off')
    
    p_dir = PROJECTS_DIR / project_id
    p_dir.mkdir(exist_ok=True)
    image_path = str(p_dir / 'prisma_flow.png')
    plt.savefig(image_path, bbox_inches='tight', dpi=300, format='png')
    pdf_path = str(p_dir / 'prisma_flow.pdf') 
    plt.savefig(pdf_path, bbox_inches='tight', format='pdf')
    plt.close(fig)

    update_project_status(session, project_id, status='completed', prisma_path=image_path)
    session.commit() # Commit the status update
    send_project_notification(project_id, 'analysis_completed', 'Le diagramme PRISMA est pr√™t.', {'analysis_type': 'prisma_flow'})

@with_db_session
def run_meta_analysis_task(session, project_id: str):
    """M√©ta-analyse simple des scores de pertinence (distribution + IC 95%)."""
    update_project_status(session, project_id, 'generating_analysis')
    
    scores_list = session.execute(text("SELECT relevance_score FROM extractions WHERE project_id = :pid AND relevance_score IS NOT NULL AND relevance_score > 0"), {"pid": project_id}).scalars().all()
    if len(scores_list) < 2:
        update_project_status(session, project_id, status='failed')
        send_project_notification(project_id, 'analysis_failed', 'Pas assez de donn√©es pour la m√©ta-analyse (au moins 2 scores requis).')
        return
    
    scores = np.array(scores_list, dtype=float)
    mean_score, n = np.mean(scores), len(scores)
    stddev = np.std(scores, ddof=1)
    ci = stats.t.interval(0.95, df=n-1, loc=mean_score, scale=stddev / np.sqrt(n))
    
    analysis_result = {"mean_score": float(mean_score), "stddev": float(stddev), "confidence_interval": [float(ci[0]), float(ci[1])], "n_articles": n}
    
    p_dir = PROJECTS_DIR / project_id
    p_dir.mkdir(exist_ok=True)
    plot_path = str(p_dir / 'meta_analysis_plot.png')
    
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.hist(scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')
    ax.axvline(mean_score, color='red', linestyle='--', linewidth=2, label=f'Moyenne: {mean_score:.2f}')
    ax.set_xlabel('Score de Pertinence')
    ax.set_ylabel('Nombre d\'Articles')
    ax.set_title('Distribution des Scores de Pertinence')
    ax.legend()
    plt.savefig(plot_path, bbox_inches='tight')
    plt.close(fig)
    session.commit() # Commit the status update
    update_project_status(session, project_id, status='completed', analysis_result=analysis_result, analysis_plot_path=plot_path)
    send_project_notification(project_id, 'analysis_completed', 'M√©ta-analyse termin√©e.')

@with_db_session
def run_descriptive_stats_task(session, project_id: str):
    """G√©n√®re des statistiques descriptives sur les extractions."""
    logger.info(f"√∞≈∏‚Äú≈† Statistiques descriptives pour projet {project_id}")
    update_project_status(session, project_id, 'generating_analysis')
    
    rows = session.execute(text("SELECT relevance_score FROM extractions WHERE project_id = :pid AND relevance_score IS NOT NULL"), {"pid": project_id}).mappings().all()
    if not rows:
        update_project_status(session, project_id, status='failed')
        return
    
    scores = [r['relevance_score'] for r in rows]
    stats_result = {
        'total_extractions': len(scores), 'mean_score': float(np.mean(scores)),
        'median_score': float(np.median(scores)), 'std_dev': float(np.std(scores)),
        'min_score': float(np.min(scores)), 'max_score': float(np.max(scores))
    }
    
    update_project_status(session, project_id, status='completed', analysis_result=stats_result)
    session.commit() # Commit the status update
    send_project_notification(project_id, 'analysis_completed', 'Statistiques descriptives g√©n√©r√©es')

# ================================================================ 
# === CHAT RAG
# ================================================================ 

@with_db_session
def answer_chat_question_task(session, project_id: str, question: str):
    """R√©pond √† une question via RAG sur les PDFs index√©s."""
    logger.info(f"√∞≈∏‚Äô¬¨ Question chat pour projet {project_id}")
    
    if embedding_model is None:
        response = "Mod√®le d'embedding non disponible"
    else:
        try:
            client = chromadb.Client()
            collection = client.get_collection(name=f"project_{project_id}")
            query_embedding = embedding_model.encode([question]).tolist()
            results = collection.query(query_embeddings=query_embedding, n_results=3)
            
            if results['documents'] and results['documents'][0]:
                context = "\n---\n".join(results['documents'][0])
                prompt = f"""En te basant sur ces extraits de documents, r√©ponds √† la question:

Question: {question}

Contexte:
{context}

R√©ponds de fa√ßon concise et pr√©cise."""
                response = call_ollama_api(prompt, "llama3.1:8b")
            else:
                response = "Aucun document index√© trouv√© pour r√©pondre √† cette question."
        except Exception:
            response = "Erreur lors de la recherche dans la base de connaissances."
    
    session.execute(text("INSERT INTO chat_messages (id, project_id, role, content, timestamp) VALUES (:id1, :pid, 'user', :q, :ts1), (:id2, :pid, 'assistant', :a, :ts2)"), {"id1": str(uuid.uuid4()), "id2": str(uuid.uuid4()), "pid": project_id, "q": question, "a": response, "ts1": datetime.now().isoformat(), "ts2": datetime.now().isoformat()})
    return response

# ================================================================ 
# === IMPORT/EXPORT
# ================================================================ 

@with_db_session
def import_from_zotero_file_task(session, project_id: str, json_file_path: str):
    """Importe les articles depuis un fichier JSON Zotero en utilisant le parseur robuste."""
    logger.info(f"√∞≈∏‚Äú≈° Import Zotero depuis {json_file_path} pour projet {project_id}")
    extractor = ZoteroAbstractExtractor(json_file_path)
    records = extractor.process()

    if not records:
        send_project_notification(project_id, 'import_failed', 'Aucun article valide trouv√© dans le fichier Zotero.')
        return

    records_to_insert = []
    for record in records:
        if record.get('article_id'):
            records_to_insert.append({
                "id": str(uuid.uuid4()), "pid": project_id, "aid": record['article_id'],
                "title": record.get('title', 'Sans titre'), "abstract": record.get('abstract', ''),
                "authors": record.get('authors', ''), "pub_date": record.get('publication_date', ''),
                "journal": record.get('journal', ''), "doi": record.get('doi', ''),
                "url": record.get('url', ''), "src": record.get('database_source', 'zotero'),
                "ts": datetime.now().isoformat()
            })
    
    if records_to_insert:
        session.execute(text("""
            INSERT INTO search_results (id, project_id, article_id, title, abstract, authors, publication_date, journal, doi, url, database_source, created_at)
            VALUES (:id, :pid, :aid, :title, :abstract, :authors, :pub_date, :journal, :doi, :url, :src, :ts)
            ON CONFLICT (project_id, article_id) DO NOTHING
        """), records_to_insert)

    total_articles = session.execute(text("SELECT COUNT(*) FROM search_results WHERE project_id = :pid"), {"pid": project_id}).scalar_one()
    session.execute(text("UPDATE projects SET pmids_count = :count WHERE id = :pid"), {"count": total_articles, "pid": project_id})

    send_project_notification(project_id, 'import_completed', f'Import Zotero termin√©: {len(records_to_insert)} nouveaux articles ajout√©s.')
    
    try:
        os.remove(json_file_path)
    except Exception as e:
        logger.warning(f"Impossible de supprimer le fichier temporaire {json_file_path}: {e}")

def import_pdfs_from_zotero_task(project_id: str, pmids: list, zotero_user_id: str, zotero_api_key: str):
    
    """Importe les PDFs depuis Zotero pour les articles sp√©cifi√©s."""
    logger.info(f"√∞≈∏‚Äú‚Äû Import PDFs Zotero pour {len(pmids)} articles")
    try:
        from pyzotero import zotero
        zot = zotero.Zotero(zotero_user_id, 'user', zotero_api_key)
        project_dir = Path(config.PROJECTS_DIR) / project_id
        project_dir.mkdir(exist_ok=True)

        success_count = 0
        for pmid in pmids:
            try:
                # Recherche basique dans Zotero
                items = zot.items(q=pmid, limit=5)
                if items:
                    # Prendre le premier r√©sultat
                    item = items[0]
                    attachments = zot.children(item['key'])
                    for att in attachments:
                        if att.get('data', {}).get('contentType') == 'application/pdf':
                            # CORRECTION : Le bloc de code manquant est ajout√© ici
                            pdf_filename = sanitize_filename(att['data'].get('filename', f'{pmid}.pdf'))
                            pdf_path = project_dir / pdf_filename
                            if not pdf_path.exists():
                                zot.dump(att['key'], str(pdf_path))
                                success_count += 1
                                logger.info(f"PDF t√©l√©charg√©: {pdf_filename}")
                            break # On ne prend que le premier PDF trouv√©
            except Exception as e:
                logger.warning(f"Erreur r√©cup√©ration PDF pour {pmid} via Zotero: {e}")
                continue

        send_project_notification(project_id, 'import_completed', f'{success_count} PDF(s) import√©(s) depuis Zotero.')

    except Exception as e:
        logger.error(f"Erreur majeure dans import_pdfs_from_zotero_task: {e}")
        send_project_notification(project_id, 'import_failed', f'Erreur Zotero: {e}')

@with_db_session
def index_project_pdfs_task(session, project_id: str): # Ajout de 'session'
    """Indexe les PDFs d'un projet pour le RAG."""
    logger.info(f"√∞≈∏‚Äù  Indexation des PDFs pour projet {project_id}")
    try:
        project_dir = PROJECTS_DIR / project_id
        if not project_dir.exists():
            send_project_notification(project_id, 'indexing_failed', 'Dossier projet introuvable.', {'task_name': 'indexation'})
            return
        
        pdf_files = list(project_dir.glob("*.pdf"))
        if not pdf_files:
            send_project_notification(project_id, 'indexing_completed', 'Aucun PDF √† indexer.', {'task_name': 'indexation'})
            return
        
        if embedding_model is None:
            # CORRECTION: Mettre √† jour le statut du projet en cas d'√©chec pr√©coce
            try:
                session.execute(text("UPDATE projects SET status = 'failed' WHERE id = :pid"), {"pid": project_id})
                session.commit()
            except Exception as db_err:
                logger.error(f"Impossible de mettre √† jour le statut du projet {project_id} en 'failed': {db_err}")
                session.rollback()
            # FIN CORRECTION
            logger.warning("Mod√®le d'embedding non disponible")
            send_project_notification(project_id, 'indexing_failed', 'Mod√®le embedding non charg√©.', {'task_name': 'indexation'})
            return
        
        client = chromadb.Client()
        collection = client.get_or_create_collection(name=f"project_{project_id}")
        
        all_chunks = []
        all_metadatas = []
        all_ids = []
        
        total_pdfs = len(pdf_files)
        for i, pdf_path in enumerate(pdf_files):
            progress_message = f"Indexation PDF {i + 1} sur {total_pdfs}..."
            send_project_notification(
                project_id,
                'task_progress',
                progress_message,
                {'current': i, 'total': total_pdfs, 'task_name': 'indexation'} # Progression de 0 √† N-1
            )

            extracted_content = extract_text_from_pdf(str(pdf_path))
            if extracted_content and len(extracted_content) > 100:
                chunks = [extracted_content[i:i+CHUNK_SIZE] for i in range(0, len(extracted_content), CHUNK_SIZE-CHUNK_OVERLAP)]
                for chunk_index, chunk in enumerate(chunks):
                    if len(chunk) > MIN_CHUNK_LEN:
                        all_chunks.append(chunk)
                        all_ids.append(f"{sanitize_filename(pdf_path.stem)}_{chunk_index}")
                        all_metadatas.append({"source_id": pdf_path.stem, "filename": pdf_path.name, "chunk": chunk_index})

        if not all_chunks:
            send_project_notification(project_id, 'indexing_completed', 'Aucun contenu textuel trouv√© dans les PDFs.', {'task_name': 'indexation'})
            return

        # √âtape d'encodage par lots (beaucoup plus rapide)
        send_project_notification(project_id, 'task_progress', f'Encodage de {len(all_chunks)} segments de texte...', {'current': total_pdfs, 'total': total_pdfs + 1})
        embeddings = embedding_model.encode(all_chunks, batch_size=EMBED_BATCH, show_progress_bar=False).tolist()

        # √âtape d'ajout √† ChromaDB
        send_project_notification(project_id, 'task_progress', 'Sauvegarde dans la base de connaissances...', {'current': total_pdfs + 1, 'total': total_pdfs + 1})
        collection.add(
            documents=all_chunks,
            embeddings=embeddings,
            ids=all_ids,
            metadatas=all_metadatas
        )
        
        # Mettre √† jour le projet (cette ligne peut maintenant appeler la fonction 'text' import√©e)
        session.execute(text("UPDATE projects SET indexed_at = :ts WHERE id = :pid"), {"ts": datetime.now().isoformat(), "pid": project_id})
        
        send_project_notification(project_id, 'indexing_completed', f'{total_pdfs} PDF(s) ont √©t√© trait√©s et index√©s.', {'task_name': 'indexation'})
    
    except Exception as e:
        logger.error(f"Erreur index_project_pdfs_task: {e}", exc_info=True)
        # Am√©lioration de la gestion d'erreur : Mettre √† jour le statut du projet en cas d'√©chec
        try:
            session.execute(text("UPDATE projects SET status = 'failed' WHERE id = :pid"), {"pid": project_id})
            session.commit()
        except Exception as db_err:
            logger.error(f"Impossible de mettre √† jour le statut du projet {project_id} en 'failed' apr√®s une erreur d'indexation: {db_err}")
            session.rollback()
        
        send_project_notification(project_id, 'indexing_failed', f'Erreur lors de l\'indexation: {e}', {'task_name': 'indexation'})

@with_db_session
def fetch_online_pdf_task(session, project_id: str, article_id: str):
    """R√©cup√®re un PDF en ligne pour un article via Unpaywall si possible."""
    logger.info(f"√∞≈∏‚Äú‚Äû R√©cup√©ration PDF en ligne pour {article_id}")
    try: # Le bloc try/finally n'est plus n√©cessaire gr√¢ce au d√©corateur
        # ‚úÖ PATCH DE ROBUSTESSE : Garantit l'existence de la table dans le contexte du worker
        session.execute(text("""
            CREATE TABLE IF NOT EXISTS analylit_schema.search_results (
                id VARCHAR PRIMARY KEY, project_id VARCHAR, article_id VARCHAR,
                title TEXT, abstract TEXT, authors TEXT, publication_date TEXT,
                journal TEXT, doi VARCHAR, url VARCHAR, database_source VARCHAR, created_at TIMESTAMP, query VARCHAR
            );
        """))
        session.commit() # S'assurer que la table est cr√©√©e avant la requ√™te
        article = session.execute(text("""
            SELECT doi, url FROM search_results
            WHERE project_id = :pid AND article_id = :aid
        """), {"pid": project_id, "aid": article_id}).mappings().fetchone()
        
        if not article:
            return
        
        pdf_url = None
        if article.get("doi"):
            pdf_url = fetch_unpaywall_pdf_url(article["doi"])
        
        if pdf_url:
            response = http_get_with_retries(pdf_url, timeout=60)
            if response and response.headers.get('content-type', '').startswith('application/pdf'):
                project_dir = PROJECTS_DIR / project_id
                project_dir.mkdir(exist_ok=True)
                
                pdf_path = project_dir / f"{sanitize_filename(article_id)}.pdf"
                pdf_path.write_bytes(response.content)
                
                send_project_notification(project_id, 'pdf_upload_completed', f'PDF r√©cup√©r√© pour {article_id}')
                return
        
        send_project_notification(project_id, 'pdf_fetch_failed', f'PDF non trouv√© pour {article_id}')
    except Exception as e:
        logger.error(f"Erreur fetch_online_pdf_task: {e}")

# ================================================================ 
# === OLLAMA
# ================================================================ 

def pull_ollama_model_task(model_name: str):
    """T√©l√©charge un mod√®le Ollama."""
    logger.info(f"√∞≈∏¬§‚Äì T√©l√©chargement du mod√®le Ollama: {model_name}")
    
    try:
        import requests
        url = f"{config.OLLAMA_BASE_URL}/api/pull"
        payload = {"name": model_name, "stream": False}
        response = requests.post(url, json=payload, timeout=3600)
        response.raise_for_status()
        
        logger.info(f"√¢≈ì‚Ä¶ Mod√®le {model_name} t√©l√©charg√© avec succ√®s")
    except Exception as e:
        logger.error(f"√¢ ≈í Erreur t√©l√©chargement {model_name}: {e}")
        raise

# ================================================================ 
# === VALIDATION INTER-√âVALUATEURS
# ================================================================ 

@with_db_session
def calculate_kappa_task(session, project_id: str):
    """Calcule le coefficient Kappa de Cohen pour la validation inter-√©valuateurs."""
    logger.info(f"√∞≈∏‚Äú≈† Calcul du Kappa pour projet {project_id}")
    rows = session.execute(text("SELECT validations FROM extractions WHERE project_id = :pid AND validations IS NOT NULL"), {"pid": project_id}).mappings().all()
    if not rows:
        send_project_notification(project_id, 'kappa_failed', 'Aucune validation trouv√©e.')
        return
    
    eval1_decisions, eval2_decisions = [], []
    for r in rows:
        try:
            v = json.loads(r["validations"])
            if "evaluator1" in v and "evaluator2" in v:
                eval1 = 1 if v["evaluator1"].lower() == "include" else 0
                eval2 = 1 if v["evaluator2"].lower() == "include" else 0
                eval1_decisions.append(eval1)
                eval2_decisions.append(eval2)
        except Exception:
            continue
    
    if len(eval1_decisions) < 2:
        send_project_notification(project_id, 'kappa_failed', 'Pas assez de validations communes (minimum 2 requises).')
        return
    
    kappa = cohen_kappa_score(eval1_decisions, eval2_decisions)
    
    if kappa < 0: interpretation = "Accord pire que le hasard"
    elif kappa < 0.20: interpretation = "Accord faible"
    elif kappa < 0.40: interpretation = "Accord passable"
    elif kappa < 0.60: interpretation = "Accord mod√©r√©"
    elif kappa < 0.80: interpretation = "Accord substantiel"
    else: interpretation = "Accord quasi parfait"
    
    result = {
        "kappa": float(kappa), "interpretation": interpretation,
        "n_comparisons": len(eval1_decisions),
        "agreement_rate": float(np.mean(np.array(eval1_decisions) == np.array(eval2_decisions)))
    }
    
    session.execute(text("UPDATE projects SET inter_rater_reliability = :result WHERE id = :pid"), {"result": json.dumps(result), "pid": project_id})
    session.commit()
    message = f"Kappa = {kappa:.3f} ({interpretation}), n = {len(eval1_decisions)}"
    send_project_notification(project_id, 'kappa_calculated', message)

# ================================================================ 
# === SCORES ATN
# ================================================================ 

@with_db_session
def run_atn_stakeholder_analysis_task(session, project_id: str):
    """Analyse multipartie prenante sp√©cialis√©e pour l'ATN."""
    update_project_status(session, project_id, 'analyzing')
    rows = session.execute(text("SELECT extracted_data, stakeholder_perspective, ai_type, platform_used FROM extractions WHERE project_id = :pid AND extracted_data IS NOT NULL"), {"pid": project_id}).mappings().all()
    if not rows:
        update_project_status(session, project_id, 'failed')
        send_project_notification(project_id, 'analysis_failed', 'Aucune extraction disponible.')
        return
    
    total_studies, atn_metrics, ai_types_dist, platforms, ethical, regulatory = len(rows), {"empathy_scores_ai": [], "empathy_scores_human": [], "wai_sr_scores": [], "adherence_rates": [], "algorithmic_trust": [], "acceptability_scores": []}, {}, set(), [], {"gdpr": 0, "ai_act": 0}
    
    for row in rows:
        try:
            data = json.loads(row['extracted_data'])
            if data.get("Score_empathie_IA"): atn_metrics["empathy_scores_ai"].append(float(data["Score_empathie_IA"]))
            if data.get("Score_empathie_humain"): atn_metrics["empathy_scores_human"].append(float(data["Score_empathie_humain"]))
            if data.get("WAI-SR_modifi√©"): atn_metrics["wai_sr_scores"].append(float(data["WAI-SR_modifi√©"]))
            if data.get("Taux_adh√©sion"): atn_metrics["adherence_rates"].append(data["Taux_adh√©sion"])
            if data.get("Confiance_algorithmique"): atn_metrics["algorithmic_trust"].append(data["Confiance_algorithmique"])
            if data.get("Acceptabilit√©_patients"): atn_metrics["acceptability_scores"].append(data["Acceptabilit√©_patients"])
            if data.get("Type_IA"): ai_types_dist[data["Type_IA"]] = ai_types_dist.get(data["Type_IA"], 0) + 1
            if data.get("Plateforme"): platforms.add(data["Plateforme"])
            if data.get("Consid√©ration_√©thique"): ethical.append(data["Consid√©ration_√©thique"])
            if data.get("RGPD_conformit√©") and "oui" in data["RGPD_conformit√©"].lower(): regulatory["gdpr"] += 1
            if data.get("AI_Act_risque"): regulatory["ai_act"] += 1
        except (json.JSONDecodeError, ValueError, TypeError) as e:
            logger.warning(f"Erreur parsing extraction: {e}")
            continue
    
    analysis_result = {
        "total_studies": total_studies,
        "atn_metrics": {"empathy_analysis": {"mean_ai_empathy": np.mean(atn_metrics["empathy_scores_ai"]) if atn_metrics["empathy_scores_ai"] else None, "mean_human_empathy": np.mean(atn_metrics["empathy_scores_human"]) if atn_metrics["empathy_scores_human"] else None, "studies_with_empathy": len(atn_metrics["empathy_scores_ai"])}, "alliance_metrics": {"mean_wai_sr": np.mean(atn_metrics["wai_sr_scores"]) if atn_metrics["wai_sr_scores"] else None, "studies_with_wai": len(atn_metrics["wai_sr_scores"])}, "adherence_trust": {"adherence_rates": atn_metrics["adherence_rates"], "algorithmic_trust": atn_metrics["algorithmic_trust"], "acceptability": atn_metrics["acceptability_scores"]}},
        "technology_analysis": {"ai_types_distribution": ai_types_dist, "platforms_used": list(platforms), "most_common_ai_type": max(ai_types_dist.items(), key=lambda x: x[1])[0] if ai_types_dist else None},
        "ethical_regulatory": {"ethical_considerations_count": len(ethical), "gdpr_mentions": regulatory["gdpr"], "ai_act_mentions": regulatory["ai_act"], "regulatory_compliance_rate": (regulatory["gdpr"] / total_studies * 100) if total_studies > 0 else 0}
    }
    
    if ai_types_dist:
        project_dir, plot_path = PROJECTS_DIR / project_id, str(PROJECTS_DIR / project_id / 'atn_ai_types_distribution.png')
        project_dir.mkdir(exist_ok=True)
        fig, ax = plt.subplots(figsize=(10, 6))
        bars = ax.bar(list(ai_types_dist.keys()), list(ai_types_dist.values()), color=['#4CAF50', '#2196F3', '#FF9800', '#9C27B0', '#F44336'][:len(ai_types_dist)])
        ax.set_xlabel('Types d\'IA'); ax.set_ylabel('Nombre d\'√©tudes'); ax.set_title('Distribution des Types d\'IA dans les √âtudes ATN'); ax.tick_params(axis='x', rotation=45)
        for bar in bars: ax.text(bar.get_x() + bar.get_width()/2., bar.get_height(), f'{int(bar.get_height())}', ha='center', va='bottom')
        plt.tight_layout(); plt.savefig(plot_path, bbox_inches='tight'); plt.close(fig)
        analysis_result["plot_path"] = plot_path

    update_project_status(session, project_id, status='completed', analysis_result=analysis_result)
    session.commit()
    send_project_notification(project_id, 'atn_analysis_completed', 'Analyse ATN multipartie prenante termin√©e.')

@with_db_session  
def run_atn_score_task(session, project_id: str):
    """Calcule les scores ATN pour tous les articles extraits du projet."""
    logger.info(f"üìä Calcul des scores ATN pour le projet {project_id}")
    
    # Attendre que les extractions se terminent (max 2 minutes)
    max_wait = 120  # secondes
    wait_interval = 5
    waited = 0
    
    while waited < max_wait:
        extractions_count = session.execute(text("""
            SELECT COUNT(*) as total FROM extractions 
            WHERE project_id = :pid AND extracted_data IS NOT NULL
        """), {"pid": project_id}).mappings().fetchone()
        
        if extractions_count and extractions_count.get('total', 0) > 0:
            logger.info(f"‚úÖ Extractions trouv√©es - D√©marrage de l'analyse ATN")
            break
    try:
        # V√©rifier s'il y a des extractions √† analyser
        extractions = session.execute(text("""
            SELECT COUNT(*) as total, 
                   COUNT(CASE WHEN extracted_data IS NOT NULL THEN 1 END) as with_data
            FROM extractions 
            WHERE project_id = :pid
        """), {"pid": project_id}).mappings().fetchone()
        
        if not extractions or extractions.get('total', 0) == 0:
            logger.warning(f"Aucune extraction trouv√©e pour le projet {project_id}")
            send_project_notification(project_id, 'analysis_failed', 
                'Aucun article extrait pour calculer les scores ATN', {})
            return {"status": "skipped", "reason": "no_extractions"}
        
        # R√©cup√©rer toutes les extractions avec donn√©es
        rows = session.execute(text("""
            SELECT pmid, title, extracted_data, relevance_score
            FROM extractions 
            WHERE project_id = :pid AND extracted_data IS NOT NULL
        """), {"pid": project_id}).mappings().fetchall()
        
        if not rows:
            logger.warning(f"Aucune donn√©e extraite trouv√©e pour le projet {project_id}")
            send_project_notification(project_id, 'analysis_failed',
                'Aucune donn√©e extraite disponible pour l\'analyse ATN', {})
            return {"status": "skipped", "reason": "no_extracted_data"}
        
        # Calculer les statistiques ATN
        total_articles = len(rows)
        atn_scores = []
        empathy_scores = []
        trust_scores = []
        
        for row in rows:
            try:
                data = json.loads(row['extracted_data']) if isinstance(row['extracted_data'], str) else row['extracted_data']
                
                # Extraction des scores num√©riques
                if 'Score_empathie_IA' in data:
                    try:
                        score = float(str(data['Score_empathie_IA']).replace('%', '').replace(',', '.'))
                        if 0 <= score <= 100:
                            empathy_scores.append(score)
                    except (ValueError, TypeError):
                        pass
                
                if 'Confiance_algorithmique' in data:
                    try:
                        score = float(str(data['Confiance_algorithmique']).replace('%', '').replace(',', '.'))
                        if 0 <= score <= 100:
                            trust_scores.append(score)
                    except (ValueError, TypeError):
                        pass
                
                # Score composite bas√© sur la pertinence
                atn_scores.append(row.get('relevance_score', 0))
                
            except Exception as e:
                logger.warning(f"Erreur traitement extraction {row['pmid']}: {e}")
        
        # Calculs statistiques
        results = {
            'total_articles_analyzed': total_articles,
            'atn_mean_score': np.mean(atn_scores) if atn_scores else 0,
            'atn_std_score': np.std(atn_scores) if len(atn_scores) > 1 else 0,
            'empathy_scores': {
                'count': len(empathy_scores),
                'mean': np.mean(empathy_scores) if empathy_scores else None,
                'std': np.std(empathy_scores) if len(empathy_scores) > 1 else None,
                'min': np.min(empathy_scores) if empathy_scores else None,
                'max': np.max(empathy_scores) if empathy_scores else None
            },
            'trust_scores': {
                'count': len(trust_scores), 
                'mean': np.mean(trust_scores) if trust_scores else None,
                'std': np.std(trust_scores) if len(trust_scores) > 1 else None,
                'min': np.min(trust_scores) if trust_scores else None,
                'max': np.max(trust_scores) if trust_scores else None
            },
            'analysis_date': datetime.now().isoformat()
        }
        
        # Sauvegarde des r√©sultats
        session.execute(text("""
            INSERT INTO analyses (id, project_id, analysis_type, results, created_at)
            VALUES (:id, :pid, 'atn_scores', :results, :ts)
            ON CONFLICT (project_id, analysis_type) DO UPDATE SET
                results = EXCLUDED.results, created_at = EXCLUDED.created_at
        """), {
            "id": str(uuid.uuid4()),
            "pid": project_id, 
            "results": json.dumps(results),
            "ts": datetime.now().isoformat()
        })
        
        logger.info(f"‚úÖ Scores ATN calcul√©s pour {total_articles} articles du projet {project_id}")
        send_project_notification(project_id, 'analysis_completed', 
            f'Analyse ATN termin√©e ({total_articles} articles)', results)
        
        return {"status": "success", "results": results}
        
    except Exception as e:
        logger.exception(f"Erreur lors du calcul des scores ATN: {e}")
        send_project_notification(project_id, 'analysis_failed', 
            f'Erreur analyse ATN: {str(e)}', {})
        raise


# ================================================================ 
# === ANALYSE DU RISQUE DE BIAIS (RoB)
# ================================================================ 

@with_db_session
def run_risk_of_bias_task(session, project_id: str, article_id: str):
    """
    T√¢che pour √©valuer le risque de biais d'un article en utilisant l'IA.
    Ceci est une version simplifi√©e inspir√©e de RoB 2.
    """
    logger.info(f"√¢≈°‚Äì√Ø¬∏  Analyse RoB pour article {article_id} dans projet {project_id}")
    pdf_path = PROJECTS_DIR / project_id / f"{sanitize_filename(article_id)}.pdf"

    if not pdf_path.exists():
        send_project_notification(project_id, 'rob_failed', f"PDF non trouv√© pour {article_id}.")
        return



    text_content = extract_text_from_pdf(str(pdf_path))
    if not text_content or len(text_content) < 500:
        send_project_notification(project_id, 'rob_failed', f"Contenu insuffisant dans le PDF pour {article_id}.")
        return

    prompt = f"""
        En tant qu'expert en analyse critique de litt√©rature scientifique, √©valuez le risque de biais de l'article suivant.
        Concentrez-vous sur deux domaines cl√©s :
        1.  **Biais dans le processus de randomisation** : La m√©thode d'assignation des participants aux groupes √©tait-elle vraiment al√©atoire ? L'assignation √©tait-elle cach√©e (allocation concealment) ?
        2.  **Biais d√ª aux donn√©es manquantes** : Y a-t-il beaucoup de donn√©es manquantes ? Sont-elles g√©r√©es de mani√®re appropri√©e (ex: analyse en intention de traiter) ?

        Pour chaque domaine, fournissez une √©valuation ("Low risk", "Some concerns", "High risk") et une br√®ve justification bas√©e sur le texte.
        Enfin, donnez une √©valuation globale du risque de biais.

        R√©pondez UNIQUEMENT avec un objet JSON valide avec les cl√©s suivantes : "domain_1_bias", "domain_1_justification", "domain_2_bias", "domain_2_justification", "overall_bias", "overall_justification".

        TEXTE DE L'ARTICLE :
        ---
        {text_content[:15000]}

        ---
        """


    rob_data = call_ollama_api(prompt, model="llama3.1:8b", output_format="json")

    if not rob_data or not isinstance(rob_data, dict):
        raise ValueError("La r√©ponse de l'IA pour l'analyse RoB est invalide.")

    session.execute(text("""
        INSERT INTO risk_of_bias (id, project_id, pmid, article_id, domain_1_bias, domain_1_justification, domain_2_bias, domain_2_justification, overall_bias, overall_justification, created_at)
        VALUES (:id, :project_id, :pmid, :article_id, :d1b, :d1j, :d2b, :d2j, :ob, :oj, :ts)
        ON CONFLICT (project_id, article_id) DO UPDATE SET
            domain_1_bias = EXCLUDED.domain_1_bias, domain_1_justification = EXCLUDED.domain_1_justification,
            domain_2_bias = EXCLUDED.domain_2_bias, domain_2_justification = EXCLUDED.domain_2_justification,
            overall_bias = EXCLUDED.overall_bias, overall_justification = EXCLUDED.overall_justification;
        """), {
            "id": str(uuid.uuid4()), "project_id": project_id, "pmid": article_id,
            "article_id": article_id, "d1b": rob_data.get("domain_1_bias", "N/A"),
            "d1j": rob_data.get("domain_1_justification", "N/A"), "d2b": rob_data.get("domain_2_bias", "N/A"),
            "d2j": rob_data.get("domain_2_justification", "N/A"), "ob": rob_data.get("overall_bias", "N/A"),
            "oj": rob_data.get("overall_justification", "N/A"), "ts": datetime.now().isoformat()
        })
    
    send_project_notification(project_id, 'rob_completed', f"Analyse RoB termin√©e pour {article_id}.")

# ================================================================ 
# === T√ÇCHE POUR AJOUT MANUEL D'ARTICLES (ASYNCHRONE)
# ================================================================ 

@with_db_session
def add_manual_articles_task(session, project_id: str, identifiers: list):
    """
    T√¢che d'arri√®re-plan pour ajouter des articles manuellement.

    """

    logger.info(f"√∞≈∏‚Äú  Ajout manuel d'articles pour le projet {project_id}")
    if not identifiers:
        logger.warning(f"Aucun identifiant fourni pour le projet {project_id}.")
        return

    records_to_insert = []
    for article_id in identifiers:
        try:

            details = fetch_article_details(article_id)
        except Exception as e:
            logger.warning(f"Impossible de r√©cup√©rer les d√©tails pour {article_id}: {e}")
            continue
        try:
            exists = session.execute(text("SELECT 1 FROM search_results WHERE project_id = :pid AND article_id = :aid"), {"pid": project_id, "aid": details.get('id') or article_id}).fetchone()
            if exists:

                continue
            
            records_to_insert.append({
                "id": str(uuid.uuid4()), "pid": project_id, "aid": details.get('id') or article_id,
                "title": details.get('title', '') or f"Article {article_id}", "abstract": details.get('abstract', '') or '',
                "authors": details.get('authors', '') or '', "pub_date": details.get('publication_date', '') or '',
                "journal": details.get('journal', '') or '', "doi": details.get('doi', '') or '',
                "url": details.get('url', '') or '', "src": details.get('database_source', 'manual'),
                "ts": datetime.now().isoformat()
            })

        except Exception as e:
            logger.warning(f"Ajout manuel ignor√© pour {article_id}: {e}")
            continue
    
    if records_to_insert:
        session.execute(text("""
            INSERT INTO search_results (id, project_id, article_id, title, abstract, authors, publication_date, journal, doi, url, database_source, created_at)
            VALUES (:id, :pid, :aid, :title, :abstract, :authors, :pub_date, :journal, :doi, :url, :src, :ts)
        """), records_to_insert)



    successful_articles = [record['aid'] for record in records_to_insert]

    session.execute(text("UPDATE projects SET pmids_count = (SELECT COUNT(*) FROM search_results WHERE project_id = :pid), updated_at = :ts WHERE id = :pid"), {"pid": project_id, "ts": datetime.now().isoformat()})  # This line was fixed

    send_project_notification(project_id, 'import_completed', f'Ajout manuel termin√©: {len(records_to_insert)} article(s) ajout√©(s).') # This line was fixed

    logger.info(f"√¢≈ì‚Ä¶ Ajout manuel termin√© pour le projet {project_id}. {len(records_to_insert)} articles ajout√©s.") # This line was fixed
    # Lancer l'extraction automatique pour tous les articles ajout√©s
    project = session.query(Project).filter_by(id=project_id).first()
    profile_used = project.profile_used if project else 'standard'
    default_profile = {
        'preprocess': 'phi3:mini',
        'extract': 'llama3:8b', 
        'synthesis': 'llama3:8b'
    }

    # Optionnel: utiliser la config si disponible
    if hasattr(config, 'DEFAULT_MODELS') and config.DEFAULT_MODELS:
        available_models = list(config.DEFAULT_MODELS.keys())
        if available_models:
            model_key = available_models[0]  # Prendre le premier disponible
            config_profile = config.DEFAULT_MODELS[model_key]
            if isinstance(config_profile, dict):
                default_profile = normalize_profile(config_profile)

    logger.info(f"Lancement extraction automatique pour {len(successful_articles)} articles avec profile {default_profile}")
    for article_id in successful_articles:
        analysis_queue.enqueue(
            'backend.tasks_v4_complete.process_single_article_task',
            project_id=project_id, article_id=article_id,
            profile=default_profile, analysis_mode="full_extraction"
        )

@with_db_session
def import_from_zotero_json_task(session, project_id: str, items_list: list):
    """
    T√¢che asynchrone pour importer une LISTE d'objets JSON Zotero (envoy√©e par l'extension)
    et les convertir en SearchResult dans la base de donn√©es.
    """
    logger.info(f"Importation JSON Zotero (Extension) d√©marr√©e pour {project_id} : {len(items_list)} articles.")
    
    # 1. Utiliser la fonction de traitement pour nettoyer et d√©dupliquer
    processed_records = process_zotero_item_list(items_list)
    
    if not processed_records:
        msg = "Importation Zotero (Extension) termin√©e : Aucun nouvel article √† ajouter."
        send_project_notification(project_id, 'import_completed', msg)
        logger.info(msg)
        return

    # 2. Ins√©rer les enregistrements uniques dans la base de donn√©es
    records_to_insert = []
    new_articles = []
    for record in processed_records:
        article_id = record.get('article_id')
        if not article_id:
            continue
        
        existing = session.query(SearchResult).filter_by(project_id=project_id, article_id=article_id).first()
        if not existing:
            record.pop('zotero_key', None)
            record.pop('__hash', None)
            record['project_id'] = project_id
            new_articles.append(SearchResult(**record))
        else:
            # If article already exists, skip it for deduplication test
            logger.debug(f"Article existant ignor√© (d√©duplication) : {article_id}")
            continue # Skip to the next record

    if new_articles:
        session.add_all(new_articles)

    # Le commit est d√©j√† g√©r√© par le d√©corateur @with_db_session
    # session.commit() # Assurez-vous que ceci est hors de la boucle for

    # Le message de notification doit √™tre mis √† jour pour refl√©ter le nombre d'articles ajout√©s et mis √† jour.
    # Pour l'instant, on se concentre sur la correction du test.
    # Le test attend un message sp√©cifique, nous allons le construire.
    failed_imports = len(items_list) - len(processed_records)
    msg = f"Importation Zotero (Extension) termin√©e : {len(new_articles)} articles ajout√©s, {failed_imports} √©checs."
    send_project_notification(project_id, 'import_completed', msg)
    logger.info(msg)

def run_extension_task(session, project_id: str, extension_name: str):
    """
    Placeholder task for running a specific extension.
    """
    logger.info(f"üöÄ Ex√©cution de l'extension '{extension_name}' pour le projet {project_id}")
    # Here, you would add the actual logic for the extension
    # For now, it just logs and sends a notification
    send_project_notification(project_id, 'extension_completed', f"Extension '{extension_name}' ex√©cut√©e avec succ√®s.", {'extension_name': extension_name})

# ================================================================
# === REPORTING TASKS
# ================================================================

@with_db_session
def generate_bibliography_task(session, project_id: str):
    """
    G√©n√®re une bibliographie pour le projet sp√©cifi√© et la sauvegarde.
    """
    logger.info(f"√∞≈∏‚Äú≈° G√©n√©ration de la bibliographie pour le projet {project_id}")
    try:
        project = session.query(Project).filter_by(id=project_id).first()
        if not project:
            send_project_notification(project_id, 'report_failed', 'Projet non trouv√©.')
            return

        articles = session.query(SearchResult).filter_by(project_id=project_id).all()
        if not articles:
            send_project_notification(project_id, 'report_completed', 'Aucun article pour g√©n√©rer la bibliographie.')
            return

        bibliography_entries = []
        for article in articles:
            # Simple APA-like formatting for demonstration
            authors = article.authors.split(';')[0] if article.authors else 'N.A.'
            year = article.publication_date.split('-')[0] if article.publication_date else 'N.D.'
            title = article.title if article.title else 'Sans titre'
            journal = article.journal if article.journal else 'N.D.'
            doi = f"DOI: {article.doi}" if article.doi else ''
            url = article.url if article.url else ''

            bibliography_entries.append(
                f"{authors} ({year}). {title}. {journal}. {doi} {url}".strip()
            )

        bibliography_content = "\n\n".join(sorted(bibliography_entries))
        
        project_dir = PROJECTS_DIR / project_id
        project_dir.mkdir(exist_ok=True)
        file_path = project_dir / f"bibliography_{project_id}.txt"
        file_path.write_text(bibliography_content, encoding='utf-8')

        send_project_notification(project_id, 'report_completed', 'Bibliographie g√©n√©r√©e avec succ√®s.', {'report_path': str(file_path)})
        logger.info(f"√¢≈ì‚Ä¶ Bibliographie g√©n√©r√©e pour le projet {project_id} √† {file_path}")

    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration de la bibliographie pour le projet {project_id}: {e}", exc_info=True)
        send_project_notification(project_id, 'report_failed', f'Erreur lors de la g√©n√©ration de la bibliographie: {e}')

@with_db_session
def generate_summary_table_task(session, project_id: str):
    """
    G√©n√®re un tableau de synth√®se des extractions pour le projet sp√©cifi√© et le sauvegarde.
    """
    logger.info(f"√∞≈∏‚Äú≈° G√©n√©ration du tableau de synth√®se pour le projet {project_id}")
    try:
        project = session.query(Project).filter_by(id=project_id).first()
        if not project:
            send_project_notification(project_id, 'report_failed', 'Projet non trouv√©.')
            return

        # Fetch articles and their extractions
        results = (session.query(SearchResult, Extraction)
            .join(Extraction, SearchResult.article_id == Extraction.pmid)
            .filter(SearchResult.project_id == project_id, Extraction.project_id == project_id)
            .all()
        )

        if not results:
            send_project_notification(project_id, 'report_completed', 'Aucune donn√©e d\'extraction pour le tableau de synth√®se.')
            return

        summary_data = []
        for article, extraction in results:
            extracted_data = json.loads(extraction.extracted_data) if extraction.extracted_data else {}
            summary_data.append({
                "PMID": article.article_id,
                "Titre": article.title,
                "Auteurs": article.authors,
                "Ann√©e": article.publication_date.split('-')[0] if article.publication_date else 'N.D.',
                "Journal": article.journal,
                "Score de pertinence": extraction.relevance_score,
                "Justification pertinence": extraction.relevance_justification,
                **extracted_data # Include all extracted fields
            })
        
        # Convert to pandas DataFrame for easy handling
        df = pd.DataFrame(summary_data)
        
        project_dir = PROJECTS_DIR / project_id
        project_dir.mkdir(exist_ok=True)
        file_path = project_dir / f"summary_table_{project_id}.json" # Save as JSON for simplicity
        df.to_json(file_path, orient='records', indent=4, force_ascii=False)

        send_project_notification(project_id, 'report_completed', 'Tableau de synth√®se g√©n√©r√© avec succ√®s.', {'report_path': str(file_path)})
        logger.info(f"√¢≈ì‚Ä¶ Tableau de synth√®se g√©n√©r√© pour le projet {project_id} √† {file_path}")

    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du tableau de synth√®se pour le projet {project_id}: {e}", exc_info=True)
        send_project_notification(project_id, 'report_failed', f'Erreur lors de la g√©n√©ration du tableau de synth√®se: {e}')

@with_db_session
def export_excel_report_task(session, project_id: str):
    """
    Exporte toutes les donn√©es pertinentes du projet vers un fichier Excel.
    """
    logger.info(f"√∞≈∏‚Äú≈° Export Excel pour le projet {project_id}")
    try:
        project = session.query(Project).filter_by(id=project_id).first()
        if not project:
            send_project_notification(project_id, 'report_failed', 'Projet non trouv√©.')
            return

        # Fetch all search results
        search_results = session.query(SearchResult).filter_by(project_id=project_id).all()
        search_data = [{c.name: getattr(sr, c.name) for c in sr.__table__.columns} for sr in search_results]
        df_search = pd.DataFrame(search_data)

        # Fetch all extractions
        extractions = session.query(Extraction).filter_by(project_id=project_id).all()
        extraction_data = []
        for ext in extractions:
            row = {c.name: getattr(ext, c.name) for c in ext.__table__.columns}
            if ext.extracted_data:
                try:
                    # Flatten extracted_data JSON into top-level columns
                    flattened_data = json.loads(ext.extracted_data)
                    row.update(flattened_data)
                except json.JSONDecodeError:
                    logger.warning(f"Could not decode extracted_data for extraction ID {ext.id}")
            extraction_data.append(row)
        df_extractions = pd.DataFrame(extraction_data)

        # Fetch all risk of bias assessments
        rob_assessments = session.query(RiskOfBias).filter_by(project_id=project_id).all()
        rob_data = [{c.name: getattr(rob, c.name) for c in rob.__table__.columns} for rob in rob_assessments]
        df_rob = pd.DataFrame(rob_data)

        project_dir = PROJECTS_DIR / project_id
        project_dir.mkdir(exist_ok=True)
        file_path = project_dir / f"report_{project_id}.xlsx"

        with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:
            if not df_search.empty:
                df_search.to_excel(writer, sheet_name='Search Results', index=False)
            if not df_extractions.empty:
                df_extractions.to_excel(writer, sheet_name='Extractions', index=False)
            if not df_rob.empty:
                df_rob.to_excel(writer, sheet_name='Risk of Bias', index=False)
            
            # Optionally, merge dataframes for a combined view
            if not df_search.empty and not df_extractions.empty:
                df_combined = pd.merge(df_search, df_extractions, left_on=['project_id', 'article_id'], right_on=['project_id', 'pmid'], how='left', suffixes=('_search', '_extraction'))
                df_combined.to_excel(writer, sheet_name='Combined Data', index=False)


        send_project_notification(project_id, 'report_completed', 'Export Excel g√©n√©r√© avec succ√®s.', {'report_path': str(file_path)})
        logger.info(f"√¢≈ì‚Ä¶ Export Excel g√©n√©r√© pour le projet {project_id} √† {file_path}")

    except Exception as e:
        logger.error(f"Erreur lors de l'export Excel pour le projet {project_id}: {e}", exc_info=True)
        send_project_notification(project_id, 'report_failed', f'Erreur lors de l\'export Excel: {e}')

print(f"DEBUG: tasks_v4_complete.py loaded. run_extension_task is defined: {'run_extension_task' in globals()}")

@with_db_session
def run_atn_specialized_extraction_task(session, project_id: str, **kwargs):
    """Dummy task for ATN specialized extraction."""
    logger.info(f"Running ATN specialized extraction for project {project_id}")
    send_project_notification(project_id, 'analysis_completed', 'ATN specialized extraction completed.', {'analysis_type': 'atn_specialized_extraction'})

@with_db_session
def run_empathy_comparative_analysis_task(session, project_id: str, **kwargs):
    """Dummy task for empathy comparative analysis."""
    logger.info(f"Running empathy comparative analysis for project {project_id}")
    send_project_notification(project_id, 'analysis_completed', 'Empathy comparative analysis completed.', {'analysis_type': 'empathy_comparative_analysis'})
