Social Science & Medicine 338 (2023) 116357
Available online 4 November 2023
0277-9536/© 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
Multi-stakeholder preferences for the use of artificial intelligence in healthcare: A systematic review and thematic analysis
Vinh Vo a,*, Gang Chen a, Yves Saint James Aquino b, Stacy M. Carter b, Quynh Nga Do c,
Maame Esi Woode a,d
a Centre for Health Economics, Monash University, Australia
b Australian Centre for Health Engagement, Evidence and Values, School of Health and Soceity, University of Wollongong, Australia
c Department of Economics, Monash University, Australia
d Monash Data Futures Research Institute, Australia
ARTICLE INFO
Handling Editor: Prof. Richard Smith
JEL classification: I10 I18
Keywords:
Artificial intelligence Healthcare Health professional General public Patients
ABSTRACT
Introduction: Despite the proliferation of Artificial Intelligence (AI) technology over the last decade, clinician, patient, and public perceptions of its use in healthcare raise a number of ethical, legal and social questions. We systematically review the literature on attitudes towards the use of AI in healthcare from patients, the general public and health professionals’ perspectives to understand these issues from multiple perspectives. Methodology: A search for original research articles using qualitative, quantitative, and mixed methods published between 1 Jan 2001 to 24 Aug 2021 was conducted on six bibliographic databases. Data were extracted and classified into different themes representing views on: (i) knowledge and familiarity of AI, (ii) AI benefits, risks, and challenges, (iii) AI acceptability, (iv) AI development, (v) AI implementation, (vi) AI regulations, and (vii) Human – AI relationship. Results: The final search identified 7,490 different records of which 105 publications were selected based on predefined inclusion/exclusion criteria. While the majority of patients, the general public and health professionals generally had a positive attitude towards the use of AI in healthcare, all groups indicated some perceived risks and challenges. Commonly perceived risks included data privacy; reduced professional autonomy; algorithmic bias; healthcare inequities; and greater burnout to acquire AI-related skills. While patients had mixed opinions on whether healthcare workers suffer from job loss due to the use of AI, health professionals strongly indicated that AI would not be able to completely replace them in their professions. Both groups shared similar doubts about AI’s ability to deliver empathic care. The need for AI validation, transparency, explainability, and patient and clinical involvement in the development of AI was emphasised. To help successfully implement AI in health care, most participants envisioned that an investment in training and education campaigns was necessary, especially for health professionals. Lack of familiarity, lack of trust, and regulatory uncertainties were identified as factors hindering AI implementation. Regarding AI regulations, key themes included data access and data privacy. While the general public and patients exhibited a willingness to share anonymised data for AI development, there remained concerns about sharing data with insurance or technology companies. One key domain under this theme was the question of who should be held accountable in the case of adverse events arising from using AI. Conclusions: While overall positivity persists in attitudes and preferences toward AI use in healthcare, some prevalent problems require more attention. There is a need to go beyond addressing algorithm-related issues to look at the translation of legislation and guidelines into practice to ensure fairness, accountability, transparency, and ethics in AI.
* Corresponding author. Centre for Health Economics, Australia. E-mail address: vinh.vo@monash.edu (V. Vo).
Contents lists available at ScienceDirect
Social Science & Medicine
journal homepage: www.elsevier.com/locate/socscimed
https://doi.org/10.1016/j.socscimed.2023.116357 Received 28 April 2023; Received in revised form 4 September 2023; Accepted 24 October 2023


Social Science & Medicine 338 (2023) 116357
2
1. Introduction
Healthcare systems globally face massive issues regarding expenses, infrastructure, lack of access, an aging population, and new strains of diseases. This has been made more evident by the COVID-19 pandemic where there were clear-cut examples of how current healthcare systems lacked resources, had little information exchange, conducted insufficient or erroneous diagnostic tests, and exhausted frontline health professionals (Greenberg et al., 2020; Pavli et al., 2021). Other main challenges and unmet needs also include a rising trend of chronic diseases (OECD and European, 2020), lack of health personnel (Michel and Ecarnot, 2020), inefficiency (Behr and Theune, 2017), problems in sustainability (Shelton et al., 2018), and healthcare inequities (Goodyear-Smith and Ashton, 2019; Ortega and Roby, 2021). Artificial intelligence (AI), refers to a set of interconnected technologies that are utilized to tackle problems that would otherwise necessitate human cognitive abilities. These technologies are designed to learn from their experiences, adapt to new inputs, and perform a variety of tasks, from making predictions, recommendations or decisions with varying levels of autonomy (Organization for Economic Co-operation and Development, 2019). In the 1990s, the development of AI models shifted. Newer so-called connectionist models, such as decision trees, k-means algorithms and artificial neural networks, allowed for a shift from a knowledge-driven approach to a data-driven approach. Instead of coding the machine by hand, the developer would train the machine on examples or experiences to make inferences from new data. This process is called machine learning (ML). Since 2010, a new subfield of machine learning called deep learning has been gaining traction. Deep learning models consist of multilayered neural networks. In contrast to simpler neural networks, they do not require the developer to design the input features that are relevant to the problem at hand. In healthcare, AI promises to improve the efficiency and accuracy of various healthcare services. This can include the use of machine learning algorithms to analyze large amounts of medical data, natural language processing to improve the accuracy of medical diagnoses, and robotic systems to assist with surgical procedures (Davenport and Kalakota, 2019). Proponents of AI in healthcare claim that it will improve patient outcomes, reduce the workload of healthcare professionals, and make medical care more accessible and affordable. AI-enhanced wearable technology is used for patient health monitoring, and AI-powered chatbots are used for telemedicine. Natural language processing is used in mental health screening, and AI is used in assistive technologies for the elderly. These technologies are based on real-world data from clinical trials (Stewart et al., 2018; Ting et al., 2018; Topol, 2019). However, studies show that quality assessment tools are not fully used in reviews of diagnostic accuracy studies that use artificial intelligence, and it also reveals inconsistent reporting across all areas of quality assessment, leading to barriers in clinical implementation. There are both international and national recommendations on strategies to mitigate unwanted consequences arising from AI. In Europe, a recently published report by the European Parliament explains the areas in which AI can contribute to the medical and healthcare field (European Parliament, 2022). It highlights the most significant risks relating to its application in this high-stakes and fast-changing field, and presents policy options to counteract these risks, in order to optimise the use of biomedical AI. In the United States, the Department of Health and Human Services drew up an AI strategy which provides direction and guidance to lead advances in the health and wellbeing of the population, respond to the use of AI across the health and human services ecosystem, and scale-up trustworthy AI adoption (U.S. Department of Health and Human Services, 2021). In Australia, the Roadmap for Artificial Intelligence in Healthcare for Australia was released in December 2021 and recommended priority areas where AI use is expected to be safe for patients and could be
developed and used ethically (Australian Alliance for Artificial Intelligence in Healthcare, 2021). Nevertheless, there are challenges in fully realizing the potential of AI in healthcare. Previous instances of disparities between reality and expectations have resulted in significant declines in the adoption of this technology, and there remains a possibility of such an occurrence, particularly within the healthcare sector (Shaw et al., 2019). Not only does using AI require retraining of the workforce and retooling health services (Coiera et al., 2023), but the deployment of AI in itself brings with it a mixed array of challenges from legal, ethical, and social perspectives. This is because potential risks and harms such as automation bias, lack of clarity about what is required for effective regulations, products with minimal clinical input, unbalanced workforce supply/demand, or unclear accountability are pertinent to any technology and need to be evaluated against the benefits (Bartoletti, 2019; Carter et al., 2020; Rajpurkar et al., 2022). While AI is promising for healthcare applications and such guidelines have delineated strategic plans for AI use in healthcare, there is a need to understand providers’ and users’ perspectives and perceptions of AI in the delivery of healthcare, clinician, patient, and public voice needs to be taken into account for safe and equitable use of AI in health care. A previous systematic review investigating the perspectives of patients and members of the public provided valuable findings for AI in clinical practice (Young et al., 2021). It looked at the narrower scope of AI used for the diagnosis and/or treatment that might be directed towards patients, caregivers, health-care providers, or a combination. Three other reviews including one integrative review (Shinners et al., 2020), one exploratory analysis (Scott et al., 2021), and one scoping review (Chew and Achananuparp, 2022) also explored perceptions of either health professionals, patients or general stakeholders. To date, there has not been a systematic review that analyzes and synthesizes the results from all key stakeholders including health professionals, patients and members of the general public. Our mixed research synthesis is intended to close this gap and deepen the understanding of their views on AI use in healthcare. The experience gained in various applications can provide important evidence to inform guidelines and policies to ensure a safe and equitable health system. Our systematic review contributes significantly to the discussion of the perspectives of multiple stakeholders by providing a comprehensive understanding of where patients, members of the general public and health professionals converge or diverge on their opinions of a variety of AI use in health care.
2. Methodology
We conducted a systematic review following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines (Page et al., 2021). The protocol has been registered on PROSPERO (registration ID: CRD42021277230). The study used the Mixed Methods Appraisal Tool (2018) (Hong et al., 2017) to investigate the quality of the evidence. It also used a data-based convergent synthesis design to analyze included studies using thematic synthesis.
2.1. Search strategy
Six bibliographic databases were systematically searched for all relevant studies from January 2001 to August 2021: Scopus, CINAHL, PubMed, Medline, Embase, and PsychInfo. Grey literature included search engines: Bing, DuckDuckGo, Google, and Yahoo; targeted websites: Australian Institute of Health and Welfare (AIHW), Analysis and Policy Observatory (APO), Australia Department of Health, the National Institute for Health and Care Excellence (NICE), RAND Corporation, World Bank and the World Health Organization; and thesis databases: ProQuest, EthOS, DART-Europe, Trove and American Doctoral Dissertations. Grey literature was also included. The search strategy for grey
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
3
literature in this systematic review aimed to incorporate three different sources: (1) dissertations and theses, (2) search engines, and (3) targeted sites. The first source involved databases on dissertations and theses recorded particularly in Australia, the UK, the US, and Europe. The second source involved search engines (Google, Bing, Yahoo, and DuckDuckGo) that indexed and made available documents published on the Internet. The first 50 results appearing for each search engine were screened. The third source involved looking through relevant targeted websites of organisations and agencies operating in the field of artificial intelligence and health care. The list of these sources was obtained from Monash University’s Library guide for grey literature (Monash University, 2022). Three sets of keywords for the search strategy were generated, including (i) keywords specific to preferences: attitudes, preferences, views, opinions, experiences, perceptions, perspectives, (ii) keywords specific to AI: artificial intelligence, machine learning, deep learning, neural networks, and (iii) keywords specific to research subjects and context (health): patients, members of the public, health professionals. The detailed search terms and results for each bibliographic database and grey literature are presented in Supplementary 1 – Appendix A-B.
2.2. Inclusion criteria
The included studies had to be 1) published from 1st January 2001 to
14th August 2021, 2) involve human participants, 3) written in English, with 4) preferences explicitly stated by patients, health professionals, or members of the general public relating to a service where the primary purpose is any area of AI for health-focused applications, including but not limited to machine learning, deep learning or neural networks explicitly presented to participants. Definition of health professionals followed Sub-major groups 22, 3142, and 32 Life science and health professionals in the International Standard Classification of Occupations-88 (World Health Organisation, 2010). This group includes medical doctors (generalist medical practitioners, specialist medical practitioners), nursing and midwifery professionals, allied health professionals (dentists, pharmacists, environmental and occupational health and hygiene professionals, physiotherapists, dieticians and nutritionists, allied health professionals not elsewhere classified), and medical laboratory technicians, with an extended inclusion of medical students. We also included studies that sampled a mixture of our participants of interest (patients, public, and health professionals) in addition to other populations that were not of interest. We chose to focus on quantitative and qualitative studies to understand the breadth of preferences that different groups may have for AI applications in healthcare.
2.3. Exclusion criteria
Abstracts from conference proceedings, protocols, and reviews were excluded, as were editorials, opinion pieces, and commentaries. In addition, articles that did not directly discuss or mention artificial intelligence in general or AI-related algorithms to participants were excluded as were studies that sampled participants only based on their role as industry or regulatory representatives.
2.4. Outcome measures
The primary outcomes were quantitative and qualitative measurements of preferences, views, opinions, perceptions, attitudes, or experiences regarding AI use or applications in healthcare, through choice studies, surveys, qualitative studies, tool development or validation studies.
2.5. Study selection
To identify the relevant studies, two reviewers (VV and QND)
independently reviewed all titles and abstracts generated from the bibliographic database. Using Covidence as an online systematic review program, screening for title, abstract and full text was conducted. Articles were rejected on initial screening if the reviewer could determine that the article did not meet the inclusion criteria or did meet any of the exclusion criteria. If abstracts were not available or unable to provide sufficient exclusion information, the entire article was transferred to the full-text screening stage. The discrepancies in the title and abstract screening were identified and resolved through discussion or deliberation with two other co-authors (GC and MEW).
2.6. Data extraction
The data extraction spreadsheet was created by the first (VV) and second (QND) authors, with the first author completing the initial round of data extraction from a set of peer-reviewed articles and grey literature. Each article’s extracted information included details such as the year and first author of the publication, country, health area, study aim, design, sampling method, AI application category, participant type, sample characteristics, and key findings. The second author then rigorously reviewed the included articles to ensure the accuracy and appropriateness of the extracted data, with any discrepancies discussed and resolved through consensus. Both authors worked together to finalize the data extraction spreadsheet, which can be found in Supplementary 2 for further study design details.
2.7. Data synthesis and analysis
In accordance with the data-based convergent synthesis design (Hong et al., 2017) and Guidance on the Conduct of Narrative Synthesis reviews (ESRC guidance) (Popay et al., 2006), quantitative data was translated into codes. This design was also recommended by the updated Joanna Briggs Institute methodological guidance for conducting a mixed methods systematic review, which involves data transformation and allows reviewers to combine quantitative and qualitative data (Stern et al., 2020). In this design, all included studies are analysed using the same synthesis method and results are presented together. Since only one synthesis method is used for all evidence, data transformation is involved (e.g., qualitative data transformed into numerical values or quantitative data transformed into categories/themes) to ensure its consistency. By analysing the percentages of participants endorsing themes across various studies, we grouped each one into three categories: ’low’ (endorsed by less than 10% of participants), ’medium’ (endorsed by 10% to less than 50% of participants), and ’high’ (endorsed by more than 50% of participants). To minimise the impact of coded themes reported by only a small minority (<10%) of participants, coded themes categorised as ‘low’ frequency were not included in subsequent analyses. Within this design, our main goal was to describe the findings of the included studies by identifying and defining the main themes using thematic analysis. This is helpful when exploring the main barriers and facilitators to the development and implementation of AI use in health care. Results or interpretations from quantitative or mixed methods studies were included. The review also involved an iterative process of reading, re-reading, and subsequently coding relevant text from the results of each article or report. The thematic synthesis followed methods for the thematic synthesis of qualitative research in systematic reviews (Thomas and Harden, 2008) as it allowed for transparent summarizing of existing qualitative research evidence. Data were entered into NVivo 12 data analysis software to facilitate analysis. We built upon a pre-constructed codebook built from the paper “Patient Perspectives on the Use of Artificial Intelligence for Skin Cancer Screening: A Qualitative Study” (Nelson et al., 2020) by adding new concepts to the coding to develop more complete descriptive themes. During the first stage, author VV read and re-read through each publication until an acceptable level of acquaintance was achieved. Each
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
4
section of the results of the article or report was coded individually line by line. Each article was revisited after coding to ensure the consistency of the codes generated. In the second phase, QND investigated the differences and similarities between codes to modify the codebook where applicable. In the third phase, both VV and QND integrated these descriptive themes into a synthesis of findings, which led to analytical themes. The frequency and pertinence of these codes were the decisive factors in determining how these analytical topics were established. Routine meetings and discussions between the two researchers led to some consensus on the final themes. A meta-analysis was not possible due to considerable heterogeneity between the quantitative studies which sampled different populations and used different surveys (Ioannidis et al., 2008).
2.8. Critical appraisal
VV and QND independently evaluated each study using the Mixed Methods Appraisal Tool (MMAT) (Hong et al., 2019). The tool is designed for appraising the quality of peer-reviewed studies. It was developed in 2006 and revised in 2018 upon a review of the tools, interviews with MMAT users, and an e-Delphi study with international experts to validate the usage of this tool. We closely followed MMAT guidelines to provide a detailed presentation of ratings of each criterion, rather than excluding studies with low methodological quality. The checklist developed by Flinders University: AACODS (Authority,
Accuracy, Coverage, Objectivity, Date, Significance) to appraise the quality of grey literature was followed (Tyndall, 2010). Prior to inclusion in the review, papers selected for data extraction were evaluated by one reviewer (VV). The next reviewer (QND) reviewed a quarter of the studies to check for consistency. Disagreements were resolved with GC and MEW until a consensus was reached.
3. Results
3.1. Screening
A total of 7,290 references were retrieved from six bibliographic databases, with 5,827 studies left after duplicates were removed. There were 247 studies eligible for full-text screening after excluding 5,580 studies based on the title and abstract screening criteria. A total of 91 studies met the inclusion criteria and hence were included in the final data extraction, critical appraisal, and thematic synthesis. For grey literature references, 18 references were included in the full-text screening from 200 references retrieved. 14 references fulfilled the inclusion criteria and were added to the final data extraction, critical appraisal and thematic synthesis. Fig. 1 presents details of the screening process.
Fig. 1. Preferred Reporting Items for Systematic reviews and Meta-Analyses flow diagram (PRISMA).
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
5
3.2. Description of included studies
There were 56 studies investigating health professionals’ perceptions and 55 publications investigating the views of patients and the general public (Table 1). For the health professionals, participants were from a wide range of specialities including radiology (Botwe et al., 2021a, 2021b; Codari et al., 2019; Coppola et al., 2021; Eltorai et al., 2020; Jungmann et al., 2021; Sit et al., 2020; Strohm et al., 2020; Waymel et al., 2019; Yurdaisik and Aksoy, 2021), surgery (Collins et al., 2021; Layard Horsfall et al., 2021; Staartjes et al., 2020), dentistry (Abouzeid et al., 2021; Yuzbasioglu, 2021) and specific health conditions including mental health, (Benrimoh et al., 2021; Blease et al., 2021; Doraiswamy et al., 2020; Thenral and Annamalai, 2021) cancer (Hendrix et al., 2021; Wong et al., 2021), and diabetes (Romero-Brufau et al., 2020a; Scheetz et al., 2021b). The rest of the studies reported views of health professionals on general applications of AI in health care (Abdullah and Fakieh, 2020; Ardon and Schmidt, 2020; Banerjee et al., 2021; Castagno and Khalifa, 2020; Diprose et al., 2020; Fan et al., 2020; Gillan et al., 2019; Goss
et al., 2019; Kocaballi et al., 2020; Laï et al., 2020; Oh et al., 2019; Palanica et al., 2019; Pinto dos Santos et al., 2019; Romero-Brufau et al., 2020b; Ryan et al., 2021; Samuel et al., 2021; Shen et al., 2020). A majority of studies (67%) are quantitative (Table 1). An increasing number of studies were published between 2017 and 2021 and no relevant studies published before 2017 met the inclusion criteria for this review. Among the uses of AI, studies referring to “diagnosis and treatment applications” accounted for nearly 65% of included studies. More than 75% of the included studies only used questionnaires/interviews without directly engaging participants with either simulated AI applications or hypothetical scenarios related to AI use. The complete data extraction table is described in Supplementary 2. High-income countries (World Bank, 2022) accounted for 80% of the included studies, with the U.S. leading the list, followed by European countries, as depicted in Fig. 2. Despite contributing to a minor proportion, studies from China, Ghana, and India may indicate that lowand middle-income countries are now commencing to explore how different stakeholders view AI use in healthcare. The quality of the methods of the included studies was mixed (Table 2). All quantitative, qualitative, and mixed method studies sufficiently answered the screening questions: i) Are there clear research questions? and ii) Do the collected data allow us to address the research questions? Detailed quality assessment for quantitative, qualitative studies and grey literature is included in Supplementary 1 – Appendix C.
Most of the quantitative studies had a high or unclear risk of selection bias. Around 75% of quantitative studies did not disclose sampling strategies relevant to addressing the research questions. Among the included studies stating the sampling method, the majority of them were non-probabilistic. Methods used included convenience sampling, purposive sampling, snowball sampling, and voluntary sampling. Across all assessment criteria, nearly 100% of included studies obtained appropriate measurements and statistical analysis to answer the research question. More than 60% of qualitative studies included representative quotes to illustrate participants’ views or attitudes expressed towards AI use in healthcare. 100% of these studies applied appropriate approaches to answer the research question and their data collection methods are adequate to address the research question. Mixed-methods studies provided adequate rationale for using this method to enhance or build upon qualitative findings with quantitative results and vice versa and provide a comprehensive and complete understanding of subjects of interest.
3.3. Thematic synthesis
We identified 9 analytical themes related to views, attitudes, and preferences of health professionals, patients, and the general public for AI use in healthcare. Descriptive themes arising from these analytical themes are discussed. A detailed codebook and coding are presented in Supplementary 1 – Appendix D. Although some descriptive themes can be assigned to several analytical themes (not mutually exclusive), we categorise them into the most relevant themes through an iterative process of reading, re-reading, and subsequently coding relevant text from the results of each article or report. A summary table with heatmap color for frequencies of appearance of all detailed themes and subthemes is included in Supplementary 3 (Thematic coding).
3.3.1. Perceived knowledge and familiarity of AI (n = 18)
This theme relates to knowledge and familiarity with AI. Both patients and the general public linked AI to cognition (game playing) (Nelson et al., 2020), machines (calculator, chatbots, Google, robots) (Haan et al., 2019; Nadarzynski et al., 2019; Ongena et al., 2020; Palmisciano et al., 2020) or fear (Adams et al., 2020; Gao et al., 2020; Lennox-Chhugani et al., 2021; Palmisciano et al., 2020). Among these groups, participants from three studies also expressed no or limited
Table 1
Characteristics of included studies.
Category n %
Study type
Quantitative 70 67% Qualitative 21 20% Mixed methods 14 13% Health focus and specialties
Radiology 14 13% Surgery 9 9% Mental health 5 5% Cancer 11 10% Others 66 63% Country
United States of America 44 38% United Kingdom 20 17% China 8 7% Canada 6 5% Germany 5 4% France 4 3% Netherlands 4 3% Australia 4 3% Switzerland 4 3% Saudi Arabia 4 3% South Korea 3 3% Austria 2 2% Ghana 2 2% Turkey 2 2% Italy 1 1% India 1 1% New Zealand 1 1% Taiwan 1 1% Year of publication
2017 1 1% 2018 5 5% 2019 18 17% 2020 39 37% 2021 42 40%
Stakeholders representeda
Health professionals 56 53% Members of public 35 33% Patients 20 19% Use of AI
Diagnosis and treatment applications 67 64% Administrative applications 9 9% Patient engagement and adherence applications 5 5% Not reported 24 23% Exposure to AI
Simulated AI applications 11 10% Hypothetical scenarios 13 12% No simulated AI applications/hypothetical scenarios 81 77%
a There are studies exploring both health professionals and members of the public or patients, which makes the total percentage higher than 100%.
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
6
knowledge of AI (Aggarwal et al., 2021; Nelson et al., 2020; Palmisciano et al., 2020). Health professionals from 11 studies (60%) also expressed their lack of experience with or exposure to AI (Abdullah and Fakieh, 2020; Abouzeid et al., 2021; Banerjee et al., 2021; Castagno and Khalifa, 2020; Codari et al., 2019; Sandhu et al., 2020; Scheetz et al., 2021a; Sit et al., 2020; Waymel et al., 2019; Wong et al., 2021; Yuzbasioglu, 2021) and respondents from two studies showed their concerns on limited understanding of how AI works (Academy of Medical Royal Colleges, 2019; Thenral and Annamalai, 2021). Despite being aware of emerging trends in AI use, participants from three studies were concerned that AI is flawed and its capability has been exaggerated by industry and media with small impacts in this area (Academy of Medical Royal Colleges, 2019; Laï et al., 2020; Samuel et al., 2021).
3.3.2. Perceived benefits of AI (n = 49)
Patients and the general public shared some similar expectations for AI benefits including test accuracy (n = 9) (Abdoul et al., 2021; Adams et al., 2020; Gao et al., 2020; Jutzi et al., 2020; Liu et al., 2021b; Nelson et al., 2020; Tran et al., 2019; Xiang et al., 2020; Yang et al., 2019), medical error reduction (n = 5) (Jutzi et al., 2020; Lennox-Chhugani et al., 2021; Ongena et al., 2020; Tran et al., 2019; Yang et al., 2019), and reduced workload for health professionals (n = 5) (Adams et al., 2020; Gao et al., 2020; Jutzi et al., 2020; Nelson et al., 2020; Tran et al., 2019). They also hoped that AI might lower health expenses (n = 3) (Haan et al., 2019; Nelson et al., 2020; Yang et al., 2019), increase healthcare access for people from regional areas or those with limited mobility (n = 3) (Meyer et al., 2020; Nelson et al., 2020; Tran et al., 2019), and reduce waiting and traveling times for patients (n = 6) (Adams et al., 2020; Haan et al., 2019; Jutzi et al., 2020; Nadarzynski et al., 2019; Ongena et al., 2020; van der Veer et al., 2021) (Table 3A). Health professionals expected that the most value-added benefits from AI use are efficiency which includes clinical and non-clinical workload reduction (n = 18, 37%) (Academy of Medical Royal Colleges, 2019; Banerjee et al., 2021; Blease et al., 2019, 2021; Codari et al., 2019; Doraiswamy et al., 2020; EIT Health, 2020a; Goss et al., 2019; Hayes, 2020; Kocaballi et al., 2020; Laï et al., 2020; Lee et al., 2020; Liang HF et al., 2019; Palanica et al., 2019; Samuel et al., 2021; Scheetz et al., 2021a; Strohm et al., 2020; Xiang et al., 2020), time savings (n =
12, 25%) (Academy of Medical Royal Colleges, 2019; Ardon and Schmidt, 2020; Goetz et al., 2020; Goss et al., 2019; Hardie et al., 2021; Jauk et al., 2021; Laï et al., 2020; Oh et al., 2019; Strohm et al., 2020; Waymel et al., 2019; Wong et al., 2021; Yurdaisik and Aksoy, 2021), and workflow efficiency improvement (n = 5) (Adams et al., 2020; Jutzi et al., 2020; Nelson et al., 2020; Strohm et al., 2020; Tran et al., 2019). Health professionals also perceived another primary benefit of AI to be the enhancement of medical capabilities because AI helps reduce medical errors (n = 10) (Abdullah and Fakieh, 2020; Ardon and Schmidt, 2020; Coppola et al., 2021; Doraiswamy et al., 2020; Goetz et al., 2020; Goss et al., 2019; Hui et al., 2021; Oh et al., 2019; Strohm et al., 2020; Waymel et al., 2019) by increasing test accuracy (n = 9) (Abouzeid et al., 2021; Botwe et al., 2021b; Coppola et al., 2021; Hardie et al., 2021; Layard Horsfall et al., 2021; Oh et al., 2019; Shen et al., 2020; Strohm et al., 2020; Walter et al., 2020), sharpen quality of clinical skills (n = 9) (Abdullah and Fakieh, 2020; Australian Academy of Health and Medical Sciences, 2020; Banerjee et al., 2021; Codari et al., 2019; Gillan et al., 2019; Laï et al., 2020; Oh et al., 2019; Paranjape et al., 2021; Scheetz et al., 2021b), improve risk detection (n = 8) (Australian Academy of Health and Medical Sciences, 2020; Blease et al., 2021; Hardie et al., 2021; Paranjape et al., 2021; Pinto dos Santos et al., 2019; Romero-Brufau et al., 2020a; Sandhu et al., 2020; Staartjes et al., 2020), make better decisions with information capacity (n = 7) (Academy of Medical Royal Colleges, 2019; Blease et al., 2021; Botwe et al., 2021b; Doraiswamy et al., 2020; Goetz et al., 2020; Jauk et al., 2021; Scheetz et al., 2021a), and enhance recommendation systems (n = 6) (Australian Academy of Health and Medical Sciences, 2020; Benrimoh et al., 2021; Blease et al., 2021; Coppola et al., 2021; Oh et al., 2019; Staartjes et al., 2020) derived from AI. Health professionals also expressed a belief that AI could improve patient-clinician relationship by enabling clinicians to spend more time providing greater care to patients (n = 9) (Banerjee et al., 2021; Blease et al., 2021; Codari et al., 2019; EIT Health, 2020a; Gillan et al., 2019; Goss et al., 2019; Liang HF et al., 2019; Samuel et al., 2021; Waymel et al., 2019). Health professionals perceived the benefits that AI could provide to patients, which are similar to what patients and members of the public expected from AI. In particular, they believed AI helps patients better understand their health situation (n = 5) (Antes et al., 2021; Hui et al., 2021; Meyer et al., 2020; Nelson et al., 2020; Zhang et al., 2021a)
Fig. 2. Distribution of included studies in countries.
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
7
while making the interaction between health professionals and patients feel less judgmental or personal (n = 4) (Meyer et al., 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Ongena et al., 2020). For example, virtual therapists are less likely to have a bad day or feel antipathy toward the patient or patients may feel comfortable disclosing more information to chatbots compared with human doctors (Table 3B).
3.3.3. Perceived AI risks (n = 47)
Patients, general public and health professionals often expressed data privacy concerns (n = 18, 38%) (Ardon and Schmidt, 2020; Australian Academy of Health and Medical Sciences, 2020; Botwe et al.,
2021b; Castagno and Khalifa, 2020; Doraiswamy et al., 2020; Esmaeilzadeh, 2020; Gao et al., 2020; Girosi et al., 2021; Layard Horsfall et al., 2021; Lennox-Chhugani et al., 2021; Liang HF et al., 2019; McCradden et al., 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Ongena et al., 2020; Ryan et al., 2021; Thenral and Annamalai, 2021; Yarborough and Stumbo, 2021). This included concern about risks of having patient health records hacked or leaked with AI use (n = 8) (Academy of Medical Royal Colleges, 2019; Blease et al., 2021; Jutzi et al., 2020; Kocaballi et al., 2020; Thenral and Annamalai, 2021; Tran et al., 2019; Wong et al., 2021; Zhang et al., 2021a). Moreover, use of patient data for unwarranted commercial purposes is of paramount concern to both groups (n = 9) (Blease et al., 2021; Botwe et al., 2021b, 2021a; Jutzi et al., 2020; McCradden et al., 2020; Nelson et al., 2020; Palmisciano et al., 2020; Select Committee on Artificial Intelligence, 2018; Tran et al., 2019). Patients and general public perceived that AI has a risk of increasing healthcare disparities, which could create fragmentation and polarisation within communities (n = 6) (Antes et al., 2021; Australian Academy of Health and Medical Sciences, 2020; Jutzi et al., 2020; Nelson et al., 2020; Thenral and Annamalai, 2021; Tran et al., 2019) where there are significant differences in age, educational attainment, socioeconomic status, and residential locations (n = 3) (Academy of Medical Royal Colleges, 2019; Australian Academy of Health and Medical Sciences, 2020; Blease et al., 2021) (Table 4A). While patients had mixed opinions on whether healthcare workers suffer from job loss due to the introduction of AI (n = 6) (Gao et al., 2020; Lee et al., 2020; Lennox-Chhugani et al., 2021; McCradden et al.,
Table 2
Summary of quality assessment for included studies.
(i) Criteria for qualitative studies Assessment Proportion of included studies meeting this assessment
1.1. Is the qualitative approach appropriate to answer the research question?
Y 100% N 0% C 0% 1.2. Are the qualitative data collection methods adequate to address the research question?
Y 100% N 0% C 0% 1.3. Are the findings adequately derived from the data?
Y 81% N 19% C 0% 1.4. Is the interpretation of results sufficiently substantiated by data?
Y 62% N 33% C 5% 1.5. Is there coherence between qualitative data sources, collection, analysis and interpretation?
Y 86% N 14% C 0%
(ii) Criteria for quantitative studies Assessment Proportion of included studies meeting this assessment
4.1. Is the sampling strategy relevant to address the research question?
Y 25% N 0% C 75% 4.2. Is the sample representative of the target population?
Y 12% N 38% C 50% 4.3. Are the measurements appropriate?
Y 100% N 0% C 0% 4.4. Is the risk of nonresponse bias low?
Y 19% N 22% C 59% 4.5. Is the statistical analysis appropriate to answer the research question?
Y 97% N 3% C 0%
(iii) Criteria for mixed methods studies
Assessment Proportion of included studies meeting this assessment
5.1. Is there an adequate rationale for using a mixed methods design to address the research question?
Y 100% N 0% C 0% 5.2. Are the different components of the study effectively integrated to answer the research question?
Y 100% N 0% C 0% 5.3. Are the outputs of the integration of qualitative and quantitative components adequately interpreted?
Y 93% N 7% C 0%
5.4. Are divergences and inconsistencies between quantitative and qualitative results adequately addressed?
Y 100% N 0% C 0%
5.5. Do the different components of the study adhere to the quality criteria of each tradition of the methods involved?
Y 14% N 86% C 0%
Y = Yes, N––No, C = Can’t tell.
Table 3A
AI benefits – Patients and members of general public.
Themes Analytical themes References
AI benefits For health workers: (Abdoul et al., 2021; Adams et al., 2020; Antes et al., 2021; Gao et al., 2020; Hui et al., 2021; Jutzi et al., 2020; Lennox-Chhugani et al., 2021; Liu et al., 2021b; Meyer et al., 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Ongena et al., 2020, 2020, 2020; Tran et al., 2019, 2019, 2019; van der Veer et al., 2021; Xiang et al., 2020; Yang et al., 2019),
Increase efficiency Increase labour efficiency Address workforce shortage Reduce workload Increase medical capabilities For patients: AI makes the interaction with patients feel less personal Better manage own health Better understanding healthcare situation Book appointments Help patients prepare well prior to physician consultation Improve patient safety Increase compliance with treatment regimes Increase triage efficiency Increased quality of care Lower health expenses Patient activation Reduce patient anxiety Reduce patient times Reduce unnecessary biopsies Reduce unnecessary followup Reduce unnecessary procedures Reduce unnecessary visits to physician Seek a second opinion from AI Improve patient-clinician relationship; Increase healthcare access; Reduce healthcare disparities; Stimulate technology or training in research, audit and quality improvement
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
8
2020; Nelson et al., 2020; Ongena et al., 2020), health professionals strongly indicated that AI would not be able to completely replace their professions (n = 22, 47%) (Abouzeid et al., 2021; Blease et al., 2021, 2020, 2019; Castagno and Khalifa, 2020; Doraiswamy et al., 2020; EIT Health, 2020a; Gillan et al., 2019; Oh et al., 2019; Pinto dos Santos et al., 2019; Romero-Brufau et al., 2020b; Samuel et al., 2021; Sandhu et al., 2020; Shen et al., 2020; Sit et al., 2020; Strohm et al., 2020; Thenral and Annamalai, 2021; van Hoek et al., 2019; Waymel et al., 2019; Xiang et al., 2020; Yurdaisik and Aksoy, 2021; Yuzbasioglu, 2021). This is one of the major subthemes in the thematic analysis. Roles could evolve or be altered to suit the introduction of AI (n = 3) (Gillan et al., 2019; Strohm et al., 2020; van Hoek et al., 2019). However, salary reduction is conceivable (n = 3) (Botwe et al., 2021a, 2021b; Coppola et al., 2021) and there exist workforce gaps where health professionals are unlikely to consider a career or role that might
require further practice or AI knowledge (n = 3) (Academy of Medical Royal Colleges, 2019; Australian Academy of Health and Medical Sciences, 2020; Sit et al., 2020) (Table 4B). Contrary to hopes that AI can help reduce workload, health professionals also expressed concerns regarding possible greater burnout with the greater workload required from AI use (n = 8) (Academy of Medical Royal Colleges, 2019; Australian Academy of Health and Medical Sciences, 2020; Banerjee et al., 2021; Codari et al., 2019; Doraiswamy et al., 2020; Hardie et al., 2021; Lee et al., 2020; Scheetz et al., 2021b). Participants were also concerned with the risk of reduced healthcare access through inflated health expenses (n = 7) (Antes et al., 2021; Australian Academy of Health and Medical Sciences, 2020; Jutzi et al., 2020; Layard Horsfall et al., 2021; Lee et al., 2020; Nelson et al., 2020; Yang et al., 2019). Patients, the general public, and health professionals perceived one of the primary risks of reliance on AI to be deskilling of health professionals. Specifically, this was attributed to reduced skills in clinical judgment and assessment (n = 11, 23%) (Antes et al., 2021; Ardon and Schmidt, 2020; Banerjee et al., 2021; Gillan et al., 2019; Hayes, 2020; Jutzi et al., 2020; McCradden et al., 2020; Ongena et al., 2020; Samuel et al., 2021; van der Veer et al., 2021; Yarborough and Stumbo, 2021) because of less opportunities for clinical practice (n = 4) (Banerjee et al., 2021; Coppola et al., 2021; Layard Horsfall et al., 2021; Liang HF et al., 2019). Health professionals also pointed to other concerns, that is that the use of unrepresentative historical datasets for training AI could lead to biases in outcomes (Collins et al., 2021; Girosi et al., 2021; Kocaballi et al., 2020) and subsequently lead to reduced trust in health professionals (n = 5) (Coppola et al., 2021; Nelson et al., 2020; Ryan et al., 2021; Scheetz et al., 2021a; Yarborough and Stumbo, 2021).
3.3.4. Perceived AI challenges (n = 30)
Participants from all groups of health professionals, patients and the general public indicated that the main challenge of AI is to deliver empathic care to patients (n = 27, 90%) (Academy of Medical Royal Colleges, 2019; Adams et al., 2020; Almalki, 2021; Australian Academy of Health and Medical Sciences, 2020; Blease et al., 2019, 2020, 2021; Doraiswamy et al., 2020; Gao et al., 2020; Goetz et al., 2020; Hardie et al., 2021; Jungmann et al., 2021; Jutzi et al., 2020; Layard Horsfall et al., 2021; Lee et al., 2020; Lennox-Chhugani et al., 2021; Liang HF et al., 2019; Longoni et al., 2019; McCradden et al., 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Ongena et al., 2020; Palanica et al., 2019; Tran et al., 2019; Wong et al., 2021; Yang et al., 2019; Zhang et al., 2021a) (Table 5A). While health professionals perceived some other challenges including a lack of evidence-based AI technologies (Academy of Medical Royal Colleges, 2019; Botwe et al., 2021b; EIT Health, 2020b; Gillan et al., 2019; Laï et al., 2020; Layard Horsfall et al., 2021; Strohm et al., 2020; Thenral and Annamalai, 2021) in which cost-effectiveness studies should be further explored for AI use in health care (Academy of
Table 3B
AI benefits – Health professionals.
Themes Analytical themes References
AI benefits For health workers: (Abdullah and Fakieh, 2020; Abouzeid et al., 2021; Academy of Medical Royal Colleges, 2019; Adams et al., 2020; Antes et al., 2021; Ardon and Schmidt, 2020; Australian Academy of Health and Medical Sciences, 2020; Banerjee et al., 2021; Benrimoh et al., 2021; Blease et al., 2021; Botwe et al., 2021a; Codari et al., 2019; Coppola et al., 2021; Doraiswamy et al., 2020; EIT Health, 2020a; Gillan et al., 2019; Goetz et al., 2020; Goss et al., 2019; Hardie et al., 2021; Hui et al., 2021; Jauk et al., 2021; Jutzi et al., 2020; Laï et al., 2020; Layard Horsfall et al., 2021; Liang HF et al., 2019; Meyer et al., 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Oh et al., 2019; Ongena et al., 2020; Paranjape et al., 2021; Pinto dos Santos et al., 2019; Romero-Brufau et al., 2020b; Samuel et al., 2021; Sandhu et al., 2020; Shen et al., 2020; Staartjes et al., 2020; Strohm et al., 2020; Tran et al., 2019; Walter et al., 2020; Waymel et al., 2019; Wong et al., 2021; Yurdaisik and Aksoy, 2021; Zhang et al., 2021a)
Increase efficiency Increase labour efficiency Increase medical capabilities For patients: AI makes the interaction with patients feel less personal Better manage own health Better understanding healthcare situation Improve patient confidence Improve patient safety Increase compliance with treatment regimes Lower health expenses Reduce patient anxiety Reduce patient times Improve patient-clinician relationship; Increase healthcare access; Reduce healthcare disparities; Stimulate technology or training in research, audit and quality improvement
Table 4A
Perceived AI risks – Patients and members of general public.
Themes Analytical themes References
Perceived AI risks Algorithmic bias (Academy of Medical Royal Colleges, 2019; Antes et al., 2021; Ardon and Schmidt, 2020, 2020; Australian Academy of Health and Medical Sciences, 2020; Banerjee et al., 2021; Blease et al., 2021; Botwe et al., 2021b; Castagno and Khalifa, 2020; Doraiswamy et al., 2020; Esmaeilzadeh, 2020; Gao et al., 2020, 2020; Gillan et al., 2019; Girosi et al., 2021; Hayes, 2020; Layard Horsfall et al., 2021; Lee et al., 2020; Lennox-Chhugani et al., 2021; Liang HF et al., 2019; McCradden et al., 2020, 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Ongena et al., 2020; Ryan et al., 2021; Thenral and Annamalai, 2021; Tran et al., 2019; Wong et al., 2021; Yarborough and Stumbo, 2021; Zhang et al., 2021a)
Data privacy Dependence on technologies Increase healthcare disparities Less accurate diagnosis or treatment Reduce healthcare access
Table 4B
Perceived AI risks – Health professionals.
Themes Analytical themes References
Perceived AI risks Bias (Abouzeid et al., 2021; Blease et al., 2021, 2020, 2019; Castagno and Khalifa, 2020; Doraiswamy et al., 2020; EIT Health, 2020a; Gillan et al., 2019; Oh et al., 2019; Pinto dos Santos et al., 2019; Romero-Brufau et al., 2020b; Samuel et al., 2021; Sandhu et al., 2020; Shen et al., 2020; Sit et al., 2020; Strohm et al., 2020; Thenral and Annamalai, 2021; van Hoek et al., 2019; Waymel et al., 2019; Xiang et al., 2020; Yurdaisik and Aksoy, 2021; Yuzbasioglu, 2021)
Data privacy Decrease efficiency Dependence on technologies Increase healthcare disparities Medical errors Patient safety Reduce healthcare access
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
9
Medical Royal Colleges, 2019; EIT Health, 2020b), patients and general public questioned the technical uncertainties arising from AI use, including infrastructure failure or possibility of technical malfunction (Antes et al., 2021; Esmaeilzadeh, 2020; Jutzi et al., 2020; Lee et al., 2020; Nadarzynski et al., 2019; Tran et al., 2019). Both health professionals and patients also voiced their concerns over the suitability of AI applications in the health area. This is because AI applications might not be useful to every patient (Academy of Medical Royal Colleges, 2019; Oh et al., 2019), in emergencies (Biller-Andorno et al., 2021; Palanica et al., 2019), be unable to deal with complications (Hui et al., 2021; Yang et al., 2019), or lack context or human experience during AI development (Hui et al., 2021; Nelson et al., 2020) (Table 5B).
Participants also perceived a lack of AI that correctly interprets outcomes (interpretability) (Blease et al., 2021; Girosi et al., 2021; Hardie et al., 2021; Samuel et al., 2021) and a lack of AI which explains how it comes to a decision (explainability) (Ardon and Schmidt, 2020; Collins et al., 2021; Doraiswamy et al., 2020) as other challenges.
3.3.5. AI acceptability (n = 59)
Overall, participants from all groups of health professionals, patients and the general public showed positivity towards AI use in healthcare. Acceptability is explored through the lens of acceptance, satisfaction, trust, intention to use, and willingness to use. AI explainability (also referred to as “interpretability” or “transparency”) is the concept that an AI model and its output can be explained in a way that “makes sense” to a human being at an acceptable level. Patients and the general public expressed their expectation to understand how AI systems function technologically and how they come up with a result in the end (Adams et al., 2020; Haan et al., 2019; Nadarzynski et al., 2019; Ongena et al., 2020; Xiang et al., 2020; Zhang
et al., 2021a). Patients and the general public also perceived that the use of AI in health systems would be more acceptable if costs were affordable to ensure access for everyone (Lee et al., 2020; Liu et al., 2021a, 2021b; Stai et al., 2020). Some studies also showed acceptability of AI use depends on familiarity with technology (Lennartz et al., 2021; Yang et al., 2019; Ye et al., 2019), but others showed their acceptance did not necessarily depend on their familiarity with the technology (Juravle et al., 2020; Yang et al., 2019). Health professionals identified emerging factors influencing the acceptability of AI use (Fig. 3). Their acceptance was influenced by the trust of patients and the general public (Blease et al., 2019, 2021; Codari et al., 2019; Hardie et al., 2021) while patients and the general public expressed that they would accept AI use in health care if it is trusted by health professionals (Adams et al., 2020; Nelson et al., 2020; Yun et al., 2021), which indicates the mutual reliance between these two groups. Health professionals would be likely to use AI if its decision is easy to understand and that is also associated with trust (Diprose et al., 2020; Fan et al., 2020; Humphrey, 2021). They included AI application’s high predictive accuracy (Jauk et al., 2021; Walter et al., 2020), safety (Academy of Medical Royal Colleges, 2019; Palanica et al., 2019), and ease of use (Hayes, 2020; Jauk et al., 2021; Scheetz et al., 2021b). For patients and the general public, there were other key factors influencing the acceptability of AI use (Fig. 3). Members of general public and patients perceived AI accuracy (Almalki, 2021; Nadarzynski et al., 2019; Zhang et al., 2021a, 2021b) to be one of the important factors leading to acceptability. When confronted with AI accuracy and AI explainability, participants preferred accuracy over explainability (van der Veer et al., 2021; Zhang et al., 2021b). Participants valued reliability and accuracy rather than AI explainability, and where participants disagreed with AI predictions, they expressed that detailed explanations of AI outcomes might do more harm than good. Regarding attitudes towards AI use in healthcare, a clear pattern is that most studies involving either health professionals or patients or the general public expressed more positive attitudes (n = 28, 47%) (Abdoul et al., 2021; Abouzeid et al., 2021; Aggarwal et al., 2021; Ahmed et al., 2021; Almalki, 2021; Ardon and Schmidt, 2020; Benrimoh et al., 2021; Coppola et al., 2021; Haan et al., 2019; Hui et al., 2021; Jauk et al., 2021; Juravle et al., 2020; Jutzi et al., 2020; Laï et al., 2020; Lennox-Chhugani et al., 2021; Meyer et al., 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Ongena et al., 2020; Palmisciano et al., 2020; Ryan et al., 2021; Scheetz et al., 2021b; Sit et al., 2020; van Hoek et al., 2019; Waymel et al., 2019; Xiang et al., 2020, 2020, 2020; Yarborough and Stumbo, 2021; Zhang et al., 2021a) towards AI use than negative attitudes (n = 11, 19%) (Bigman and Gray, 2018; Gao et al., 2020; Jenney et al., 2020; Jungmann et al., 2021; McCradden et al., 2020; Ongena et al., 2020, 2021; Romero-Brufau et al., 2020b; Ryan et al., 2021; Stai et al., 2020; Yang et al., 2019). Patients and members of the public were hesitant about using AI in healthcare (n = 8) mainly because they preferred human interaction over AI-based communication (Nadarzynski et al., 2019; Ongena et al., 2020, 2021; Palmisciano et al., 2020), mistrusted AI companies (Esmaeilzadeh, 2020; Gao et al., 2020; McCradden et al., 2020), or were sceptical about AI capabilities due to a belief that the technology is premature (Gao et al., 2020; Haan et al., 2019; Nadarzynski et al., 2019; Yarborough and Stumbo, 2021). For those who supported AI use (n = 14), it came with a key condition that AI should support rather than replace human doctors (Palmisciano et al., 2020; Xiang et al., 2020; Yarborough and Stumbo, 2021). Similar to patients and the general public, health professionals from some studies (n = 2) had reservations about using AI as they were not convinced by the added value of AI or were sceptical about its capabilities (Ardon and Schmidt, 2020; Jungmann et al., 2021; Samuel et al., 2021; Staartjes et al., 2020). Among those supporting AI use (n = 11), radiologists were more optimistic about positive consequences than
Table 5A
Perceived AI challenges – Patients and members of general public.
Themes Analytical themes References
Perceived AI challenges Care experience (Academy of Medical Royal Colleges, 2019; Adams et al., 2020; Almalki, 2021; Australian Academy of Health and Medical Sciences, 2020; Blease et al., 2019, 2020, 2021; Doraiswamy et al., 2020; Gao et al., 2020; Goetz et al., 2020; Hardie et al., 2021; Jungmann et al., 2021; Jutzi et al., 2020; Layard Horsfall et al., 2021; Lee et al., 2020; Lennox-Chhugani et al., 2021; Liang HF et al., 2019; Longoni et al., 2019; McCradden et al., 2020; Nadarzynski et al., 2019; Nelson et al., 2020; Ongena et al., 2020; Palanica et al., 2019; Tran et al., 2019; Wong et al., 2021; Yang et al., 2019; Zhang et al., 2021a)
Suitability: Lack of capability to deal with complications Lack of capability to detect rare conditions Lack of context or human experience Technical uncertainties Unregulated standards
Table 5B
Perceived AI challenges – Health professionals.
Themes Analytical themes References
Perceived AI challenges Care experience (Academy of Medical Royal Colleges, 2019; Ardon and Schmidt, 2020; Biller-Andorno et al., 2021; Blease et al., 2021; Botwe et al., 2021b; Collins et al., 2021; EIT Health, 2020b; Gillan et al., 2019; Girosi et al., 2021; Hardie et al., 2021; Hui et al., 2021; Laï et al., 2020; Layard Horsfall et al., 2021; Palanica et al., 2019; Samuel et al., 2021; Strohm et al., 2020; Thenral and Annamalai, 2021; Yang et al., 2019)
Suitability: Difficult to apply to controversial subjects Not applicable to some health areas Not applicable to every patient Not useful in emergencies Technical barriers
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
10
other health professionals (Banerjee et al., 2021; Laï et al., 2020).
3.3.6. AI development (n = 11)
Patients and the general public expect to have AI algorithms or models set up or developed from or within the clinical context (Nelson et al., 2020; Yarborough and Stumbo, 2021) to ensure such applications direct their benefits to those in need. This resonates with preferences of health professionals that AI results should be validated by clinicians (Scheetz et al., 2021b) and should be first commercialised in a university hospital (Oh et al., 2019) (Table 6A). Health professionals had different expectations regarding AI development. They included a need for AI validation (n = 2) (EIT Health, 2020a; Jungmann et al., 2021), specifically envisioning more focus on RCT rather than just observational studies (Hendrix et al., 2021). Sharing this view, patients also conveyed that the AI algorithms needed to be validated in terms of their accuracy before their use in practice (n = 6) (Adams et al., 2020; Haan et al., 2019; Hui et al., 2021; Lennox-Chhugani et al., 2021; Nelson et al., 2020; Palmisciano et al., 2020). Regarding AI transparency, health professionals preferred AI decision-making mechanisms to be transparent and accessible for inspection, testing, and path correction throughout their lifecycle (n = 3) (Ardon and Schmidt, 2020; Australian Academy of Health and Medical Sciences, 2020; Laï et al., 2020). Health professionals also expected AI auditability which refers to the ability of an AI system to be evaluated and assessed, as an AI system should not be a "black box" (n = 2) (Academy of Medical Royal Colleges, 2019; EIT Health, 2020b). Health professionals also brought to the discussion views on how machine learning output should be explained – AI explainability. They were of the opinion that AI results and decisions must be understandable and comprehensible to health professionals (n = 4) (Biller-Andorno et al., 2021; Collins et al., 2021; Jungmann et al., 2021; Samuel et al., 2021). Data normalisation (further enabling machine learning of
outcome data) was also expected to further enable outcome data. In the surgical setting, this practice included elements of a surgical procedure identified for data normalisation, metrics to train, test, and measure surgical performance, and utilising task deconstruction on surgical procedures to identify key tasks and errors (Collins et al., 2021). Health professionals emphasised that AI development needed to focus on providing improved patient care and safety. This mattered more than automation (n = 6, 55%) (Academy of Medical Royal Colleges, 2019; EIT Health, 2020a; Kocaballi et al., 2020; Laï et al., 2020; Lee et al., 2020; Scheetz et al., 2021a). They believed that AI should not become a ’consumer good’ that health professionals would not need, and it must be developed or implemented to enhance patient care (Table 6B). Health professionals insisted on playing a role in the development of AI. Clinicians can and must be part of any change that will accompany the development and use of AI. Such involvement would increase trust in AI and improve explainability to patients (n = 8, 72%) (Academy of Medical Royal Colleges, 2019; Ardon and Schmidt, 2020; Banerjee et al., 2021; Codari et al., 2019; Gillan et al., 2019; Paranjape et al., 2021; Ryan et al., 2021; Strohm et al., 2020). Also, a broader group of patients, the public, and health professionals should be consulted during AI development (n = 4) (Academy of Medical Royal Colleges, 2019; Collins et al., 2021; Hardie et al., 2021; Select Committee on Artificial Intelligence, 2018). Patients and the general public conveyed a keen interest in giving their voice to AI development as it would help them understand how AI diagnosis or treatment is acquired, interpreted and
Fig. 3. Key factors influencing the acceptability of AI use.
Table 6A
AI development – Patients and members of general public.
Themes Analytical themes References
AI development Accuracy of AI data input and output
(Adams et al., 2020; Haan et al., 2019; Hui et al., 2021; Lee et al., 2020; Lennox-Chhugani et al., 2021; Longoni et al., 2019; Nelson et al., 2020; Palmisciano et al., 2020; Yarborough and Stumbo, 2021).
Clinical context used to build models Features to improve care experience Human oversight Patient engagement Patient utility matters
Table 6B
AI development – Health professionals.
Themes Analytical themes References
AI development AI Auditability (Academy of Medical Royal Colleges, 2019; Ardon and Schmidt, 2020; Australian Academy of Health and Medical Sciences, 2020; Banerjee et al., 2021; Biller-Andorno et al., 2021; Codari et al., 2019; Collins et al., 2021; EIT Health, 2020b; Hardie et al., 2021; Hendrix et al., 2021; Jungmann et al., 2021; Kocaballi et al., 2020; Laï et al., 2020; Paranjape et al., 2021; Ryan et al., 2021; Samuel et al., 2021; Scheetz et al., 2021a; Select Committee on Artificial Intelligence, 2018)
AI Transparency AI Validation Data normalisation Data privacy Diversity of training and validation data Features to enhance care experience First commercialised at a university hospital Patient utility matters Sensitivity Specificity Stakeholder engagement Supporting evidence Transparent and secure way of data storage and protection
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
11
communicated while being personalized to patient uniqueness (Lee et al., 2020; Longoni et al., 2019).
3.3.7. AI implementation (n = 27)
Patients, the general public, and health professionals envisioned many facilitating factors for AI implementation. More awareness of AI application in health care should be raised among patients and the general public (n = 5) (Adams et al., 2020; Jutzi et al., 2020; Lennox-Chhugani et al., 2021; Ongena et al., 2020; Yarborough and Stumbo, 2021). Participants anticipated that health professionals needed to be ready and capable of handling AI (Jutzi et al., 2020; Tran et al., 2019). While not evident across descriptive themes, hopes for a subsidy or public health insurance policy on AI diagnoses would encourage AI use in health care (Liu et al., 2021a). Hindering factors for successful AI implementation that patients and the general public perceived included a lack of familiarity with AI, lack of trust, and regulatory uncertainties (n = 5) (Gao et al., 2020; Jutzi et al., 2020; Lennox-Chhugani et al., 2021; Nadarzynski et al., 2019; Zhang et al., 2021a). Physician disapproval of AI was also one of the challenges to hinder AI use by participants (Meyer et al., 2020). For health professionals, factors hampering AI implementation consisted of lack of knowledge and guidance on AI applications, financial support, and data to develop a reliable machine learning model (n = 9) (Australian Academy of Health and Medical Sciences, 2020; Botwe et al., 2021a, 2021b; EIT Health, 2020b; Hardie et al., 2021; Scheetz et al., 2021b; Staartjes et al., 2020; Strohm et al., 2020; Thenral and Annamalai, 2021), hindered by high costs (Botwe et al., 2021b; Paranjape et al., 2021), or inconsistent performance of AI technologies (Strohm et al., 2020) (Table 7A). Current and prospective health professionals, including medical students (Abouzeid et al., 2021; Blease et al., 2020; Pinto dos Santos et al., 2019; Select Committee on Artificial Intelligence, 2018; Sit et al., 2020), expressed strong messages on the need for more education and training on advantages and limitations of AI (n = 26, 96%) (Abdullah and Fakieh, 2020; Abouzeid et al., 2021; Banerjee et al., 2021; Blease et al., 2021; Botwe et al., 2021a; EIT Health, 2020a, 2020b; Eltorai et al., 2020; Gillan et al., 2019; Goss et al., 2019; Hayes, 2020; Laï et al., 2020; Layard Horsfall et al., 2021; Paranjape et al., 2021; Ryan et al., 2021; Sandhu et al., 2020; Scheetz et al., 2021b, 2021a; Select Committee on Artificial Intelligence, 2018; Sit et al., 2020; Thenral and Annamalai, 2021; Waymel et al., 2019; Wong et al., 2021; Xiang et al., 2020; Yuzbasioglu, 2021). Education and training will help reskill rather than deskill health professionals (n = 7) (Ardon and Schmidt, 2020;
Australian Academy of Health and Medical Sciences, 2020; Blease et al., 2020; Botwe et al., 2021a; Codari et al., 2019; EIT Health, 2020b; Ryan et al., 2021) (Table 7B)
3.3.8. AI regulations (n = 24)
Fig. 4 captures the two most significant subthemes under AI regulations: data-related issues and responsibility mechanisms. Patients, general public, and health professionals shared similar views. Recognising the importance of data for AI, health professionals expected that once governance standards were met, data access should be allowed with mechanisms demonstrating societal benefit (n = 4) (Academy of Medical Royal Colleges, 2019; Australian Academy of Health and Medical Sciences, 2020; Blease et al., 2021; Select Committee on Artificial Intelligence, 2018). Participants expressed the need for governments to clarify who gets to access data for AI development (Academy of Medical Royal Colleges, 2019; Australian Academy of Health and Medical Sciences, 2020; Blease et al., 2021). Regarding data privacy, patients, the general public, and health professionals all shared the same view that guidelines should be developed around anonymising data (n = 3) (Adams et al., 2020; Aggarwal et al., 2021; Collins et al., 2021) and reliably labelling data (Collins et al., 2021; EIT Health, 2020b). While complying with data privacy regulations, health professionals expressed the need to balance data privacy risks and usefulness of AI. They advocated providing explanations to patients about the use of their data and promoting helpfulness of the service provided, rather than just concerns about privacy risks (Hui et al., 2021). Patients and the general public advocated learning more about rights to access their data (Adams et al., 2020; Tran et al., 2019) while exhibiting some concerns about sharing data with insurance or technology companies (Aggarwal et al., 2021). They however demonstrated a willingness to share anonymised data for AI development (n = 3) (Adams et al., 2020; Jutzi et al., 2020; Ongena et al., 2020). In the case of adverse events arising from using AI, participants from both groups raised similar questions on who should be held accountable (n = 13, 54%) (Botwe et al., 2021a; Codari et al., 2019; EIT Health,
Table 7A
AI implementation – Patients and members of general public.
Themes Analytical themes References
AI implementation Facilitating factors: (Australian Academy of Health and Medical Sciences, 2020; Botwe et al., 2021a; EIT Health, 2020b; Gao et al., 2020; Jutzi et al., 2020; Lennox-Chhugani et al., 2021; Nadarzynski et al., 2019; Paranjape et al., 2021; Scheetz et al., 2021a; Staartjes et al., 2020; Strohm et al., 2020; Zhang et al., 2021a)
Clarify processes and roles of those who are involved in the procedure; Human doctors need to be ready and capable of handling AI; Patient awareness of AI; Reimbursement of AI medicine or devices Hindering factors: Ambiguous understanding of how AI will be used; Lack of familiarity with AI; Lack of resources; Lack of transparency; Lack of trust; Low computer literacy; Physician disapproval of AI; Regulatory and legal uncertainties
Table 7B
AI implementation – Health professionals.
Themes Analytical themes References
AI implementation Facilitating factors (Abdullah and Fakieh, 2020; Abouzeid et al., 2021; Adams et al., 2020; Banerjee et al., 2021; Blease et al., 2021; Botwe et al., 2021a; EIT Health, 2020a, 2020b; Eltorai et al., 2020; Gao et al., 2020; Gillan et al., 2019; Goss et al., 2019; Hayes, 2020; Jutzi et al., 2020; Laï et al., 2020; Layard Horsfall et al., 2021; Lennox-Chhugani et al., 2021; Liu et al., 2021b; Meyer et al., 2020; Ongena et al., 2021; Paranjape et al., 2021; Ryan et al., 2021; Sandhu et al., 2020; Scheetz et al., 2021b, 2021a; Select Committee on Artificial Intelligence, 2018; Sit et al., 2020; Strohm et al., 2020; Thenral and Annamalai, 2021; Tran et al., 2019; Waymel et al., 2019; Wong et al., 2021; Xiang et al., 2020; Yarborough and Stumbo, 2021; Yuzbasioglu, 2021; Zhang et al., 2021b)
Clear referral pathway Easily integrated into existing IT systems Expect a large added value of AI Innovation strategies from the leadership team Local champions More education and training on AI Patient rights and safety Reimbursement of AI medicine or devices Hindering factors Easily affected by cyber threat Hindered by high costs Inconsistent performance of AI applications Lack of resources Lack of trust from stakeholders Low public engagement Regulatory and legal uncertainties Unstructured planning and monitoring of AI implementation
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
12
2020a; Esmaeilzadeh, 2020; Jungmann et al., 2021; Khullar et al., 2021; Laï et al., 2020; Oh et al., 2019; Ryan et al., 2021; Thenral and Annamalai, 2021; van Hoek et al., 2019; Waymel et al., 2019; Yurdaisik and Aksoy, 2021). See Fig. 4 above for a summary of perceptions about AI regulations. Health professionals expressed the need for a mechanism clarifying the responsibilities of clinicians who use AI (Academy of Medical Royal Colleges, 2019; Blease et al., 2019; Layard Horsfall et al., 2021; Scheetz et al., 2021a). Participants from most studies agreed that health professionals in charge (authorised party) should be held responsible in the event of adverse events arising from AI use (n = 9, 38%) (Botwe et al., 2021b; Codari et al., 2019; EIT Health, 2020a; Haan et al., 2019; Jungmann et al., 2021; Khullar et al., 2021, 2021; Nelson et al., 2020; Oh et al., 2019; Waymel et al., 2019) (Fig. 5). However, participants from some studies pointed out that health professionals were not prepared to be held responsible (Laï et al., 2020; Yurdaisik and Aksoy, 2021) and that responsibility remained unclear to them (Codari et al., 2019; EIT Health, 2020a; Jungmann et al., 2021; Ryan et al., 2021; Thenral and Annamalai, 2021; van Hoek et al., 2019; Waymel et al., 2019) when a medical error was made during AI use. Participants recommended legal or regulatory guidance in the allocation of liability, and protecting healthcare workers who may or may
not choose to follow AI recommendations (Academy of Medical Royal Colleges, 2019; EIT Health, 2020b). Government agencies and technology firms should be held responsible as well, as they are authorised to approve AI algorithms and are entitled to invent or embed AI algorithms within health applications (Botwe et al., 2021b; Khullar et al., 2021; Oh et al., 2019). Shared responsibility among stakeholders such as government, AI developers, health professionals, health organisations, and patients was also expected (Codari et al., 2019; Jungmann et al., 2021; Waymel et al., 2019).
3.3.9. Human – AI relationship (n = 17)
Patients and the general public insisted that AI should assist, rather than replace human doctors (n = 6) (Jutzi et al., 2020; Longoni et al., 2019; Nelson et al., 2020; Palmisciano et al., 2020; Xiang et al., 2020; Zhang et al., 2021a). To maintain the professional autonomy of health professionals, participants from this group strongly preferred health professionals’ supervision over AI (n = 3) (Jutzi et al., 2020; Lennartz et al., 2021; Ongena et al., 2021) and leveraging AI to sharpen clinical reasoning rather than heavily relying on it (Adams et al., 2020; Haan et al., 2019). There were mixed responses on decisions to choose AI or human doctors when it comes to differences in accuracy between AI and
Fig. 4. Perceptions about AI regulations.
Fig. 5. Who should be held responsible in case of adverse events from AI use?
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
13
human doctors. Some participants who were patients and the general public were inclined to choose AI if AI performed better in the context of asthma management, skin cancer diagnosis and radiology (Abdoul et al., 2021; Haan et al., 2019; Jutzi et al., 2020). Others opted for human doctors if human doctors and AI were equally skilled (Haan et al., 2019; Jutzi et al., 2020) while a majority of participants trusted human doctors no matter how well AI performed (n = 10) (Abdoul et al., 2021; Ahmed et al., 2021; Hui et al., 2021; Jenney et al., 2020; Juravle et al., 2020; Lennartz et al., 2021; Liu et al., 2021b; Nelson et al., 2020; Xiang et al., 2020; Yang et al., 2019) or even if AI performed better (Ongena et al., 2020). Participants in many studies envisioned AI as a second opinion for providers to reconfirm health professionals’ conclusions (n = 4) (Haan et al., 2019; Juravle et al., 2020; Nelson et al., 2020; Ongena et al., 2020). An ideal synergy was expected to be that where AI would be responsible for diagnostic analysis and human doctors for the provision of final results (Liu et al., 2021a, 2021b) (Table 8A). Having similar responses to AI’s role in clinical care, health professionals emphasised that AI should assist and make suggestions, not replace human judgment or make decisions (n = 5) (Biller-Andorno et al., 2021; Hayes, 2020; Hui et al., 2021; Paranjape et al., 2021; van Hoek et al., 2019). Participants strongly conveyed the message of professional autonomy through maintaining clinical reasoning and empathic interaction (n = 3) (Hayes, 2020; Hui et al., 2021; Kocaballi et al., 2020) while still supervising AI (n = 4) (Australian Academy of Health and Medical Sciences, 2020; Kocaballi et al., 2020; Scheetz et al., 2021a; Wong et al., 2021). Health professionals stated that they trust decisions made by human doctors or specialists rather than the ones made by AI (n = 5) (Biller-Andorno et al., 2021; Botwe et al., 2021b; EIT Health, 2020b; Oh et al., 2019; Xiang et al., 2020) (Table 8B).
4. Discussion
This systematic review sought to achieve an understanding of the perspectives of patients, the general public, and health professionals regarding the use of AI in healthcare. Specifically, it elaborated on patterns of views among stakeholders and priorities presented given their roles in the healthcare system. Our review showed that many health professionals and most patients and members of the general public expect high benefits from and have a positive attitude towards AI use in healthcare. However, there are concerns among health professionals as to whether AI could deliver better care and whether patients actually gain greater care experience. Our findings are reinforced by an integrative review conducted by Shinners et al. (2020), which concluded that the belief that AI would replace healthcare professionals in healthcare settings was not apparent. Health professionals and medical students were interested in getting further education and training in AI to improve clinical outcomes and workflow efficiencies. There is therefore an increasing need to integrate healthcare AI into the medical education curriculum. The COVID-19 pandemic shifted entire practices and hospitals to telehealth and
emphasised the need to educate doctors in advanced healthcare technologies (Grunhut et al., 2021). Findings also showed that patients and the general public had no or limited knowledge of AI. This indicates that knowledge promotion to this population about the use of AI in healthcare receives scant attention (Richardson et al., 2021). Further, health professionals expressed their lack of real-world exposure and understanding of how AI really works in health care. AI systems are successful in a wide variety of retrospective medical studies, but relatively few AI tools have been translated into medical practice (Wiens et al., 2019), leading to missed opportunities for health professionals to interact with these applications. Both apprehensions and trust without having adequate knowledge of AI use in healthcare might hence be misplaced or unwarranted, implying that the extremely optimistic and extremely pessimistic viewpoints should be considered cautiously or with some degree of scepticism. The core of this systematic review is to inform existing preferences and attitudes of key stakeholders including the general public, patients, and health professionals to facilitate the provision of more efficacious, equitable, and responsible AI-driven healthcare applications. Therefore, we provide four practical implications. First, we identify the most salient expectations around the use of AI in healthcare. Patients and general public shared some similar expectations for AI benefits including test accuracy, medical error, and more workload reduction for health professionals. Data privacy, reduced clinical judgment, and increased health disparities are among concerns that patients, the general public, and health professionals shared. We learn from this systematic review that key policy and business stakeholders should focus on addressing doubts of patients and members of general public about AI’s ability to deliver empathic or personalized care to patients, trust in decisions made by human doctors in the AI context, and high expectations of AI accuracy. This is consistent with the findings revealed in another review which reported patient and general public perspectives toward clinical AI (Young et al., 2021). For health professionals, government agencies, clinical institutions, and AI developers might collaborate further to provide more education and training to enhance health professionals’ understanding of AI, warrant clearer legal responsibilities of using AI, and build stronger involvement in guiding the introduction of AI. Second, we highlight key challenges that need to be addressed. For instance, participants from both groups of health professionals and patients and the general public indicated that the main challenge of AI is to deliver empathic care to patients. As exposure to AI in healthcare remains limited among members of the general public, patients, and health professionals, it might explain that prevalent concern about AI’s ability to understand and respond to human emotions and deliver empathic care emphasises the importance of human agency during AIenabled healthcare. Elsewhere, authors also highlighted the relational dimension of patient care that makes health care work different from other occupations (Aquino et al., 2023). Also, current challenges become more of how to get our health professionals adapted to the introduction of AI rather than a previously held concern that health professionals may lose their jobs. Third, this review has provided a detailed analysis of the multi
Table 8A
Human-AI relationship – Patients and members of general public.
Themes Analytical themes References
Human-AI relationship Define AI’s role in clinical care (Abdoul et al., 2021; Adams et al., 2020; Ahmed et al., 2021; Haan et al., 2019; Hui et al., 2021; Jenney et al., 2020; Juravle et al., 2020; Jutzi et al., 2020; Lennartz et al., 2021; Longoni et al., 2019; Nelson et al., 2020; Ongena et al., 2021, 2021, 2021; Palmisciano et al., 2020; Xiang et al., 2020; Yang et al., 2019; Zhang et al., 2021a)
Health professionals maintain professional autonomy Responses to conflicts between human and AI clinical decisionmaking Synergy of AI and human doctor
Table 8B
Human-AI relationship – Health professionals.
Themes Analytical themes References
Human-AI relationship Define AI’s role in clinical care (Biller-Andorno et al., 2021; Botwe et al., 2021b; EIT Health, 2020b; Hayes, 2020; Hui et al., 2021; Kocaballi et al., 2020; Oh et al., 2019; Paranjape et al., 2021; van Hoek et al., 2019; Wong et al., 2021; Xiang et al., 2020)
Health professionals maintain professional autonomy Responses to conflicts between human and AI clinical decisionmaking Synergy of AI and human doctor
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
14
faceted approaches related to the development and implementation of AI in healthcare. Specifically, this review confirms a sense of urgency to develop regulatory standards that are designed to balance benefits, safety, and innovation. By doing so, policymakers and regulatory bodies can mitigate unwanted risks and protect the public, patient, and health professionals’ interests while better adopting AI applications in healthcare. Fourth, from a regulatory perspective, clinical AI systems need to be validated before large-scale deployment while data privacy and responsibility mechanisms play a central role in galvanising the use of AI in health care. There is a need to go beyond addressing algorithmrelated issues to looking at the translation of legislation and guidelines into practice to ensure fairness, accountability, transparency, and ethics in AI. With the advent of AI in healthcare, who will end up getting the best from the application of AI is still to be determined; the translation and balance of regulatory standards to ensure that patients benefit most must be of paramount importance. This review used robust systematic review methods (e.g., search, screening, extraction, appraisal process) and identified a rich body of relatively high-quality research. Exploring health professionals, patients, and members of the public also provided a rounded understanding of their perspectives towards AI use in healthcare, with clear practice and policy implications. This review however has some limitations. First, besides the search term ‘artificial intelligence’, our search strategy was limited to the other three terms in the title and abstract of the publications (machine learning, deep learning, neural networks). Piloted terms included clinical decision support, assistive technology, computer-assisted diagnosis, and digital health. These latter search terms resulted in substantial results without returning additional relevant articles. The terms that explicitly referred to different applications of AI in healthcare such as automation, intelligent, robot, or mobile health were omitted (Yu et al., 2018). However, as long as the detailed definition was made in studies that relate to AI-related terms, such studies will be included in the review. This review also included all relevant studies in the previous systematic review on the general public and patient population (Young et al., 2021) while bringing the perspectives of health professionals into the discussion, which is the first of its kind to the best of our knowledge. Novel technologies in health care such as biologic, precision medicine may use AI as part of their algorithm; however, only AI applications that were explicitly described or classified using AI algorithms were included in the studies. Also, caution is advised with interpretation as only a small proportion of the identified studies used representative samples and we can not rule out the influence of potential sample selection issues. Cross-sectional studies are prone to nonresponse bias, which can result in a nonrepresentative sample. Moreover, most of studies investigated preferences or attitudes of respondents in a variety of contexts (patients on outpatient basis, or health professionals on radiology department, etc.), which may not be representative of the entire target population. The wide use of convenience samples also influenced the quality of the studies. Indeed, the variations in the study quality pose methodological challenges on how best to weigh the findings by the quality of different types of studies. The authors are not aware of any previous review studies that have weighed the findings and this also remains a limitation to this review. We suggest that readers refer to Table 2. Summary of quality assessment for included studies when interpreting themes. Further, given we are not aiming to present prevalence of different themes, weighting of evidence will have less impact on key findings of this systematic review. Another limitation is that the search strategy only included English language articles for resource reasons. Views and attitudes towards AI use in healthcare may likely vary across countries and so findings may not be generalizable to other cultures. While developing countries such as China or India have commenced exploring stakeholder attitudes towards AI in health care as indicated from our findings, the review might miss out articles published in non-English speaking languages (e.g.
Mandarin, Hindi) due to language strain. This systematic review found only 10% of all included studies that recruited participants with a certain level of lived experience with AI tools through cross-sectional surveys, or semi-structured interviews. It will be critical and valid to assess participants’ attitudes towards their real-world use and their exposure to care experience that AI applications provided to them. This is understandable due to the limited progress of AI implementation in clinical practice (Rajpurkar et al., 2022). However, the systematic review included 13 scenario-based studies with hypothetical prompts which would be an immediate alternative to address the lack of familiarity or knowledge of how AI could be used in real practices (Schoenberg and Ravdal, 2000). Among these studies, only three stated preference (SP) studies were found. SP studies are used to understand how people might behave in a new situation by presenting them with hypothetical scenarios (Bateman et al., 2002). From our systematic review, despite an ever-growing field, AI applications in healthcare are still little known to the general public, patients, or even health professionals. By making choices in SP studies, respondents indirectly reveal the aspects that are most important to them and help decision-makers understand how people value factors (or attributes) when making decisions about AI acceptability. In a limited resource allocation, future research should explore the relative importance of factors or attributes contributing to the trust and acceptability of patients, members of the public, and health professionals across health conditions and corresponding AI applications.
5. Conclusion
While overall positivity persists in attitudes and preferences toward AI use in healthcare, some prevalent problems require more attention. AI has enhanced clinical diagnosis and decision-making performance in different task domains. Translation and adoption of AI in healthcare in a wide range of applications including diagnosis and treatment, administration, and patient monitoring, will depend on how nimbly AI applications are developed and implemented within a healthcare system that is under limited resource allocation while catching up with rapid advances than ever before. Clinicians will need to adapt to their evolving roles with the introduction of AI, and they should be well-equipped with the tools and methods to deliver great care to patients. This is the ultimate goal in which patient safety and utility come before automation. Further engagement between health professionals and AI developers should be made to prioritize and develop the applications that address crucial clinical needs and ease clinical workflow by introducing new AI applications within the clinical setting. Patients and members of the public place high trust in human doctors with oversight of AI applications. This emphasises professional autonomy maintained by health professionals during AI implementation. Both patients and members of the public show conditionally positive attitudes towards AI use in health care, which indicates a promising signal for trust and acceptability. However, various concerns should be addressed to successfully implement AI in clinical practice, especially lack of familiarity and gaps in knowledge of AI and potential bias exhibiting polarisation and fragmentation among socioeconomically disadvantaged participants.
Funding source
Funding for the study was supported by the National Health and Medical Research Council grant “The algorithm will see you now: ethical, legal and social implications of adopting machine learning systems for diagnosis and screening”. The grant has no role in study design, in the collection, analysis and interpretation of data, in the writing of the articles or in the decision to submit for publication.
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
15
Declaration of competing interest
None declared.
Data availability
Data will be made available on request.
Acknowledgements
We would like to acknowledge the contributions of Jenny Fafeita, David Horne, and Gabby Lamb, the dedicated librarians at Monash University who supported the authors to conduct NVivo and literature search for the review. We are also thankful for supports provided by Associate Professor Duncan Mortimer and PhD researchers at the Centre for Health Economics, Monash University during the development of this review.
Appendix A. Supplementary data
Supplementary data to this article can be found online at https://doi. org/10.1016/j.socscimed.2023.116357.
References
Abdoul, C., Cros, P., Coutier, L., Hadchouel, A., Drummond, D., Neuraz, A., Burgun, A., Giovannini-Chami, L., 2021. Parents’ views on artificial intelligence for the daily management of childhood asthma: a survey. J. Allergy Clin. Immunol. Pract. 9, 1728. https://doi.org/10.1016/j.jaip.2020.11.048. Abdullah, R., Fakieh, B., 2020. Health care employees’ perceptions of the use of artificial intelligence applications: survey study. J. Med. Internet Res. 22, e17620 https://doi. org/10.2196/17620. Abouzeid, H.L., Chaturvedi, S., Abdelaziz, K.M., Alzahrani, F.A., AlQarni, A.A.S., Alqahtani, N.M., 2021. Role of robotics and artificial intelligence in oral health and Preventive dentistry - knowledge, perception and attitude of dentists. Oral Health Prev. Dent. 19, 353–363. https://doi.org/10.3290/j.ohpd.b1693873. Academy of Medical Royal Colleges, 2019. Artificial Intelligence in Healthcare. Academy of Medical Royal Colleges. Adams, S.J., Tang, R., Babyn, P., 2020. Patient perspectives and priorities regarding artificial intelligence in radiology: opportunities for patient-Centered radiology. J. Am. Coll. Radiol. 17, 1034–1036. https://doi.org/10.1016/j.jacr.2020.01.007. Aggarwal, R., Farag, S., Martin, G., Ashrafian, H., Darzi, A., 2021. Patient perceptions on data sharing and applying artificial intelligence to health care data: cross-sectional survey. J. Med. Internet Res. 23 https://doi.org/10.2196/26162. Ahmed, N.J., Alzahrani, A.A., Alonazi, R.E., Menshawy, M.A., 2021. The knowledge and attitudes of the public toward the clinical use of artificial intelligence. Asian J. Pharm. 15, 168–171. https://doi.org/10.22377/ajp.v15i1.3974. Almalki, M., 2021. Exploring the Influential factors of consumers’ willingness toward using COVID-19 related chatbots: an Empirical study. Med. Arch. Sarajevo Bosnia Herzeg. 75, 50–55. https://doi.org/10.5455/medarh.2021.75.50-55. Antes, A.L., Burrous, S., Sisk, B.A., Schuelke, M.J., Keune, J.D., DuBois, J.M., 2021. Exploring perceptions of healthcare technologies enabled by artificial intelligence: an online, scenario-based survey. BMC Med Inform. Decis Mak 21. https://doi.org/ 10.1186/s12911-021-01586-8. Aquino, Y.S.J., Rogers, W.A., Braunack-Mayer, A., Frazer, H., Win, K.T., Houssami, N., Degeling, C., Semsarian, C., Carter, S.M., 2023. Utopia versus dystopia: professional perspectives on the impact of healthcare artificial intelligence on clinical roles and skills. Int. J. Med. Inf. 169, 104903 https://doi.org/10.1016/j. ijmedinf.2022.104903. Ardon, O., Schmidt, R.L., 2020. Clinical laboratory employees’ attitudes toward artificial intelligence. Lab. Med. 51, 649–654. https://doi.org/10.1093/labmed/lmaa023. Australian Academy of Health and Medical Sciences, 2020. Artificial Intelligence in Health: Exploring the Opportunities and Challenges. Australian Academy of Health and Medical Sciences. Australian Alliance for Artificial Intelligence in Healthcare, 2021. A Roadmap for Artificial Intelligence in Healthcare for Australia. Banerjee, M., Chiew, D., Patel, K.T., Johns, I., Chappell, D., Linton, N., Cole, G.D., Francis, D.P., Szram, J., Ross, J., Zaman, S., 2021. The impact of artificial intelligence on clinical education: perceptions of postgraduate trainee doctors in London (UK) and recommendations for trainers. BMC Med. Educ. 21, 429. https:// doi.org/10.1186/s12909-021-02870-x. Bartoletti, I., 2019. AI in healthcare: ethical and privacy challenges. In: Ria ̃no, D., Wilk, S., ten Teije, A. (Eds.), Artificial Intelligence in Medicine, Lecture Notes in Computer Science. Springer International Publishing, Cham, pp. 7–10. https://doi. org/10.1007/978-3-030-21642-9_2. Bateman, I., Carson, R., Day, B., Hanemann, M., Hanley, N., Hett, T., Jones-Lee, M., Loomes, G., 2002. Economic Valuation with Stated Preference Techniques: a Manual.
Behr, A., Theune, K., 2017. Health system efficiency: a fragmented Picture based on OECD data. PharmacoEconomics - Open 1, 203–221. https://doi.org/10.1007/ s41669-017-0010-y. Benrimoh, D., Tanguay-Sela, M., Perlman, K., Israel, S., Mehltretter, J., Armstrong, C., Fratila, R., Parikh, S.V., Karp, J.F., Heller, K., Vahia, I.V., Blumberger, D.M., Karama, S., Vigod, S.N., Myhr, G., Martins, R., Rollins, C., Popescu, C., Lundrigan, E., Snook, E., Wakid, M., Williams, J., Soufi, G., Perez, T., Tunteng, J.-F., Rosenfeld, K., Miresco, M., Turecki, G., Gomez Cardona, L., Linnaranta, O., Margolese, H.C., 2021. Using a simulation centre to evaluate preliminary acceptability and impact of an artificial intelligence-powered clinical decision support system for depression treatment on the physician-patient interaction. BJPsych Open 7. https://doi.org/ 10.1192/bjo.2020.127. Bigman, Y.E., Gray, K., 2018. People are averse to machines making moral decisions. Cognition 181, 21–34. https://doi.org/10.1016/j.cognition.2018.08.003. Biller-Andorno, N., Ferrario, A., Joebges, S., Krones, T., Massini, F., Barth, P., Arampatzis, G., Krauthammer, M., 2021. AI support for ethical decision-making around resuscitation: proceed with care. J. Med. Ethics. https://doi.org/10.1136/ medethics-2020-106786. Blease, C., Kaptchuk, T.J., Bernstein, M.H., Mandl, K.D., Halamka, J.D., DesRoches, C.M., 2019. Artificial intelligence and the future of primary care: exploratory qualitative study of UK general practitioners’ views. J. Med. Internet Res. 21 https://doi.org/ 10.2196/12802. N.PAG-N.PAG. Blease, C., Kharko, A., Annoni, M., Gaab, J., Locher, C., 2021. Machine learning in clinical Psychology and Psychotherapy education: a mixed methods pilot survey of postgraduate students at a Swiss university. Front. Public Health 9, 623088. https:// doi.org/10.3389/fpubh.2021.623088. Blease, C., Kharko, A., Locher, C., DesRoches, C.M., Mandl, K.D., 2020. US primary care in 2029: a Delphi survey on the impact of machine learning. PLoS One 15, e0239947. https://doi.org/10.1371/journal.pone.0239947. Botwe, B.O., Akudjedu, T.N., Antwi, W.K., Rockson, P., Mkoloma, S.S., Balogun, E.O., Elshami, W., Bwambale, J., Barare, C., Mdletshe, S., Yao, B., Arkoh, S., 2021a. The integration of artificial intelligence in medical imaging practice: perspectives of African radiographers. Radiography 27, 861–866. https://doi.org/10.1016/j. radi.2021.01.008. Botwe, B.O., Antwi, W.K., Arkoh, S., Akudjedu, T.N., 2021b. Radiographers’ perspectives on the emerging integration of artificial intelligence into diagnostic imaging: the Ghana study. J. Med. Radiat. Sci. https://doi.org/10.1002/jmrs.460. Carter, S.M., Rogers, W., Win, K.T., Frazer, H., Richards, B., Houssami, N., 2020. The ethical, legal and social implications of using artificial intelligence systems in breast cancer care. Breast 49, 25–32. https://doi.org/10.1016/j.breast.2019.10.001. Castagno, S., Khalifa, M., 2020. Perceptions of artificial intelligence among healthcare Staff: a qualitative survey study. Front. Artif. Intell. 3, 84. https://doi.org/10.3389/ frai.2020.578983. Chew, H.S.J., Achananuparp, P., 2022. Perceptions and needs of artificial intelligence in health care to increase adoption: scoping review. J. Med. Internet Res. 24, e32939 https://doi.org/10.2196/32939. Codari, M., Melazzini, L., Morozov, S.P., van Kuijk, C.C., Sconfienza, L.M., Sardanelli, F., European Society of Radiology (ESR), 2019. Impact of Artificial Intelligence on Radiology: a EuroAIM Survey Among Members of the European Society of Radiology. https://doi.org/10.1186/s13244-019-0798-3. Insights Imaging 10. Coiera, E.W., Verspoor, K., Hansen, D.P., 2023. We need to chat about artificial intelligence. Med. J. Aust. 219, 98–100. https://doi.org/10.5694/mja2.51992. Collins, J.W., Marcus, H.J., Stoyanov, D., Hawkes, D., Sridhar, A., Kelly, J.D., Ghazi, A., Hashimoto, D., Hager, G., Arezzo, A., Jannin, P., Maier-Hein, L., Marz, K., Valdastri, P., Mori, K., Elson, D., Giannarou, S., Slack, M., Hares, L., Beaulieu, Y., Levy, J., Laplante, G., Ramadorai, A., Jarc, A., Andrews, B., Garcia, P., Andrusaite, A., Neemuchwala, H., Kimpe, T., 2021. Ethical implications of AI in robotic surgical training: a Delphi consensus statement. Eur. Urol. Focus. https://doi. org/10.1016/j.euf.2021.04.006. Coppola, F., Golfieri, R., Faggioni, L., Neri, E., Regge, D., Giovagnoni, A., Bibbolino, C., Miele, V., Grassi, R., 2021. Artificial intelligence: radiologists’ expectations and opinions gleaned from a nationwide online survey. Radiol. Med. 126, 63–71. https:// doi.org/10.1007/s11547-020-01205-y. Davenport, T., Kalakota, R., 2019. The potential for artificial intelligence in healthcare. Future Healthc. J. 6, 94. https://doi.org/10.7861/futurehosp.6-2-94. Diprose, W.K., Buist, N., Hua, N., Thurier, Q., Shand, G., Robinson, R., 2020. Physician understanding, explainability, and trust in a hypothetical machine learning risk calculator. J. Am. Med. Inform. Assoc. 27, 592–600. https://doi.org/10.1093/jamia/ ocz229. Doraiswamy, P.M., Blease, C., Bodner, K., 2020. Artificial intelligence and the future of psychiatry: Insights from a global physician survey. Artif. Intell. Med. 102 https:// doi.org/10.1016/j.artmed.2019.101753. N.PAG-N.PAG. EIT Health, 2020a. Transforming Healthcare with AI - the Impact on the Workforce and Organisations. EIT Health. EIT Health, 2020b. Healthcare Workforce and Organisational Transformation with AI Enacting Change. EIT Health. Eltorai, A.E.M., Bratt, A.K., Guo, H.H., 2020. Thoracic radiologists’ versus computer Scientists’ perspectives on the future of artificial intelligence in radiology. J. Thorac. Imaging 35, 255–259. https://doi.org/10.1097/RTI.0000000000000453. Esmaeilzadeh, P., 2020. Use of AI-based tools for healthcare purposes: a survey study from consumers’ perspectives. BMC Med Inform. Decis Mak 20. https://doi.org/ 10.1186/s12911-020-01191-1. European Parliament, 2022. Artificial Intelligence in Healthcare: Applications, Risks, and Ethical and Societal Impacts. Fan, W., Liu, J., Zhu, S., Pardalos, P.M., 2020. Investigating the impacting factors for the healthcare professionals to adopt artificial intelligence-based medical diagnosis
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
16
support system (AIMDSS). Ann. Oper. Res. 294, 567–592. https://doi.org/10.1007/ s10479-018-2818-y. Gao, S., He, L., Chen, Y., Li, D., Lai, K., 2020. Public perception of artificial intelligence in medical care: content analysis of social media. J. Med. Internet Res. 22 https://doi. org/10.2196/16649. N.PAG-N.PAG. Gillan, C., Milne, E., Harnett, N., Purdie, T.G., Jaffray, D.A., Hodges, B., 2019. Professional implications of introducing artificial intelligence in healthcare: an evaluation using radiation medicine as a testing ground. J. Radiother. Pract. 18, 5–9. https://doi.org/10.1017/S1460396918000468. Girosi, F., Mann, S., Kareddy, V., 2021. Artificial Intelligence in Clinical Care. PatientCentered Outcomes Research Insitute, United States. Goetz, C.M., Arnetz, J.E., Sudan, S., Arnetz, B.B., 2020. Perceptions of virtual primary care physicians: a focus group study of medical and data science graduate students. PLoS One 15, e0243641. https://doi.org/10.1371/journal.pone.0243641. Goodyear-Smith, F., Ashton, T., 2019. New Zealand health system: universalism struggles with persisting inequities. Lancet 394, 432–442. https://doi.org/10.1016/ S0140-6736(19)31238-3. Goss, F.R., Bakes, S., Blackley, S.V., Ortega, C.A., Kowalski, L.T., Lin, C.-T., Landman, A. B., Bates, D.W., Zhou, L., Meteer, M., Gradwohl, S.C., 2019. A clinician survey of using speech recognition for clinical documentation in the electronic health record. Int. J. Med. Inf. 130, 103938 https://doi.org/10.1016/j.ijmedinf.2019.07.017. Greenberg, N., Docherty, M., Gnanapragasam, S., Wessely, S., 2020. Managing mental health challenges faced by healthcare workers during covid-19 pandemic. BMJ m1211. https://doi.org/10.1136/bmj.m1211. Grunhut, J., Wyatt, A.T., Marques, O., 2021. Educating future physicians in artificial intelligence (AI): an integrative review and Proposed changes. J. Med. Educ. Curric. Dev. 8, 23821205211036836 https://doi.org/10.1177/23821205211036836. Haan, M., Ongena, Y.P., Hommes, S., Kwee, T.C., Yakar, D., 2019. A qualitative study to understand patient perspective on the Use of artificial intelligence in radiology. J. Am. Coll. Radiol. JACR 16, 1416–1419. https://doi.org/10.1016/j. jacr.2018.12.043. Hardie, T., Horton, T., Warburton, W., 2021. Switched on: How Do We Get the Best Out of Automation and AI in Health Care? the Health Foundation. https://doi.org/ 10.37829/HF-2021-I03. Hayes, T.F., 2020. A Qualitative Exploratory Study of Emergency Medicine Clinician Perspectives on Clinical Decision Support Systems (CDSS) Rooted in Machine Learning in England. Hendrix, N., Hauber, B., Lee, C.I., Bansal, A., Veenstra, D.L., 2021. Artificial intelligence in breast cancer screening: primary care provider preferences. J. Am. Med. Inform. Assoc. JAMIA 28, 1117–1124. https://doi.org/10.1093/jamia/ocaa292. Hong, Q.N., Pluye, P., Bujold, M., Wassef, M., 2017. Convergent and sequential synthesis designs: implications for conducting and reporting systematic reviews of qualitative and quantitative evidence. Syst. Rev. 6, 61. https://doi.org/10.1186/s13643-0170454-2. Hong, Q.N., Pluye, P., F`abregues, S., Bartlett, G., Boardman, F., Cargo, M., Dagenais, P., Gagnon, M.-P., Griffiths, F., Nicolau, B., O’Cathain, A., Rousseau, M.-C., Vedel, I., 2019. Improving the content validity of the mixed methods appraisal tool: a modified e-Delphi study. J. Clin. Epidemiol. 111, 49–59.e1. https://doi.org/ 10.1016/j.jclinepi.2019.03.008. Hui, C.Y., Pinnock, H., McKinstry, B., Fulton, O., Buchner, M., 2021. Patients’ and clinicians’ perceived trust in Internet-of-Things systems to support asthma Selfmanagement: qualitative interview study. JMIR MHealth UHealth 9, e24127. https://doi.org/10.2196/24127. Humphrey, B.A., 2021. Data Privacy vs. Innovation: A Quantitative Analysis of Artificial Intelligence in Healthcare and its Impact on HIPAA Regarding the Privacy and Security of Protected Health Information (Ph.D.). ProQuest Diss. Theses. Robert Morris University, United States – Pennsylvania. Ioannidis, J.P.A., Patsopoulos, N.A., Rothstein, H.R., 2008. Reasons or excuses for avoiding meta-analysis in forest plots. BMJ 336, 1413–1415. https://doi.org/ 10.1136/bmj.a117. Jauk, S., Kramer, D., Avian, A., Berghold, A., Leodolter, W., Schulz, S., 2021. Technology acceptance of a machine learning algorithm predicting Delirium in a clinical setting: a mixed-methods study. J. Med. Syst. 45, 1–8. https://doi.org/10.1007/s10916-02101727-6. Jenney, H., York, T., Jones, G., 2020. Clinician and computer: a study on patient perceptions of artificial intelligence in skeletal radiography. BMJ Health Care Inform 27. https://doi.org/10.1136/bmjhci-2020-100233. Jungmann, F., Jorg, T., Hahn, F., Pinto dos Santos, D., Jungmann, S.M., Düber, C., Mildenberger, P., Kloeckner, R., 2021. Attitudes toward artificial intelligence among radiologists, IT specialists, and industry. Acad. Radiol. 28, 834–840. https://doi.org/ 10.1016/j.acra.2020.04.011. Juravle, G., Boudouraki, A., Terziyska, M., Rezlescu, C., 2020. Trust in artificial intelligence for medical diagnoses. Prog. Brain Res. 253, 263–282. https://doi.org/ 10.1016/bs.pbr.2020.06.006. Jutzi, T.B., Krieghoff-Henning, E.I., Holland-Letz, T., Utikal, J.S., Hauschild, A., Schadendorf, D., Sondermann, W., Fr ̈ohling, S., Hekler, A., Schmitt, M., Maron, R.C., Brinker, T.J., 2020. Artificial intelligence in skin cancer diagnostics: the patients’ perspective. Front. Med. 7 https://doi.org/10.3389/fmed.2020.00233. Khullar, D., Casalino, L.P., Qian, Y., Lu, Y., Chang, E., Aneja, S., 2021. Public vs physician views of liability for artificial intelligence in health care. J Am Med Inform. Assoc 28, 1574–1577. https://doi.org/10.1093/jamia/ocab055. Kocaballi, A.B., Ijaz, K., Laranjo, L., Quiroz, J.C., Rezazadegan, D., Tong, H.L., Willcock, S., Berkovsky, S., Coiera, E., 2020. Envisioning an artificial intelligence documentation assistant for future primary care consultations: a co-design study with general practitioners. J. Am. Med. Inform. Assoc. 27, 1695–1704. https://doi. org/10.1093/jamia/ocaa131.
Laï, M.-C., Brian, M., Mamzer, M.-F., 2020. Perceptions of artificial intelligence in healthcare: findings from a qualitative survey study among actors in France. J. Transl. Med. 18, 1–13. https://doi.org/10.1186/s12967-019-02204-y. Layard Horsfall, H., Khan, D.Z., Muirhead, W., Koh, C.H., Stoyanov, D., Marcus, H.J., Palmisciano, P., 2021. Attitudes of the surgical Team toward artificial intelligence in neurosurgery: international 2-stage cross-sectional survey. World Neurosurg 146, e724–e730. https://doi.org/10.1016/j.wneu.2020.10.171. Lee, H., Piao, M., Lee, J., Byun, A., Kim, J., 2020. The purpose of Bedside robots: exploring the needs of Inpatients and healthcare professionals. CIN Comput. Inform. Nurs. 38, 8–17. https://doi.org/10.1097/CIN.0000000000000558. Lennartz, S., Dratsch, T., Zopfs, D., Persigehl, T., Maintz, D., Hokamp, N.G., Santos, D.P. dos, Große Hokamp, N., Pinto Dos Santos, D., 2021. Use and Control of artificial intelligence in patients across the medical workflow: Single-Center questionnaire study of patient perspectives. J. Med. Internet Res. 23 https://doi.org/10.2196/ 24221. N.PAG-N.PAG. Lennox-Chhugani, N., Chen, Y., Pearson, V., Trzcinski, B., James, J., 2021. Women’s attitudes to the use of AI image readers: a case study from a national breast screening programme. BMJ Health Care Inform 28. https://doi.org/10.1136/bmjhci-2020100293. Liang, H.F., Wu, K.M., Weng, C.H., Hsieh, H.W., 2019. Nurses’ views on the potential use of robots in the pediatric unit. J. Pediatr. Nurs. 47, e58–e64. https://doi.org/ 10.1016/j.pedn.2019.04.027. Liu, T., Huang, F., Feng, G., Du, J., Hu, X., Ming, W.-K., Tsang, W., Xie, Y., Chen, Y., Lau, O., Tian, K., Shi, T., Chu, B., Zhao, J., Cai, Y., Akinwunmi, B., Huang, J., Zhang, C.J.P., 2021a. Preferences for artificial intelligence clinicians before and during the covid-19 pandemic: discrete choice experiment and propensity score matching study. J. Med. Internet Res. 23, e26997 https://doi.org/10.2196/26997. Liu, T., Tsang, W., Huang, F., Lau, O.Y., Chen, Y., Sheng, J., Guo, Y., Akinwunmi, B., Zhang, C.J., Ming, W.-K., 2021b. Patients’ preferences for artificial intelligence applications versus clinicians in disease diagnosis during the SARS-CoV-2 pandemic in China: discrete choice experiment. J. Med. Internet Res. 23, e22841 https://doi. org/10.2196/22841. Longoni, C., Bonezzi, A., Morewedge, C.K., 2019. Resistance to medical artificial intelligence. J. Consum. Res. 46, 629–650. https://doi.org/10.1093/jcr/ucz013. McCradden, M.D., Sarker, T., Paprica, P.A., 2020. Conditionally positive: a qualitative study of public perceptions about using health data for artificial intelligence research. BMJ Open 10, e039798. https://doi.org/10.1136/bmjopen-2020-039798. Meyer, A.N.D., Giardina, T.D., Spitzmueller, C., Shahid, U., Scott, T.M.T., Singh, H., 2020. Patient perspectives on the usefulness of an artificial intelligence-assisted symptom checker: cross-sectional survey study. J. Med. Internet Res. 22 https://doi. org/10.2196/14679. N.PAG-N.PAG. Michel, J.-P., Ecarnot, F., 2020. The shortage of skilled workers in Europe: its impact on geriatric medicine. Eur. Geriatr. Med. 11, 345–347. https://doi.org/10.1007/ s41999-020-00323-0. Monash University, 2022. Subject Guides: Grey Literature: Grey Sources [WWW Document]. https://guides.lib.monash.edu/grey-literature/greysources, 7.4.22. Nadarzynski, T., Miles, O., Cowie, A., Ridge, D., 2019. Acceptability of artificial intelligence (AI)-led chatbot services in healthcare: a mixed-methods study. Digit Health 5. https://doi.org/10.1177/2055207619871808. Nelson, C.A., P ́erez-Chada, L.M., Creadore, A., Li, S.J., Lo, K., Manjaly, P., Pournamdari, A.B., Tkachenko, E., Barbieri, J.S., Ko, J.M., Menon, A.V., Hartman, R. I., Mostaghimi, A., 2020. Patient perspectives on the Use of artificial intelligence for skin cancer screening: a qualitative study. JAMA Dermatol 156, 501–512. https:// doi.org/10.1001/jamadermatol.2019.5014. OECD, European Union, 2020. Health at a Glance: Europe 2020: State of Health in the EU Cycle, Health at a Glance: Europe. OECD. https://doi.org/10.1787/82129230-en. Oh, S., Kim, J.H., Choi, S.-W., Lee, H.J., Hong, J., Kwon, S.H., 2019. Physician confidence in artificial intelligence: an online mobile survey. J. Med. Internet Res. 21 https:// doi.org/10.2196/12422. N.PAG-N.PAG. Ongena, Y.P., Haan, M., Yakar, D., Kwee, T.C., 2020. Patients’ views on the implementation of artificial intelligence in radiology: development and validation of a standardized questionnaire. Eur. Radiol. 30, 1033–1040. https://doi.org/10.1007/ s00330-019-06486-0. Ongena, Y.P., Yakar, D., Haan, M., Kwee, T.C., 2021. Artificial intelligence in screening mammography: a population survey of women’s preferences. J. Am. Coll. Radiol. 18, 79–86. https://doi.org/10.1016/j.jacr.2020.09.042. Organization for Economic Co-operation and Development, 2019. Recommendation of the Council on Artificial Intelligence (OECD Legal Instruments. OECD/LEGAL/O449) [WWW Document]. https://legalinstruments.oecd. org/en/instruments/OECD-LEGAL-0449#mainText, 8.5.21. Ortega, A.N., Roby, D.H., 2021. Ending structural racism in the US health care system to eliminate health care inequities. JAMA 326, 613–615. https://doi.org/10.1001/ jama.2021.11160. Page, M.J., McKenzie, J.E., Bossuyt, P.M., Boutron, I., Hoffmann, T.C., Mulrow, C.D., Shamseer, L., Tetzlaff, J.M., Akl, E.A., Brennan, S.E., Chou, R., Glanville, J., Grimshaw, J.M., Hro ́bjartsson, A., Lalu, M.M., Li, T., Loder, E.W., Mayo-Wilson, E., McDonald, S., McGuinness, L.A., Stewart, L.A., Thomas, J., Tricco, A.C., Welch, V.A., Whiting, P., Moher, D., 2021. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ 372, n71. https://doi.org/10.1136/bmj.n71. Palanica, A., Flaschner, P., Thommandram, A., Li, M., Fossat, Y., 2019. Physicians’ perceptions of chatbots in health care: cross-sectional web-based survey. J. Med. Internet Res. 21 https://doi.org/10.2196/12887. Palmisciano, P., Jamjoom, A.A.B., Taylor, D., Stoyanov, D., Marcus, H.J., 2020. Attitudes of patients and their relatives toward artificial intelligence in neurosurgery. World Neurosurg 138, e627–e633. https://doi.org/10.1016/j.wneu.2020.03.029.
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
17
Paranjape, K., Schinkel, M., Hammer, R.D., Schouten, B., Panday, R.S.N., Elbers, P.W.G., Kramer, M.H.H., Nanayakkara, P., Nannan Panday, R.S., 2021. The value of artificial intelligence in laboratory medicine. Am. J. Clin. Pathol. 155, 823–831. https://doi. org/10.1093/ajcp/aqaa170. Pavli, A., Theodoridou, M., Maltezou, H.C., 2021. Post-COVID syndrome: incidence, clinical spectrum, and challenges for primary healthcare professionals. Arch. Med. Res. 52, 575–581. https://doi.org/10.1016/j.arcmed.2021.03.010. Pinto dos Santos, D., Giese, D., Brodehl, S., Chon, S.H., Staab, W., Kleinert, R., Maintz, D., Baeßler, B., 2019. Medical students’ attitude towards artificial intelligence: a multicentre survey. Eur. Radiol. 29, 1640–1646. https://doi.org/10.1007/s00330018-5601-1. Popay, J., Roberts, H., Sowden, A., Petticrew, M., Arai, L., Rodgers, M., Britten, N., 2006. Guidance on the Conduct of Narrative Synthesis in Systematic Reviews. Rajpurkar, P., Chen, E., Banerjee, O., Topol, E.J., 2022. AI in health and medicine. Nat. Med. 28, 31–38. https://doi.org/10.1038/s41591-021-01614-0. Richardson, J.P., Smith, C., Curtis, S., Watson, S., Zhu, X., Barry, B., Sharp, R.R., 2021. Patient apprehensions about the use of artificial intelligence in healthcare. Npj Digit. Med. 4, 1–6. https://doi.org/10.1038/s41746-021-00509-1. Romero-Brufau, S., Wyatt, K.D., Boyum, P., Mickelson, M., Moore, M., CognettaRieke, C., 2020a. A lesson in implementation: a pre-post study of providers’ experience with artificial intelligence-based clinical decision support. Int. J. Med. Inf. 137, 104072 https://doi.org/10.1016/j.ijmedinf.2019.104072. Romero-Brufau, S., Wyatt, K.D., Boyum, P., Mickelson, M., Moore, M., CognettaRieke, C., 2020b. What’s in a name? A comparison of attitudes towards artificial intelligence (AI) versus augmented human intelligence (AHI). BMC Med. Inform. Decis. Mak. 20, 1–5. https://doi.org/10.1186/s12911-020-01158-2. Ryan, M.-L., McNulty, J.P., O’Donovan, T., 2021. Artificial Intelligence: the Opinions of Radiographers and Radiation Therapists in Ireland. Radiography. https://doi.org/ 10.1016/j.radi.2021.07.022. Samuel, G., Diedericks, H., Derrick, G., 2021. Population health AI researchers’ perceptions of the public portrayal of AI: a pilot study. Public Underst Sci 30, 196–211. https://doi.org/10.1177/0963662520965490. Sandhu, S., Lin, A.L., Brajer, N., Sperling, J., Ratliff, W., Balu, S., Sendak, M.P., Bedoya, A.D., O’Brien, C., 2020. Integrating a machine learning system into clinical workflows: qualitative study. J. Med. Internet Res. 22, e22421 https://doi.org/ 10.2196/22421. Scheetz, J., Hadoux, X., Keel, S., Rothschild, P., van Wijngaarden, P., McGuinness, M., Soyer, H.P., Janda, M., Condon, J.J.J., Oakden-Rayner, L., Palmer, L.J., 2021a. A survey of clinicians on the use of artificial intelligence in ophthalmology, dermatology, radiology and radiation oncology. Sci. Rep. 11, 5193. https://doi.org/ 10.1038/s41598-021-84698-5. Scheetz, J., Tan, Z., O’Day, R., Sandhu, S., Keel, S., Koca, D., McGuinness, M., Holloway, E., Zhu, Z., MacIsaac, R.J., Gilfillan, C., Turner, A., He, M., 2021b. Realworld artificial intelligence-based opportunistic screening for diabetic retinopathy in endocrinology and indigenous healthcare settings in Australia. Sci. Rep. 11, 15808 https://doi.org/10.1038/s41598-021-94178-5. Schoenberg, N.E., Ravdal, H., 2000. Using vignettes in awareness and attitudinal research. Int. J. Soc. Res. Methodol. 3, 63–74. https://doi.org/10.1080/ 136455700294932. Scott, I.A., Carter, S.M., Coiera, E., 2021. Exploring stakeholder attitudes towards AI in clinical practice. BMJ Health Care Inform 28, e100450. https://doi.org/10.1136/ bmjhci-2021-100450. Select Committee on Artificial Intelligence, 2018. AI in the UK: Ready, Willing and Able. House of Lords, United Kingdom. Shaw, J., Rudzicz, F., Jamieson, T., Goldfarb, A., 2019. Artificial intelligence and the implementation challenge. J. Med. Internet Res. 21, e13659 https://doi.org/ 10.2196/13659. Shelton, R.C., Cooper, B.R., Stirman, S.W., 2018. The sustainability of evidence-based interventions and practices in public health and health care. Annu. Rev. Public Health 39, 55–76. https://doi.org/10.1146/annurev-publhealth-040617-014731. Shen, C., Li, C., Xu, F., Wang, Z., Shen, X., Gao, J., Ko, R., Jing, Y., Tang, X., Yu, R., Guo, J., Xu, F., Meng, R., Cui, Y., 2020. Web-based study on Chinese dermatologists’ attitudes towards artificial intelligence. Ann. Transl. Med. 8, 698. https://doi.org/ 10.21037/atm.2019.12.102. Shinners, L., Aggar, C., Grace, S., Smith, S., 2020. Exploring healthcare professionals’ understanding and experiences of artificial intelligence technology use in the delivery of healthcare: an integrative review. Health Inf. J 26, 1225–1236. https:// doi.org/10.1177/1460458219874641. Sit, C., Srinivasan, R., Amlani, A., Muthuswamy, K., Azam, A., Monzon, L., Poon, D.S., 2020. Attitudes and perceptions of UK medical students towards artificial intelligence and radiology: a multicentre survey. Insights Imaging 11, 14. https:// doi.org/10.1186/s13244-019-0830-7. Staartjes, V.E., Stumpo, V., Kernbach, J.M., Klukowska, A.M., Gadjradj, P.S., Schroder, M.L., Veeravagu, A., Stienen, M.N., van Niftrik, C.H.B., Serra, C., Regli, L., 2020. Machine learning in neurosurgery: a global survey. Acta Neurochir. 162, 3081–3091. https://doi.org/10.1007/s00701-020-04532-1. Stai, B., Heller, N., McSweeney, S., Rickman, J., Blake, P., Vasdev, R., Edgerton, Z., Tejpaul, R., Peterson, M., Rosenberg, J., Kalapara, A., Regmi, S., Papanikolopoulos, N., Weight, C., 2020. Public perceptions of artificial intelligence and robotics in medicine. J. Endourol. 34, 1041–1048. https://doi.org/10.1089/ end.2020.0137. Stern, C., Lizarondo, L., Carrier, J., Godfrey, C., Rieger, K., Salmond, S., Ap ́ostolo, J., Kirkpatrick, P., Loveday, H., 2020. Methodological guidance for the conduct of mixed methods systematic reviews. JBI Evid. Synth. 18, 2108–2118. https://doi.org/ 10.11124/JBISRIR-D-19-00169.
Stewart, J., Sprivulis, P., Dwivedi, G., 2018. Artificial intelligence and machine learning in emergency medicine. Emerg. Med. Australas. 30, 870–874. https://doi.org/ 10.1111/1742-6723.13145. Strohm, L., Hehakaya, C., Ranschaert, E.R., Boon, W.P.C., Moors, E.H.M., 2020. Implementation of artificial intelligence (AI) applications in radiology: hindering and facilitating factors. Eur. Radiol. 30, 5525–5532. https://doi.org/10.1007/ s00330-020-06946-y. Thenral, M., Annamalai, A., 2021. Challenges of building, deploying, and using AIenabled telepsychiatry platforms for clinical practice among urban Indians: a qualitative study. Indian J. Psychol. Med. 43, 336–342. https://doi.org/10.1177/ 0253717620973414. Thomas, J., Harden, A., 2008. Methods for the thematic synthesis of qualitative research in systematic reviews. BMC Med. Res. Methodol. 8, 45. https://doi.org/10.1186/ 1471-2288-8-45. Ting, D.S.W., Liu, Y., Burlina, P., Xu, X., Bressler, N.M., Wong, T.Y., 2018. AI for medical imaging goes deep. Nat. Med. 24, 539–540. https://doi.org/10.1038/s41591-0180029-3. Topol, E.J., 2019. High-performance medicine: the convergence of human and artificial intelligence. Nat. Med. 25, 44–56. https://doi.org/10.1038/s41591-018-0300-7. Tran, V.-T., Riveros, C., Ravaud, P., 2019. Patients’ views of wearable devices and AI in healthcare: findings from the ComPaRe e-cohort. Npj Digit. Med. 2, 1–8. https://doi. org/10.1038/s41746-019-0132-y. Tyndall, J., 2010. AACODS Checklist. U.S. Department of Health and Human Services, 2021. AI Strategy. van der Veer, S.N., Peek, N., Riste, L., Phipps, D.L., Tully, M.P., Cheraghi-Sohi, S., Bozentko, K., Atwood, S., Hubbard, A., Wiper, C., Oswald, M., 2021. Trading off accuracy and explainability in AI decision-making: findings from 2 citizens’ juries. J. Am. Med. Inform. Assoc. JAMIA. https://doi.org/10.1093/jamia/ocab127. van Hoek, J., Huber, A., Leichtle, A., Harma, K., Hilt, D., von Tengg-Kobligk, H., Heverhagen, J., Poellinger, A., 2019. A survey on the future of radiology among radiologists, medical students and surgeons: students and surgeons tend to be more skeptical about artificial intelligence and radiologists may fear that other disciplines take over. Eur. J. Radiol. 121, 108742 https://doi.org/10.1016/j. ejrad.2019.108742. Walter, S., Gruss, S., Frisch, S., Liter, J., Jerg-Bretzke, L., Zujalovic, B., Barth, E., 2020. “What about automated pain recognition for routine clinical use?” A survey of physicians and nursing staff on expectations, Requirements, and acceptance. Front. Med. 7, 566278 https://doi.org/10.3389/fmed.2020.566278. Waymel, Q., Badr, S., Demondion, X., Cotten, A., Jacques, T., 2019. Impact of the rise of artificial intelligence in radiology: what do radiologists think? Diagn. Interv. Imaging 100, 327–336. https://doi.org/10.1016/j.diii.2019.03.015. Wiens, J., Saria, S., Sendak, M., Ghassemi, M., Liu, V.X., Doshi-Velez, F., Jung, K., Heller, K., Kale, D., Saeed, M., Ossorio, P.N., Thadaney-Israni, S., Goldenberg, A., 2019. Do no harm: a roadmap for responsible machine learning for health care. Nat. Med. 25, 1337–1340. https://doi.org/10.1038/s41591-019-0548-6. Wong, K., Gallant, F., Szumacher, E., 2021. Perceptions of Canadian radiation oncologists, radiation physicists, radiation therapists and radiation trainees about the impact of artificial intelligence in radiation oncology – national survey. J. Med. Imaging Radiat. Sci. 52, 44–48. https://doi.org/10.1016/j.jmir.2020.11.013. World Bank, 2022. New World Bank Country Classifications by Income Level, pp. 2022–2023. World Health Organisation, 2010. Classifying Health Workers: Mapping Occupations to the International Standard Classification. Xiang, Y., Zhao, L., Liu, Z., Wu, X., Chen, J., Long, E., Lin, D., Zhu, Y., Chen, C., Lin, Z., Lin, H., 2020. Implementation of artificial intelligence in medicine: status analysis and development suggestions. Artif. Intell. Med. 102 https://doi.org/10.1016/j. artmed.2019.101780. N.PAG-N.PAG. Yang, K., Zeng, Z., Peng, H., Jiang, Y., 2019. Attitudes of Chinese cancer patients toward the clinical use of artificial intelligence. Patient Prefer. Adherence 13, 1867–1875. https://doi.org/10.2147/PPA.S225952. Yarborough, B.J.H., Stumbo, S.P., 2021. Patient perspectives on acceptability of, and implementation preferences for, use of electronic health records and machine learning to identify suicide risk. Gen. Hosp. Psychiatr. 70, 31–37. https://doi.org/ 10.1016/j.genhosppsych.2021.02.008. Ye, T., Xue, J., He, M., Gu, J., Lin, H., Xu, B., Cheng, Y., 2019. Psychosocial factors affecting artificial intelligence adoption in health care in China: cross-sectional study. J. Med. Internet Res. 21 https://doi.org/10.2196/14316. N.PAG-N.PAG. Young, A.T., Amara, D., Bhattacharya, A., Wei, M.L., 2021. Patient and general public attitudes towards clinical artificial intelligence: a mixed methods systematic review. Lancet Digit. Health 3, e599–e611. https://doi.org/10.1016/S2589-7500(21)001321. Yu, K.-H., Beam, A.L., Kohane, I.S., 2018. Artificial intelligence in healthcare. Nat. Biomed. Eng. 2, 719–731. https://doi.org/10.1038/s41551-018-0305-z. Yun, J.H., Lee, E.-J., Kim, D.H., 2021. Behavioral and neural evidence on consumer responses to human doctors and medical artificial intelligence. Psychol. Market. 38, 610–625. https://doi.org/10.1002/mar.21445. Yurdaisik, I., Aksoy, S.H., 2021. Evaluation of knowledge and attitudes of radiology department workers about artificial intelligence. Ann. Clin. Anal. Med. 12, 186–190. https://doi.org/10.4328/ACAM.20453.
V. Vo et al.


Social Science & Medicine 338 (2023) 116357
18
Yuzbasioglu, E., 2021. Attitudes and perceptions of dental students towards artificial intelligence. J. Dent. Educ. 85, 60–68. https://doi.org/10.1002/jdd.12385. Zhang, Z., Citardi, D., Wang, D., Genc, Y., Shan, J., Fan, X., 2021a. Patients’ perceptions of using artificial intelligence (AI)-based technology to comprehend radiology
imaging data. Health Inf. J 27, 1–13. https://doi.org/10.1177/ 14604582211011215. Zhang, Z., Genc, Y., Wang, D., Ahsen, M.E., Fan, X., 2021b. Effect of AI explanations on human perceptions of patient-facing AI-powered healthcare systems. J. Med. Syst. 45, 1–10. https://doi.org/10.1007/s10916-021-01743-6.
V. Vo et al.