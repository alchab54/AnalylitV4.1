HAL Id: hal-03470487
https://hal.science/hal-03470487
Submitted on 8 Dec 2021
HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.
Intelligence artificielle et algorithmes en santé
J Charlet
To cite this version:
J Charlet. Intelligence artificielle et algorithmes en santé. Traité de bioéthique - Tome 4, Les nouveaux territoires de la bioéthique, pp.541-554, 2018. ￿hal-03470487￿


IA en santé et éthique
Intelligence artificielle et algorithmes en santé
Jean Charlet1,2
1 Sorbonne Université, INSERM, Université Paris 13, Sorbonne Paris Cité, UMR_S 1142, LIMICS, Paris
2 Assistance Publique-Hôpitaux de Paris, DRCI, Paris, France
Résumé : L’Intelligence artificielle va impacter notre santé et les conséquences, qu’elles soient relationnelles (patient - médecins) ou éthiques méritent d’être interrogées. Nous proposons de d’abord définir ce qu’est l’Intelligence artificielle, pour donner ensuite quelques exemples d’évolution des recherches en Intelligence artificielle et finir par aborder les problèmes d’éthiques et comment les chercheurs et la puissance publique s’en emparent. Mots-clés : intelligence artificielle, IA, santé, éthique.
1 Introduction
Quand on parle d’intelligence artificielle, on se trouve face à un phénomène médiatique impressionnant : l’expression des fantasmes des gens à qui on a fait miroiter d’extraordinaires résultats, des robots en tout genre, l’homme augmenté et, parfois même, la fin de la race humaine. Pourtant l’intelligence artificielle (IA par la suite) ce sont d’abord des calculs, des algorithmes, programmés par des êtres humains et sur lesquels nous avons un certain pouvoir. Il n’empêche que ces algorithmes posent un certain nombre de problèmes par rapport à leur usage et à leur place dans la société. La santé, en raison de la difficulté de la tâche a toujours été un domaine majeur des recherches en IA et maintenant des applications que l’on peut y développer. Même en restant raisonnable, on voit apparaître des applications dont on nous dit qu’elles vont faire des diagnostics à la place des médecins. Cette dernière affirmation mérite d’être discutée, en même temps qu’il faut se poser la question de la place que va prendre l’IA dans notre santé et notre bien-être. Dans tous les cas, il semble bien que l’IA va impacter notre rapport à notre santé. Et les conséquences, qu’elles soient relationnelles (patients, médecins) ou éthiques méritent d’être interrogées. C’est ce que nous allons essayer de faire ici en définissant ce qu’est l’IA, puis, en donnant quelques exemples d’évolution des recherches dans le domaine1. Pour finir, nous aborderons les problèmes d’éthiques et comment les chercheurs et la puissance publique s’en emparent en nous fondant sur les différents rapports des chercheurs et de groupes de travail cités en référence.
2 Définition de l’IA : imiter les fonctions humaines vs développer de meilleurs algorithmes
L’IA est née en 1956 avec le programme de vouloir simuler l’activité du cerveau avec l’hypothèse que nous raisonnions avec des règles d’inférence ou plus tard, à partir des années 80 avec des réseaux de neurones formels. Ainsi, l’IA produit d’abord des programmes informatiques visant à effectuer des tâches nécessitant un certain niveau d’« intelligence ». Après des débuts tonitruants et des promesses non tenues, l’IA s’est séparé en deux courants : d’un côté, les tenants d’une machine qui, non seulement, met en œuvre des raisonnements
1 Nous n’aborderons pas la question des robots dans ce chapitre.


IC 2018
comme les humains mais a aussi une conscience de ses raisonnements ; c’est ce qu’on appelle l’IA forte; d’un autre côté, les tenants d’une machine rendant de nombreux services aux humains en mettant en œuvre des raisonnements plus ou moins proches des êtres humains ; c’est l’IA faible. L’IA forte suscite de nombreux débats autour de l’apparition possible d’une singularité où la machine, supérieure à l’être humain et consciente de cette supériorité le supplanterait dans la société. A ce jour, de nombreux chercheurs en IA pensent même que c’est impossible2. Il n’empêche que c’est une voie de recherche qui continue d’être explorée. L’IA faible va mettre en œuvre toutes les technologies à sa disposition pour essayer de rendre le service attendu par l’utilisateur. L’IA est alors un champ de recherche multidisciplinaire qui va mobiliser plusieurs disciplines – informatique, mathématique, sciences cognitives... – sans oublier les connaissances spécialisées des domaines auxquelles on souhaite l’appliquer. Enfin, les algorithmes qui la sous-tendent reposent sur des approches tout aussi variées : analyse sémantique, représentation symbolique, apprentissage statistique ou exploratoire, réseaux de neurones, etc.
L’IA symbolique
L’IA symbolique désigne une catégorie d’approches fondée sur des modèles dits « de connaissances explicites », que ce soit des modèles d’interaction entre agents pour des systèmes multi-agents (et/ou robotiques), des modèles syntaxiques et linguistiques pour le traitement automatique du langage naturel ou des ontologies informatiques pour la représentation des connaissances. Ces modèles peuvent ensuite, entre autres, être utilisés par des systèmes de raisonnement logique pour produire des nouveaux faits à partir des caractéristiques fournies en entrée. Ils ont été à la base des principaux développements en IA jusque dans les années 80. Un des exemples les plus emblématiques est celui des systèmes experts qui ont suscité de grands espoirs à cette époque. Ensuite, tout en restant dans le paradigme de l’IA symbolique, ils ont évolué d’un système apte à produire un raisonnement symbolique à partir de faits et de règles connus et censés remplacer le médecin, vers un système, toujours symbolique mais associé au médecin et l’aidant à respecter un mode de raisonnement fondé sur les connaissances médicales de sa communauté professionnelle. Dans le premier cas, des règles définies par un médecin étaient transcrites en code pour produire automatiquement des diagnostics. Dans le second cas, c’est un paradigme différent qui est mis en œuvre et traduit une évolution majeure sur 3 points : — les modèles sont des classifications a priori des objets, qui sont rangés dans des catégories fixes suivant des caractéristiques définies. Ce sont maintenant des ontologies informatiques qui, en médecine peuvent contenir des milliers, voire des dizaines de milliers d’objets – i.e. de concepts ; — la médecine s’est fortement protocolisée cette dernière décennie et les spécialités médicales élaborent des guides de bonnes pratiques qui peuvent servir de base à la modélisation d’une base de connaissances ; — enfin, on a changé de mode d’appréhension du raisonnement formel et du raisonnement médical humain : on construit une base de connaissances qui prend en compte les guides de bonnes pratiques et met à la disposition du médecin un système qui va « collaborer » avec lui dans le but de l’aider dans sa tâche. Ce qui n’est pas sans poser des problèmes éthiques (Cf. infra).
2 GANASCIA J.-G. (2017). Le mythe de la singularité : faut-il craindre l’intelligence artificielle ? Éditions du Seuil.


IA en santé et éthique L’IA numérique
L’IA numérique correspond aux approches fondées sur des algorithmes qui exploitent la configuration des données sur lesquelles on veut raisonner pour chercher des régularités entre ces données et les comportements du système visé. Ces algorithmes sont principalement des algorithmes d’apprentissage automatique qui se catégorisent grossièrement en apprentissage supervisé et apprentissage non supervisé3.
Dans l’apprentissage supervisé, on doit distinguer des classes d’objets au sein d’un référentiel connu et en présence d’exemples pour lesquels on connaît les bons résultats, c’està-dire que les données sont étiquetées ou annotées. Cette partie des données pour lesquels on connaît les bons résultats est l’échantillon d’apprentissage. On va par exemple avoir des données de patients dont on connaît les signes et les maladies qu’ils ont – issues par exemple de données d’hospitalisation – pour essayer de proposer, avec une certaine probabilité, le bon diagnostic de nouveaux patients dont on connaît les signes de départ. La validité du résultat se vérifiera sur une autre partie de l'échantillon, l'échantillon test, qui n'a pas servi à l'apprentissage et pour lequel on comparera le résultat obtenu au bon résultat donné par les experts – e.g. médecins – le gold standard. En fonction de la complexité des données et des caractéristiques à apprendre, on a des algorithmes statistiques bayésiens connus et utilisés depuis très longtemps ou des algorithmes dits d’apprentissage profond – deep learning en anglais – dont l’usage explose depuis une dizaine d’années, en particulier dans le cadre des aides que proposent les GAFA aux utilisateurs de leurs outils. Dans l’apprentissage non supervisé, au contraire, on dispose bien d’exemples mais sur des données non annotées et pour lesquelles on ne connaît pas un référentiel d’annotation possible. L’algorithme doit découvrir la structure cachée des données. Lors de la phase d’apprentissage, le but est d’établir cette structure, via des catégories. Par exemple, on pourrait avoir un ensemble de femmes avec un cancer du sein et vouloir trouver des variables explicatives, géographiques, génétiques, d’habitude de vie, d’exposition à des environnements potentiellement toxiques (appelé maintenant souvent l’exposome du patient) ou autres. Un des grands intérêts de ces approches est qu’elles ne nécessitent pas, pour ,l’apprentissage, de corpus annoté à priori, toujours difficiles à constituer.
Du symbolique au numérique
L’IA numérique peut être vue comme préparant ses raisonnements à partir des données, alors que l’IA symbolique prépare ses raisonnements à partir de modèles formels – i.e. décrits par des modèles logiques – développés en amont. Autrement dit, l’IA numérique travaille à partir de l’expérience, sans modèle à priori, l’IA symbolique à partir de règles formelles logiques et de modèles de connaissances (de domaine comme de tâche). Au vu du nombre de données disponibles depuis quelques années – le contexte de ce qu’on appelle le big data l’IA numérique a pour elle l’efficacité et des résultats indubitables, l’IA symbolique est capable d’expliquer ses raisonnements, même s’ils sont beaucoup moins efficaces, ce que l’IA numérique fait mal. Dans des situations où il y a peu de données – e.g. les maladies rares –, les algorithmes de l’IA numérique sont évidemment rapidement mis à mal. La réalité est évidemment plus complexe que cela : des travaux récents en IA numérique tendent à expliciter des modèles de raisonnement et les travaux en IA symbolique utilisent les algorithmes de l’IA numérique pour construire les modèles – e.g. la construction d’ontologies à partir d’algorithmes numériques de fouille de texte.
3 IA et médecine, un mariage d’amour déjà ancien et toujours renouvelé
3 Ce découpage est simplifié et il y a d’autres approches mais nous nous en tenons là pour plus de clarté dans le propos.


IC 2018
La médecine, au vu de la difficulté de la tâche de diagnostic a rapidement été un des domaines de challenge de l’IA. Après sa naissance, fixée en 1956, le premier système expert, DENDRAL, est apparu en 1965 ; MYCIN, le plus connu des systèmes experts en médecine est apparu en 1973 ; le premier système expert en médecine en France, SAM de O. Gascuel date de 19814 ; le début du boom des réseaux de neurones et du deep learning est fixé à 2006 ; enfin, les systèmes d’aide à la décision fondés sur des ontologies sont opérationnels à peu près en 2010. Depuis ces dates, des progrès conséquents ont été faits en IA faible pour la mise en œuvre de systèmes d’IA pour tous les types de personnes (patient, médecin, professionnel de santé), dans différents cadres d’application, pour une personne seule travaillant ou gérant sa maladie à son domicile ou pour un professionnel, dans le cadre des systèmes d’information hospitaliers. Les systèmes d’apprentissage sont revenus sur le devant de la scène médiatique avec des résultats impressionnants qui ont déclenché des questionnements éthiques majeurs (Cf. infra), les systèmes experts du siècle dernier ont évolué vers des systèmes d’aide à la décision symbolique accompagnant les médecins (Cf. supra) et le traitement automatique du langage naturel a fait des progrès qui, sans permettre de reconnaître tout ce qu’écrivent les professionnels de santé, permettent d’envisager des applications innovantes et très utiles.
Le développement de la génomique avec l’accès au séquençage de l’ADN à des coûts toujours plus faibles a créé un domaine de recherche et de soin, la médecine génomique qui s’avère être un domaine où les algorithmes de l’IA se déploient rapidement. Dans ce domaine, des bases de connaissances sont développées pour décrire les résultats de la recherche. Ces résultats relient de façon plus ou moins directe la description des éléments cliniques des patients a des parties du génome, que ce soit les parties codantes, la description des protéines, etc. Chaque nouvelle recherche clinique tend à enrichir ces bases mais l’accumulation de ces connaissances est complexe car chacune des recherches décrit les éléments reliés selon des classifications différentes et avec des modalités différentes, de la donnée structurée au texte libre. L’IA intervient alors de plusieurs manières complémentaires, a) en fournissant les classifications ou les ontologies de description des données cliniques (HPO ou NCIt pour en citer quelques unes), b) en permettant de mettre en œuvre les algorithmes d’apprentissage qui vont permettre d’expliciter des profils clinique-génome de patients et, enfin c) en fournissant des systèmes de traitement automatique du langage naturel permettant de traiter et d’analyser le contenu des bases de connaissances quand celui-ci est textuel. Ces recherches sont nécessaires pour faire « fonctionner » les types d’approche que l’on met en œuvre dans la cadre du Plan France Médecine Génomique 2025 (PFMG 2025 Aviesan (2017)). En effet, le PFMG 2025 vise à doter la France de moyens de déployer les instruments du parcours de soins génomique en se dotant des capacités de séquençage et en mettant en place un outil spécifique, un Collecteur analyseur de données (CAD) capable de traiter et d’exploiter le volume considérable de données générées en les appariant avec les données cliniques et d’imagerie et d’offrir les premiers services dans le cadre du parcours de soin et pour la recherche. Une des difficultés ici est l’appariement des données cliniques et génomiques comme décrit au paragraphe précédent. Les profils des patients servent alors dans les deux contextes de travail du PFMG 2025, la recherche où le profil du patient comprenant les données cliniques et génomiques aussi complètes que possibles enrichit les bases décrites précédemment et le soin, où le profil d’un autre patient permet de lui proposer des pronostics sur des maladies et, encore plus important, des pronostics sur des taux de réussite de thérapeutiques. Ce profilage est le moyen de proposer une médecine personnalisée dans tous ses attendus.
4 GASCUEL O. (1981). SAM : un système expert dans le domaine médical. PhD thesis.


IA en santé et éthique 4 Conséquences éthiques
Problèmes éthiques
Nous allons maintenant lister quelques questions éthiques qui se posent quand l’IA est appliquée à la médecine en commençant par préciser qu’une partie des problèmes commence dès qu’on automatise des tâches. C’est ce qui apparaît clairement dans toutes les références, la question de l’éthique et de l’IA y est incluse dans un questionnement plus large de « l’éthique des algorithmes ». Les tâches automatisées par des algorithmes et qui posent des problèmes d’éthique sont toutes celles où le résultat proposé par la machine va déclencher une action sur la personne ou sur une population. Les principales tâches sont a) l’aide à la décision, diagnostique ou thérapeutique, b) la prédiction d’une pathologie, c) la personnalisation d’un traitement (ces deux dernières tâches sont en particulier en lien avec la recherche des profils cliniques-génomiques (Cf. supra) et, finalement, d) la prévention sur population (épidémie, effets indésirables). De cette liste, on peut expliciter les questionnements qui suivent.
Personnalisation algorithmique et assurance. La personnalisation algorithmique soulève un enjeu spécifique au secteur de l’assurance. En effet, si on profile de plus en plus précisément les patients et qu’on est capable de calculer leurs risques, on peut être tenté de faire payer une assurance santé plus chère à un fumeur au prétexte que son comportement augmente son risque de cancer. La dynamique de personnalisation des offres peut ainsi conduire rapidement à une remise en cause des fondements de la mutualisation. Perte de compétences du médecin. Si l’on délègue des décisions habituellement prises par des médecins à des machines, le médecin ne va-t-il pas perdre ses compétences ? Dans le même ordre d’idées, l’IA semble supérieure à l’homme pour le dépistage de certains cancers ou pour l’analyse de radiographies. Il pourrait donc devenir hasardeux pour un médecin d’établir un diagnostic ou de faire un choix thérapeutique autre que celui recommandé par la machine, laquelle deviendrait dès lors le décideur effectif.
Biais de l’échantillonnage des données dans les bases d’analyse. Les algorithmes d’apprentissage qui se fondent sur les données dépendent de la représentativité des données sur lesquelles se fondent ces apprentissages. Depuis plusieurs années des études montrent que ces bases peuvent être biaisées par leur mode de constitution. Deux critères semblent particulièrement biaiser ces bases, a) la surreprésentativité des personnes plus âgées et b) l’origine raciale des personnes. Ce dernier point est d’autant plus compliqué à gérer qu’on oscille entre le noter, avec de possibles comportement racistes dans certains pays, et ne pas le noter, comme en France, et alors ne pas connaître son influence. Qualité des données. Les données sont la plupart du temps capturées pour un usage précis. Vouloir les réutiliser pour un autre usage nécessite, a minima, d’assumer et d’analyser les différences induites. C’est ainsi le cas des données PMSI, capturées et encodées pour le codage PMSI et qui sont utilisées, avec la mode du Big data, pour un usage médical. Les études ont montré que le biais pouvait être très important. Cela n’interdit pas cet usage mais demande à recaler les données, la plupart du temps, en les croisant avec d’autres. Responsabilité des processus d’IA. Si on met en place des systèmes qui prennent des décisions grâce à des algorithmes d’IA et qui, pour cela, se fondent sur de nombreuses données, il peut se poser la question de la responsabilité de la décision en fonction des données et des calculs, avec des problèmes de répartition des responsabilités compliqués. Les discussions autour de ces problèmes amènent les gens à se demander s’il ne faut pas doter les machines ou algorithmes d’un statut juridique autonome.
Propositions de réponses éthiques
En face de ces problèmes potentiels ou déjà avérés, le gouvernement / la société a réagi, directement ou indirectement à trois niveaux : 1) Le Comité Consultatif National d’Éthique « pour les sciences de la vie et de la santé » a mis en place une consultation générale sur la bio


IC 2018
éthique qui a rendu son rapport en juin 2018 avec un chapitre sur l’IA, 2) le Conseil National du Numérique s’est lui aussi emparé de ce sujet et, enfin, 3) diverses administrations (en particulier la CNIL) ou instances académiques (CERNA / Allistene) ont réuni des groupes de travail et proposé des rapports de synthèses (cf. webographie). En particulier, le CERNA a proposé une réflexion sur les apprentissages automatiques et rappelé un certain nombre de propriétés que doivent satisfaire les systèmes numériques et qui nous semblent fondamentales. Ces propriétés font consensus dans les autres rapports cités en référence : Loyauté et équité. Il faut que les systèmes algorithmiques développés soient loyaux. Cette loyauté des systèmes signifie qu’ils se comportent, au cours de leur exécution, comme leurs concepteurs le déclarent. Cette loyauté est celle de ceux qui conçoivent l’algorithme.
Transparence, traçabilité et explicabilité. La transparence d’un système signifie que son fonctionnement n’est pas opaque, qu’il est possible, par exemple pour un utilisateur, de vérifier son comportement. Cette transparence s’appuie notamment sur la traçabilité, la mise à disposition d’informations sur ses actions suffisamment détaillées. Par construction, la traçabilité et l’explicabilité des systèmes fondés sur l’IA symbolique sont plus faciles à assurer que pour les systèmes d’IA numérique. Il n’empêche qu’il reste beaucoup à faire dans les 2 cas et que cela passera aussi par une meilleure compréhension par les personnes impliquées (professionnels de santé comme patients) du fonctionnement des systèmes d’IA. La formation de tous sera un des critères clés de la transparence de l’IA. Responsabilité médicale. En France, seul un médecin est habilité à établir un diagnostic. Les systèmes ne sont que « d’aide à la décision ». La difficulté est qu’il faut s’assurer que le médecin garde son autonomie de décision. Cela implique, à minima, de distinguer deux agents : un concepteur et un utilisateur du système. Les dysfonctionnements viennent du concepteur, les erreurs d’utilisation viennent de l’utilisateur. Dans ce contexte, il ne semble pas pertinent de doter la machine d’une personnalité juridique qui amènerait le concepteur des algorithmes à se dédouaner des dysfonctionnements. A ce jour, le médecin doit rester le dernier décisionnaire. Conformité. Un système numérique doit demeurer conforme à son cahier des charges, et son cahier des charges doit être conforme à la législation. Cette conformité signifie que le système est conçu pour effectuer des tâches spécifiées en respectant des contraintes explicites. Pour des systèmes d’aide à la décision fondés sur des algorithmes d’apprentissage dont les décisions peuvent évoluer en fonction de l’enrichissement des données d’apprentissage, le respect de cette conformité dans le temps n’est pas évident. Dans une tribune au « Monde » datée du 5 janvier 2018, des dirigeants d’instituts scientifiques estiment que le développement du numérique rend nécessaire la création d’un comité d’éthique pour ces technologies, distinct de la CNIL. Dans la même veine, la commission Villani propose, dans son rapport de fin mars 2018, de créer un Comité d’éthique de l’IA. Nous ne savons pas ce que l’avenir nous réserve mais il sera nécessaire de coordonner toutes les initiatives prises pour proposer des réponses claires et cohérentes.
9 Perspectives et conclusion
Le travail décrit ici se fonde sur une société numérique en construction et les conclusions des consultations sur la bio-éthique qui ont abordé les questions d’IA viennent de sortir mais ne se sont pas encore traduites en décisions. Ainsi les quelques pistes de réflexions proposées ici autour de l’éthique de l’IA en santé vont très probablement devoir évoluer. Mais il est clair qu’une façon de mieux maîtriser les conséquences fonctionnelles de l’IA en santé pour la société et en éthique nécessite de former les patients et les professionnels de santé à l’IA : c’est la seule façon de rester vigilant que ce soit par rapport à l’évolution générale de la société ou la place de la machine au sein de la relation entre le patient et le professionnel de santé. A un moment où l’IA réalise dans certaines situations des tests de dépistage mieux


IA en santé et éthique que l’homme, il faut interroger la place de celle-ci dans la fonction de dépistage comme plus largement de diagnostic et de soin. Il nous semble ainsi qu’il faut apprendre à articuler la place de l’être humain et de la machine en IA et santé tout en rappelant que l’être humain doit rester décisionnaire en dernier ressort dans le trio patient, machine, professionnel de santé pour que notre santé ne nous échappe pas. Pour finir, nous espérons avoir pu faire le point sur ce qu’est l’IA et les algorithmes en santé, ce qu’elle permet de faire et quels sont les points de vigilance qu’il faudra surveiller pour que l’IA – et avec elles les algorithmes en général – serve la société. Peut-être ne faut-il pas que des algorithmes éthiques mais que les humains qui les conçoivent soient mus par des considérations éthiques.
Webographie
Pour cet article, les références concernent très peu d’ouvrages notés dans le corps du texte. De plus, nous nous sommes fondés sur un certain nombre de rapports d’administrations ou de groupes académiques et de pages Wikipedia que nous ne citons pas à des endroits précis pour alléger le texte mais que nous rappelons maintenant.
AVIESAN (2017). Plan France Médecine Génomique 2025. Document accédé en mai 2018 à https://www.aviesan.fr/aviesan/accueil/toute-l-actualite/plan-francemedecine-genomique-2025 .
CERNA (2017). Éthique de la recherche en apprentissage machine. Accédé en mai 2018 à https://www.allistene.fr/publications-cerna-sur-lethique-de-larecherche-en-apprentissage-machine/
CCNE (2018). Rapport de synthèse du Comité consultatif national d’éthique. Accédé en juin 2018 à http://www.ccne-ethique.fr/sites/default/files/rapport_de_synthese_ccne_bat.pdf CNIL (2017). Comment permettre à l’homme de garder la main ? Les enjeux éthiques des algorithmes et de l’intelligence artificielle. Synthèse du débat public animé par la CNIL dans le cadre de la mission de réflexion éthique confiée par la loi pour une république numérique. Accédé en mai 2018 à https://www.cnil.fr/fr/ethique-et-numerique-lesalgorithmes-en-debat-1 .
CNOM (2018). Médecins et patients dans le monde des data, des algorithmes et de l’intelligence artifcielle. Analyse et recommandations du Cnom (Conseil National de l’Ordre des Médecins). Accédé en mai 2018 à https://www.conseil-national.medecin.fr/node/2563 . FRANCE-IA (2017a). Rapport de synthèse France IA. Ministère de l’Économie, des Finances, de l’Action et des Comptes publics. Accédé en mai 2018 à https://www.economie.gouv.fr/files/files/PDF/2017/Rapport_synthese_France_IA _.pdf . VIGNARD B. (2017). L’éthique, grande oubliée des algorithmes ? Rapport interne, Institut Mines Télécom. Article du blog de l’IMT accédé en mai 2018 à https://blogrecherche.wp.imt.fr/2017/10/26/ethique-algorithmes/. WIKIPEDIA (2018). Pagessurl’intelligenceartificielle. Rapport interne, Wikipedia. Pages accédées en mai 2018 à https://en.wikipedia.org/wiki/Artificial_intelligence , https://fr.wikipedia.org/wiki/Intelligence_artificielle, https://fr.wikipedia.org/wiki/Apprentissage_automatique .