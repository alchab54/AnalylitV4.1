#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¯ TEST WORKFLOW ATN COMPLET - 20 ARTICLES ZOTERO CSL JSON (OPTIMISÃ‰ V2)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AnalyLit V4.2 RTX 2060 SUPER - Validation Empirique ThÃ¨se ATN
Source: Export Zotero 20ATN.json + grille-ATN.json
Workflow: Import â†’ Screening (â‰¥70/100) â†’ Extraction 30 champs â†’ PRISMA â†’ Export

ğŸ”§ CORRECTIONS MAJEURES V2:
âœ… Endpoints API corrigÃ©s selon server_v4_complete.py
âœ… Gestion robuste des timeouts Ollama (600s max)
âœ… IDs articles backend utilisÃ©s au lieu de zotero_id
âœ… Retry logic pour Ã©checs temporaires
âœ… Monitoring temps rÃ©el amÃ©liorÃ©
âœ… Batch processing intelligent

Date: 07 octobre 2025
Auteur: Ali Chabaane - ThÃ¨se Alliance ThÃ©rapeutique NumÃ©rique
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import sys
import codecs
import requests
import json
import time
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENCODAGE UTF-8 WINDOWS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
if sys.platform.startswith('win'):
    try:
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
    except Exception as e:
        print(f"WARNING: Could not set UTF-8 stdout/stderr: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION OPTIMISÃ‰E
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
API_BASE = "http://localhost:8080"
PROJECT_ROOT = Path(__file__).resolve().parent.parent
ZOTERO_JSON_PATH = PROJECT_ROOT / "20ATN.json"
GRILLE_ATN_PATH = PROJECT_ROOT / "grille-ATN.json"
OUTPUT_DIR = PROJECT_ROOT / "resultats_atn_zotero"
OUTPUT_DIR.mkdir(exist_ok=True)

# Configuration timeouts optimisÃ©e RTX 2060 SUPER
TIMEOUT_CONFIG = {
    "api_request": 30,
    "add_articles": 300,     # 5 min pour ajout 20 articles
    "analysis": 600,         # 10 min par analyse (Ollama peut Ãªtre lent)
    "extraction": 900,       # 15 min pour extraction complÃ¨te
    "synthesis": 600,        # 10 min pour synthÃ¨se
    "export": 180,           # 3 min pour export
    "task_polling": 15       # Polling toutes les 15s
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOGGING UTF-8 AMÃ‰LIORÃ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def log(level: str, message: str, indent: int = 0):
    """Affiche un message de log formatÃ© avec timestamp et emoji."""
    ts = datetime.now().strftime("%H:%M:%S")
    indent_str = "  " * indent
    emoji_map = {
        "INFO": "â„¹ï¸", "SUCCESS": "âœ…", "ERROR": "âŒ",
        "WARNING": "âš ï¸", "PROGRESS": "â³", "DATA": "ğŸ“Š", 
        "API": "ğŸ“¡", "RETRY": "ğŸ”„", "TIMEOUT": "â°"
    }
    emoji = emoji_map.get(level, "ğŸ“‹")
    print(f"[{ts}] {indent_str}{emoji} {level}: {message}")

def log_section(title: str):
    """Affiche un sÃ©parateur de section."""
    print("\n" + "â•" * 70)
    print(f"  {title}")
    print("â•" * 70 + "\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# API WRAPPER ROBUSTE AVEC RETRY
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def api_request(method: str, endpoint: str, data: Optional[Dict] = None,
                params: Optional[Dict] = None, timeout: int = 120, 
                max_retries: int = 3) -> Optional[Any]:
    """Wrapper robuste pour les requÃªtes API avec retry logic."""
    url = f"{API_BASE}{endpoint}"
    
    for attempt in range(max_retries):
        try:
            log("API", f"{method} {url} (tentative {attempt + 1}/{max_retries})", 1)
            
            if method.upper() == "GET":
                resp = requests.get(url, timeout=timeout, params=params)
            elif method.upper() == "POST":
                resp = requests.post(url, json=data, timeout=timeout, params=params)
            elif method.upper() == "PUT":
                resp = requests.put(url, json=data, timeout=timeout, params=params)
            else:
                log("ERROR", f"MÃ©thode HTTP non supportÃ©e: {method}")
                return None
                
            if resp.status_code in [200, 201, 202]:
                return resp.json()
            elif resp.status_code == 204:
                return True  # No content
            else:
                log("ERROR", f"{method} {endpoint} a Ã©chouÃ© - Code {resp.status_code}: {resp.text[:200]}", 2)
                if resp.status_code >= 500 and attempt < max_retries - 1:
                    log("RETRY", f"Erreur serveur, retry dans 5s...", 2)
                    time.sleep(5)
                    continue
                return None
                
        except requests.exceptions.Timeout:
            log("TIMEOUT", f"Timeout sur {endpoint} aprÃ¨s {timeout}s", 2)
            if attempt < max_retries - 1:
                log("RETRY", f"Retry dans 10s...", 2)
                time.sleep(10)
                continue
            return None
            
        except requests.exceptions.RequestException as e:
            log("ERROR", f"Exception API ({attempt + 1}/{max_retries}): {e}", 2)
            if attempt < max_retries - 1:
                log("RETRY", f"Retry dans 5s...", 2)
                time.sleep(5)
                continue
            
    return None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARSEUR ZOTERO CSL JSON OPTIMISÃ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def parse_zotero_csl_json(json_path: Path) -> List[Dict]:
    """
    Parse un export Zotero CSL JSON vers le format attendu par AnalyLit.
    Version optimisÃ©e avec meilleure gestion des erreurs.
    """
    log("INFO", f"Chargement et parsing de '{json_path.name}'...")
    
    if not json_path.is_file():
        log("ERROR", f"Fichier Zotero JSON introuvable: {json_path}")
        return []
        
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            zotero_items = json.load(f)
        log("SUCCESS", f"Fichier JSON chargÃ©: {len(zotero_items)} entrÃ©es trouvÃ©es")
    except (json.JSONDecodeError, IOError) as e:
        log("ERROR", f"Impossible de lire ou parser le fichier JSON: {e}")
        return []

    articles = []
    for i, item in enumerate(zotero_items):
        try:
            # --- Auteurs (robuste avec particules) ---
            authors_csl = item.get("author", [])
            authors = []
            
            for auth in authors_csl[:5]:  # Limite Ã  5 auteurs
                family = auth.get("family", "")
                given = auth.get("given", "")
                particle = auth.get("non-dropping-particle", "") or auth.get("dropping-particle", "")
                
                # Construction du nom complet avec particule
                name_parts = []
                if given:
                    name_parts.append(given)
                if particle:
                    name_parts.append(particle)
                if family:
                    name_parts.append(family)
                
                full_name = " ".join(name_parts).strip()
                if full_name:
                    authors.append(full_name)
            
            if not authors:
                authors = ["Auteur inconnu"]

            # --- AnnÃ©e robuste ---
            year = datetime.now().year  # DÃ©faut
            issued = item.get("issued", {})
            if issued and "date-parts" in issued and issued["date-parts"]:
                try:
                    year = int(issued["date-parts"][0][0])
                except (IndexError, ValueError, TypeError):
                    pass

            # --- PMID extraction amÃ©liorÃ©e ---
            pmid = ""
            note_content = f"{item.get('note', '')} {item.get('extra', '')}"
            
            # Recherche patterns PMID courants
            pmid_patterns = ["PMID:", "PMID ", "pmid:", "pmid "]
            for pattern in pmid_patterns:
                if pattern in note_content:
                    try:
                        pmid_text = note_content.split(pattern)[1].split()[0].strip()
                        # Valide que c'est un nombre
                        if pmid_text.isdigit():
                            pmid = pmid_text
                            break
                    except (IndexError, ValueError):
                        continue

            # --- DOI nettoyage ---
            doi = item.get("DOI", "").strip()
            if doi and not doi.startswith("10."):
                doi = ""  # DOI invalide

            title = item.get("title", f"Titre inconnu {i+1}").strip()
            abstract = item.get("abstract", "Aucun abstract disponible.").strip()
            
            article = {
                "title": title,
                "authors": authors,
                "year": year,
                "abstract": abstract,
                "journal": item.get("container-title", "Journal inconnu").strip(),
                "doi": doi,
                "pmid": pmid,
                "type": item.get("type", "article-journal"),
                "language": item.get("language", "en"),
                "keywords": item.get("keywords", ["thÃ¨se", "ATN"]),
                "zotero_id": item.get("id", str(uuid.uuid4())),
                # Champs additionnels pour le backend
                "source": "zotero_import",
                "import_timestamp": datetime.now().isoformat()
            }
            
            articles.append(article)
            log("SUCCESS", f"Article {i+1:2d}: {title[:50]}{'...' if len(title) > 50 else ''}", 1)
            
        except Exception as e:
            log("WARNING", f"Erreur parsing article {i+1}: {e}", 1)
            continue

    log("SUCCESS", f"ğŸ“š {len(articles)} articles parsÃ©s avec succÃ¨s depuis Zotero.")
    return articles

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHARGEMENT GRILLE ATN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def load_grille_atn(json_path: Path) -> Dict:
    """Charge le fichier de configuration de la grille ATN."""
    log("INFO", f"Chargement de la grille d'extraction '{json_path.name}'...")
    
    if not json_path.is_file():
        log("WARNING", f"Fichier de grille ATN introuvable: {json_path}")
        return {}
        
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            grille = json.load(f)
        
        fields_count = len(grille.get('fields', []))
        log("SUCCESS", f"Grille ATN chargÃ©e avec {fields_count} champs d'extraction.")
        return grille
        
    except (json.JSONDecodeError, IOError) as e:
        log("ERROR", f"Impossible de charger ou parser la grille ATN: {e}")
        return {}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLASSE WORKFLOW PRINCIPAL OPTIMISÃ‰E
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
class ATNWorkflowZoteroOptimized:
    """Classe workflow ATN optimisÃ©e avec gestion avancÃ©e des erreurs."""
    
    def __init__(self):
        self.project_id: Optional[str] = None
        self.articles_data: List[Dict] = []
        self.articles_backend_ids: List[int] = []  # IDs gÃ©nÃ©rÃ©s par le backend
        self.grille_atn: Dict = {}
        self.results: Dict[str, Any] = {
            "timestamp_start": datetime.now().isoformat(),
            "project_id": None,
            "articles_count": 0,
            "steps": {},
            "final_metrics": {},
            "errors": [],
            "warnings": []
        }

    def add_error(self, step: str, error: str):
        """Ajoute une erreur au rapport."""
        self.results["errors"].append({
            "step": step,
            "error": error,
            "timestamp": datetime.now().isoformat()
        })

    def add_warning(self, step: str, warning: str):
        """Ajoute un avertissement au rapport."""
        self.results["warnings"].append({
            "step": step,
            "warning": warning,
            "timestamp": datetime.now().isoformat()
        })

    def run_step(self, step_name: str, step_func) -> bool:
        """ExÃ©cute une Ã©tape du workflow avec mesure de performance."""
        start_time = time.time()
        success = False
        error_message = ""
        
        try:
            success = step_func()
        except Exception as e:
            error_message = str(e)
            log("ERROR", f"Exception inattendue Ã  l'Ã©tape '{step_name}': {e}")
            self.add_error(step_name, error_message)
            import traceback
            traceback.print_exc()
                
        duration = time.time() - start_time
        self.results["steps"][step_name] = {
            "success": success,
            "duration_seconds": round(duration, 2),
            "error": error_message if error_message else None
        }
                
        if not success:
            log("ERROR", f"Ã‰chec de l'Ã©tape '{step_name}'. Voir les dÃ©tails ci-dessus.")
                
        return success

    def check_api_health(self) -> bool:
        """VÃ©rifie la santÃ© de l'API AnalyLit."""
        log_section("Ã‰TAPE 0/7: VÃ‰RIFICATION DE L'API ANALYLIT")
        
        health = api_request("GET", "/api/health", timeout=10)
        if health and health.get("status") == "healthy":
            log("SUCCESS", "API AnalyLit est opÃ©rationnelle.")
            log("DATA", f"Version: {health.get('version', 'N/A')}", 1)
            log("DATA", f"Services: {', '.join(health.get('services', []))}", 1)
            return True
            
        log("ERROR", "L'API AnalyLit n'est pas disponible. Veuillez dÃ©marrer le backend.")
        return False

    def load_data_sources(self) -> bool:
        """Charge les donnÃ©es sources (articles Zotero + grille ATN)."""
        log_section("Ã‰TAPE 1/7: CHARGEMENT DES DONNÃ‰ES SOURCES")
        
        # Chargement articles Zotero
        self.articles_data = parse_zotero_csl_json(ZOTERO_JSON_PATH)
        if not self.articles_data:
            self.add_error("load_data", "Aucun article chargÃ© depuis Zotero")
            return False
            
        # Chargement grille ATN
        self.grille_atn = load_grille_atn(GRILLE_ATN_PATH)
        if not self.grille_atn:
            self.add_warning("load_data", "Grille ATN non trouvÃ©e - extraction basique sera utilisÃ©e")
                    
        self.results["articles_count"] = len(self.articles_data)
        log("DATA", f"Total articles Zotero: {len(self.articles_data)}", 1)
        log("DATA", f"Champs d'extraction ATN: {len(self.grille_atn.get('fields', []))}", 1)
        
        # Validation des donnÃ©es
        valid_articles = [a for a in self.articles_data if a.get('title') and a.get('abstract')]
        if len(valid_articles) < len(self.articles_data):
            missing = len(self.articles_data) - len(valid_articles)
            self.add_warning("load_data", f"{missing} articles sans titre/abstract seront ignorÃ©s")
            
        return True

    def create_atn_project(self) -> bool:
        """CrÃ©e le projet ATN dans AnalyLit."""
        log_section("Ã‰TAPE 2/7: CRÃ‰ATION DU PROJET ATN")
        
        project_name = f"ATN Zotero Validation - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        project_description = (
            f"Projet de validation empirique pour la thÃ¨se ATN.\n"
            f"Source: {len(self.articles_data)} articles de Zotero (20ATN.json)\n"
            f"Pipeline: Screening â†’ Extraction â†’ SynthÃ¨se â†’ Export\n"
            f"Grille: {len(self.grille_atn.get('fields', []))} champs ATN spÃ©cialisÃ©s\n"
            f"Date: {datetime.now().strftime('%d/%m/%Y %H:%M')}"
        )
        
        data = {
            "name": project_name,
            "description": project_description,
            "type": "atn_validation",  # Type spÃ©cialisÃ©
            "metadata": {
                "source": "zotero_export",
                "thesis_validation": True,
                "article_count": len(self.articles_data),
                "grille_fields": len(self.grille_atn.get('fields', []))
            }
        }
        
        result = api_request("POST", "/api/projects", data, timeout=TIMEOUT_CONFIG["api_request"])
                
        if result and "id" in result:
            self.project_id = result["id"]
            self.results["project_id"] = self.project_id
            log("SUCCESS", f"Projet crÃ©Ã© avec succÃ¨s.")
            log("DATA", f"Project ID: {self.project_id}", 1)
            log("DATA", f"URL interface: http://localhost:3000/projects/{self.project_id}", 1)
            return True
            
        log("ERROR", "La crÃ©ation du projet a Ã©chouÃ©.")
        self.add_error("create_project", "Ã‰chec crÃ©ation projet - vÃ©rifier les logs API")
        return False

    def add_articles_to_project(self) -> bool:
        """Ajoute les articles au projet et rÃ©cupÃ¨re les IDs backend."""
        log_section("Ã‰TAPE 3/7: AJOUT DES ARTICLES AU PROJET")
        
        # âœ… CORRECTION: Utilise le bon endpoint et format de donnÃ©es
        data = {
            "items": self.articles_data,
            "source": "zotero_import",
            "batch_size": 10  # Traitement par batch pour Ã©viter surcharge
        }
        
        endpoint = f"/api/projects/{self.project_id}/add-manual-articles"
        result = api_request("POST", endpoint, data, timeout=TIMEOUT_CONFIG["add_articles"])
                
        if result and result.get("task_id"):
            task_id = result["task_id"]
            log("SUCCESS", f"TÃ¢che d'ajout lancÃ©e (Job ID: {task_id})")
            log("PROGRESS", f"Ajout de {len(self.articles_data)} articles en cours...", 1)
            
            # Attendre la fin de l'ajout et rÃ©cupÃ©rer les IDs
            if self.wait_for_task(task_id, "ajout des articles", timeout_min=10):
                return self.get_articles_backend_ids()
            else:
                self.add_error("add_articles", f"Timeout sur tÃ¢che d'ajout {task_id}")
                return False
                
        log("ERROR", "L'ajout des articles a Ã©chouÃ©.")
        self.add_error("add_articles", "Ã‰chec lancement tÃ¢che d'ajout")
        return False

    def get_articles_backend_ids(self) -> bool:
        """RÃ©cupÃ¨re les IDs des articles gÃ©nÃ©rÃ©s par le backend."""
        log("INFO", "RÃ©cupÃ©ration des IDs articles du backend...")
        
        # RÃ©cupÃ¨re la liste des articles du projet
        result = api_request("GET", f"/api/projects/{self.project_id}/articles")
        
        if result and "articles" in result:
            articles = result["articles"]
            self.articles_backend_ids = [article["id"] for article in articles]
            log("SUCCESS", f"IDs rÃ©cupÃ©rÃ©s: {len(self.articles_backend_ids)} articles", 1)
            
            # Log des premiers IDs pour vÃ©rification
            sample_ids = self.articles_backend_ids[:5]
            log("DATA", f"Ã‰chantillon IDs: {sample_ids}", 1)
            return True
        else:
            log("WARNING", "Impossible de rÃ©cupÃ©rer les IDs - utilisation des titres comme fallback")
            self.add_warning("get_ids", "Fallback vers titres d'articles au lieu des IDs")
            return True  # Non bloquant

    def run_atn_screening(self) -> bool:
        """Lance le screening ATN avec seuil â‰¥ 70/100."""
        log_section("Ã‰TAPE 4/7: SCREENING ATN (SEUIL â‰¥ 70/100)")
        
        # âœ… CORRECTION: Utilise les IDs backend ou fallback
        if self.articles_backend_ids:
            article_identifiers = self.articles_backend_ids
            log("INFO", f"Utilisation des IDs backend: {len(article_identifiers)} articles")
        else:
            # Fallback vers les titres si pas d'IDs
            article_identifiers = [article["title"] for article in self.articles_data]
            log("WARNING", "Utilisation des titres comme identifiants (fallback)")
            
        data = {
            "type": "screening",
            "articles": article_identifiers,
            "profile": "atn-fast-gpu",  # Profil optimisÃ© RTX 2060 SUPER
            "screening_config": {
                "threshold": 70,
                "auto_validate": True,
                "parallel_processing": True
            },
            "analysis_options": {
                "include_confidence": True,
                "detailed_scoring": True
            }
        }
        
        # âœ… CORRECTION: Bon endpoint
        endpoint = f"/api/projects/{self.project_id}/run-analysis"
        result = api_request("POST", endpoint, data, timeout=TIMEOUT_CONFIG["analysis"])
                
        if result and result.get("job_id"):
            job_id = result["job_id"]
            log("SUCCESS", f"Screening ATN lancÃ© (Job ID: {job_id})")
            
            # Monitoring non-bloquant (screening peut Ãªtre long)
            if self.wait_for_task(job_id, "screening ATN", timeout_min=15):
                log("SUCCESS", "Screening ATN terminÃ© avec succÃ¨s")
            else:
                log("WARNING", "Screening ATN en cours - passage Ã  l'Ã©tape suivante")
                self.add_warning("screening", "Screening non terminÃ© dans le dÃ©lai imparti")
                
            return True  # Non bloquant mÃªme si timeout
        else:
            log("WARNING", "Impossible de lancer le screening - passage Ã  l'extraction")
            self.add_warning("screening", "Ã‰chec lancement screening - Ã©tape ignorÃ©e")
            return True  # Non bloquant

    def run_atn_extraction(self) -> bool:
        """Lance l'extraction avec la grille ATN spÃ©cialisÃ©e."""
        log_section("Ã‰TAPE 5/7: EXTRACTION AVEC GRILLE ATN")
        
        if not self.grille_atn.get("fields"):
            log("WARNING", "Aucune grille d'extraction - extraction basique sera utilisÃ©e")
            extraction_fields = []  # Backend utilisera grille par dÃ©faut
        else:
            extraction_fields = self.grille_atn["fields"]
            log("INFO", f"Utilisation grille ATN: {len(extraction_fields)} champs")
        
        # Utilise les IDs backend si disponibles
        if self.articles_backend_ids:
            article_identifiers = self.articles_backend_ids
        else:
            article_identifiers = [article["title"] for article in self.articles_data]
            
        data = {
            "type": "extraction",
            "articles": article_identifiers,
            "extraction_config": {
                "grid_fields": extraction_fields,
                "profile": "atn-specialized",  # Profil ATN spÃ©cialisÃ©
                "parallel_processing": True,
                "batch_size": 5  # Batch plus petit pour extraction
            },
            "output_format": "detailed_json"
        }
        
        endpoint = f"/api/projects/{self.project_id}/run-analysis"
        result = api_request("POST", endpoint, data, timeout=TIMEOUT_CONFIG["extraction"])
                
        if result and result.get("job_id"):
            job_id = result["job_id"]
            log("SUCCESS", f"Extraction ATN lancÃ©e (Job ID: {job_id})")
            
            # Extraction est critique - attendre la fin
            if self.wait_for_task(job_id, "extraction ATN", timeout_min=20):
                log("SUCCESS", "Extraction ATN terminÃ©e avec succÃ¨s")
                return True
            else:
                log("ERROR", "Timeout sur l'extraction ATN")
                self.add_error("extraction", f"Timeout sur tÃ¢che d'extraction {job_id}")
                return False
        else:
            log("ERROR", "Le lancement de l'extraction a Ã©chouÃ©.")
            self.add_error("extraction", "Ã‰chec lancement extraction")
            return False

    def run_synthesis_and_prisma(self) -> bool:
        """Lance la synthÃ¨se et gÃ©nÃ¨re le diagramme PRISMA."""
        log_section("Ã‰TAPE 6/7: SYNTHÃˆSE ET DIAGRAMME PRISMA")
        
        data = {
            "type": "synthesis",
            "synthesis_config": {
                "include_prisma": True,
                "generate_graphs": True,
                "atn_analysis": True,  # Analyse spÃ©cialisÃ©e ATN
                "export_ready": True
            },
            "prisma_config": {
                "identification": len(self.articles_data),
                "screening_threshold": 70,
                "inclusion_criteria": "ATN relevance",
                "methodology": "PRISMA-ScR"
            },
            "output_formats": ["json", "pdf", "docx"]
        }
        
        endpoint = f"/api/projects/{self.project_id}/run-analysis"
        result = api_request("POST", endpoint, data, timeout=TIMEOUT_CONFIG["synthesis"])
                
        if result and result.get("job_id"):
            job_id = result["job_id"]
            log("SUCCESS", f"SynthÃ¨se et PRISMA lancÃ©s (Job ID: {job_id})")
            
            if self.wait_for_task(job_id, "synthÃ¨se et PRISMA", timeout_min=12):
                log("SUCCESS", "SynthÃ¨se et PRISMA terminÃ©s avec succÃ¨s")
                return True
            else:
                log("WARNING", "SynthÃ¨se non terminÃ©e dans les dÃ©lais")
                self.add_warning("synthesis", "SynthÃ¨se en cours - rÃ©sultats partiels disponibles")
                return True  # Non bloquant
        else:
            log("ERROR", "Le lancement de la synthÃ¨se a Ã©chouÃ©.")
            self.add_error("synthesis", "Ã‰chec lancement synthÃ¨se")
            return False

    def export_thesis_results(self) -> bool:
        """Exporte les rÃ©sultats pour la thÃ¨se doctorale."""
        log_section("Ã‰TAPE 7/7: EXPORT DES RÃ‰SULTATS POUR LA THÃˆSE")
        
        # âœ… CORRECTION: Utilise GET pour tÃ©lÃ©charger le fichier
        endpoint = f"/api/projects/{self.project_id}/export/thesis"
        params = {
            "format": "complete",  # Export complet
            "include_raw_data": True,
            "include_analysis": True,
            "include_prisma": True
        }
        
        url = f"{API_BASE}{endpoint}"
        
        try:
            log("INFO", "DÃ©marrage de l'export des rÃ©sultats...", 1)
            resp = requests.get(url, params=params, stream=True, timeout=TIMEOUT_CONFIG["export"])
            
            if resp.status_code == 200:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                export_filename = f"export_these_atn_{self.project_id}_{timestamp}.zip"
                export_path = OUTPUT_DIR / export_filename
                
                # TÃ©lÃ©chargement avec progression
                total_size = int(resp.headers.get('content-length', 0))
                downloaded = 0
                
                with open(export_path, 'wb') as f:
                    for chunk in resp.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded += len(chunk)
                            
                            if total_size > 0:
                                progress = (downloaded / total_size) * 100
                                log("PROGRESS", f"Export: {progress:.1f}% ({downloaded:,} / {total_size:,} bytes)", 2)
                
                log("SUCCESS", f"Export terminÃ©: {export_path}")
                log("DATA", f"Taille: {export_path.stat().st_size:,} bytes", 1)
                return True
                
            elif resp.status_code == 202:
                log("INFO", "Export en cours de gÃ©nÃ©ration... Veuillez patienter.")
                # Pourrait implÃ©menter un polling ici
                return True
            else:
                error_msg = f"Export Ã©chouÃ© - Code {resp.status_code}: {resp.text[:200]}"
                log("ERROR", error_msg)
                self.add_error("export", error_msg)
                return False
                
        except requests.RequestException as e:
            error_msg = f"Erreur lors de la requÃªte d'export: {e}"
            log("ERROR", error_msg)
            self.add_error("export", error_msg)
            return False

    def wait_for_task(self, task_id: str, step_name: str, timeout_min: int = 20) -> bool:
        """Attend la fin d'une tÃ¢che avec monitoring avancÃ©."""
        log("PROGRESS", f"Monitoring de la tÃ¢che '{step_name}' (max {timeout_min} min)...")
        
        start_time = time.time()
        last_progress = ""
        check_interval = TIMEOUT_CONFIG["task_polling"]
        
        while time.time() - start_time < timeout_min * 60:
            endpoint = f"/api/tasks/{task_id}/status"
            status_data = api_request("GET", endpoint, timeout=10)
                        
            if status_data:
                status = status_data.get("status", "unknown")
                progress = status_data.get("progress", {})
                error_info = status_data.get("error")
                
                # Affiche seulement les nouveaux messages pour Ã©viter spam
                current_progress = f"{status} - {progress.get('message', '')}"
                if current_progress != last_progress:
                    log("PROGRESS", f"'{step_name}': {current_progress}", 1)
                    last_progress = current_progress
                
                if status == "finished":
                    log("SUCCESS", f"TÃ¢che '{step_name}' terminÃ©e avec succÃ¨s")
                    return True
                    
                if status == "failed":
                    error_detail = error_info or progress.get('error', 'Erreur inconnue')
                    log("ERROR", f"TÃ¢che '{step_name}' Ã©chouÃ©e: {error_detail}")
                    self.add_error(step_name, f"TÃ¢che Ã©chouÃ©e: {error_detail}")
                    return False
            else:
                log("WARNING", f"Impossible de rÃ©cupÃ©rer le statut de la tÃ¢che {task_id}", 1)
                        
            time.sleep(check_interval)
                    
        # Timeout atteint
        elapsed = (time.time() - start_time) / 60
        log("TIMEOUT", f"Timeout atteint pour '{step_name}' aprÃ¨s {elapsed:.1f} min")
        self.add_warning(step_name, f"Timeout aprÃ¨s {elapsed:.1f} minutes")
        return False

    def get_final_metrics(self) -> Dict:
        """RÃ©cupÃ¨re les mÃ©triques finales du projet."""
        log("INFO", "Collecte des mÃ©triques finales...")
        
        project = api_request("GET", f"/api/projects/{self.project_id}")
        if not project:
            log("WARNING", "Impossible de rÃ©cupÃ©rer les mÃ©triques du projet")
            return {}
                
        metrics = {
            "articles_total": len(self.articles_data),
            "articles_processed": project.get("articles_count", 0),
            "mean_atn_score": project.get("mean_score", 0),
            "status": project.get("status", "unknown"),
            "processing_time_minutes": sum(
                step.get("duration_seconds", 0) 
                for step in self.results["steps"].values()
            ) / 60,
            "success_rate": len([
                step for step in self.results["steps"].values() 
                if step.get("success", False)
            ]) / len(self.results["steps"]) * 100,
            "errors_count": len(self.results["errors"]),
            "warnings_count": len(self.results["warnings"])
        }
        
        log("DATA", f"Score ATN moyen: {metrics['mean_atn_score']:.1f}/100", 1)
        log("DATA", f"Temps total: {metrics['processing_time_minutes']:.1f} minutes", 1)
        log("DATA", f"Taux de succÃ¨s: {metrics['success_rate']:.1f}%", 1)
        
        return metrics

    def save_report(self):
        """Sauvegarde le rapport final dÃ©taillÃ© du workflow."""
        self.results["timestamp_end"] = datetime.now().isoformat()
        
        if self.project_id:
            self.results["final_metrics"] = self.get_final_metrics()
                
        # Ajout statistiques de performance
        self.results["performance"] = {
            "total_duration_minutes": (
                datetime.fromisoformat(self.results["timestamp_end"]) - 
                datetime.fromisoformat(self.results["timestamp_start"])
            ).total_seconds() / 60,
            "steps_summary": {
                step_name: {
                    "success": step_data.get("success", False),
                    "duration_seconds": step_data.get("duration_seconds", 0)
                }
                for step_name, step_data in self.results["steps"].items()
            }
        }
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = OUTPUT_DIR / f"rapport_workflow_atn_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(self.results, f, indent=2, ensure_ascii=False)
                
            log("SUCCESS", f"ğŸ“Š Rapport de workflow sauvegardÃ©: {filename}")
            log("DATA", f"Taille rapport: {filename.stat().st_size:,} bytes", 1)
            
            # Rapport rÃ©sumÃ© en texte
            self.save_summary_report(timestamp)
            
        except IOError as e:
            log("ERROR", f"Impossible de sauvegarder le rapport: {e}")

    def save_summary_report(self, timestamp: str):
        """Sauvegarde un rapport rÃ©sumÃ© lisible."""
        summary_path = OUTPUT_DIR / f"resume_workflow_atn_{timestamp}.txt"
        
        try:
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
                f.write("   RAPPORT DE VALIDATION EMPIRIQUE ATN - RÃ‰SUMÃ‰ EXÃ‰CUTIF\n")
                f.write("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n")
                
                f.write(f"ğŸ“… Date: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n")
                f.write(f"ğŸ†” Projet ID: {self.project_id}\n")
                f.write(f"ğŸ“š Articles traitÃ©s: {len(self.articles_data)}\n\n")
                
                # RÃ©sultats par Ã©tape
                f.write("ğŸ”„ RÃ‰SULTATS PAR Ã‰TAPE:\n")
                for step_name, step_data in self.results["steps"].items():
                    status = "âœ… SUCCÃˆS" if step_data.get("success") else "âŒ Ã‰CHEC"
                    duration = step_data.get("duration_seconds", 0)
                    f.write(f"   {step_name:20} â†’ {status:8} ({duration:.1f}s)\n")
                
                # MÃ©triques finales
                if "final_metrics" in self.results:
                    metrics = self.results["final_metrics"]
                    f.write(f"\nğŸ“Š MÃ‰TRIQUES FINALES:\n")
                    f.write(f"   Score ATN moyen: {metrics.get('mean_atn_score', 0):.1f}/100\n")
                    f.write(f"   Temps total: {metrics.get('processing_time_minutes', 0):.1f} min\n")
                    f.write(f"   Taux de succÃ¨s: {metrics.get('success_rate', 0):.1f}%\n")
                
                # Erreurs et avertissements
                if self.results["errors"]:
                    f.write(f"\nâŒ ERREURS ({len(self.results['errors'])}):\n")
                    for error in self.results["errors"]:
                        f.write(f"   â€¢ {error['step']}: {error['error']}\n")
                
                if self.results["warnings"]:
                    f.write(f"\nâš ï¸  AVERTISSEMENTS ({len(self.results['warnings'])}):\n")
                    for warning in self.results["warnings"]:
                        f.write(f"   â€¢ {warning['step']}: {warning['warning']}\n")
                
                f.write(f"\nğŸ”— Interface web: http://localhost:3000/projects/{self.project_id}\n")
                f.write("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n")
                
            log("SUCCESS", f"ğŸ“‹ Rapport rÃ©sumÃ© sauvegardÃ©: {summary_path}")
            
        except IOError as e:
            log("WARNING", f"Impossible de sauvegarder le rapport rÃ©sumÃ©: {e}")

    def run_complete_workflow(self):
        """Orchestre l'exÃ©cution complÃ¨te du workflow ATN optimisÃ©."""
        log_section("ğŸš€ DÃ‰MARRAGE DU WORKFLOW ATN COMPLET OPTIMISÃ‰ V2 ğŸš€")
        log("INFO", f"RTX 2060 SUPER - {len(self.articles_data) if hasattr(self, 'articles_data') else 'N/A'} articles Zotero")
        log("INFO", f"Timeouts configurÃ©s: Analysis={TIMEOUT_CONFIG['analysis']}s, Extraction={TIMEOUT_CONFIG['extraction']}s")
                
        steps_config = [
            ("health_check", self.check_api_health, True),      # Bloquant
            ("load_data", self.load_data_sources, True),        # Bloquant
            ("create_project", self.create_atn_project, True),  # Bloquant
            ("add_articles", self.add_articles_to_project, True), # Bloquant
            ("screening", self.run_atn_screening, False),       # Non bloquant
            ("extraction", self.run_atn_extraction, True),      # Bloquant
            ("synthesis_prisma", self.run_synthesis_and_prisma, False), # Non bloquant
            ("export_thesis", self.export_thesis_results, False)  # Non bloquant
        ]
        
        try:
            failed_critical = False
            
            for step_name, step_func, is_critical in steps_config:
                success = self.run_step(step_name, step_func)
                
                if not success and is_critical:
                    log("ERROR", f"Ã‰tape critique '{step_name}' Ã©chouÃ©e - ArrÃªt du workflow")
                    failed_critical = True
                    break
                elif not success:
                    log("WARNING", f"Ã‰tape '{step_name}' Ã©chouÃ©e mais non critique - Continuation")
            
            if not failed_critical:
                log_section("ğŸ‰ WORKFLOW TERMINÃ‰ ğŸ‰")
                
                # Affichage des rÃ©sultats finaux
                if self.project_id:
                    metrics = self.get_final_metrics()
                    log("SUCCESS", "ğŸ† RÃ‰SULTATS DE LA VALIDATION EMPIRIQUE ATN:")
                    log("DATA", f"Articles traitÃ©s: {metrics.get('articles_processed', 'N/A')} / {metrics.get('articles_total', 'N/A')}", 1)
                    log("DATA", f"Score ATN moyen: {metrics.get('mean_atn_score', 0):.1f}/100", 1)
                    log("DATA", f"Temps de traitement: {metrics.get('processing_time_minutes', 0):.1f} min", 1)
                    log("DATA", f"Taux de succÃ¨s: {metrics.get('success_rate', 0):.1f}%", 1)
                    
                    if metrics.get('errors_count', 0) == 0:
                        log("SUCCESS", "âœ… Aucune erreur - Validation empirique rÃ©ussie!")
                    else:
                        log("WARNING", f"âš ï¸ {metrics.get('errors_count')} erreur(s) - Voir rapport dÃ©taillÃ©")
                    
                    log("INFO", f"ğŸ”— Interface: http://localhost:3000/projects/{self.project_id}")
                else:
                    log("WARNING", "Projet non crÃ©Ã© - RÃ©sultats limitÃ©s")
            else:
                log("ERROR", "âŒ Workflow interrompu suite Ã  une erreur critique")
                
        except Exception as e:
            log("ERROR", f"ğŸ’¥ Erreur critique dans le workflow: {e}")
            self.add_error("workflow", f"Exception critique: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # Toujours sauvegarder le rapport
            self.save_report()
            log("INFO", "ğŸ“Š Rapport de validation sauvegardÃ© dans:", 1)
            log("INFO", f"   {OUTPUT_DIR}", 1)
            log("INFO", "ğŸ Fin du script de validation empirique ATN.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# POINT D'ENTRÃ‰E PRINCIPAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    """Point d'entrÃ©e principal avec gestion des exceptions globales."""
    try:
        workflow = ATNWorkflowZoteroOptimized()
        workflow.run_complete_workflow()
        
    except KeyboardInterrupt:
        log("WARNING", "âš ï¸ Interruption manuelle du script (Ctrl+C)")
        log("INFO", "Sauvegarde d'urgence en cours...")
        sys.exit(130)
        
    except Exception as e:
        log("ERROR", f"ğŸ’¥ Erreur fatale: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()